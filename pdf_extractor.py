#!/usr/bin/env python3
"""
Script d'extraction automatique d'informations de propri√©taires depuis des PDFs.

Auteur: Assistant IA
Date: 2025
"""

import os
import json
import logging
import base64
from pathlib import Path
from typing import List, Dict, Optional
import fitz  # PyMuPDF
import pdfplumber
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from PIL import Image
import io
import logging.handlers
import sys
import tempfile
import shutil
import re
import gc
import time

# Configuration du logging avec encodage UTF-8 pour Windows
def setup_logging():
    """Configure le logging avec support UTF-8 pour Windows"""
    # Cr√©er un formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Logger principal
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    # Supprimer les handlers existants
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Handler pour fichier avec encodage UTF-8
    try:
        file_handler = logging.FileHandler('extraction.log', encoding='utf-8', mode='a')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    except Exception as e:
        print(f"Erreur configuration fichier log: {e}")
    
    # Handler pour console avec gestion des erreurs d'encodage
    try:
        # Cr√©er un stream qui ignore les erreurs d'encodage
        console_stream = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        console_handler = logging.StreamHandler(console_stream)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
    except Exception:
        # Fallback vers un handler console simple sans emojis
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
    
    return logger

# Initialiser le logger
logger = setup_logging()

def clean_commune_code(commune: str) -> str:
    """
    Extrait le code commune sur EXACTEMENT 3 chiffres.
    ‚úÖ PR√âSERVE LA COMMUNE au lieu de la vider
    
    Exemples:
    - "238 MAILLY-LE-CHATEAU" ‚Üí "238"
    - "38" ‚Üí "038" 
    - "5" ‚Üí "005"
    - "2380" ‚Üí "238"
    """
    if not commune:
        return ""
    
    # Chercher les premiers chiffres dans la cha√Æne
    match = re.search(r'(\d+)', commune.strip())
    if match:
        # Prendre exactement 3 chiffres
        number = match.group(1)
        if len(number) >= 3:
            return number[:3]  # "238" ou "2380" ‚Üí "238"
        else:
            return number.zfill(3)  # "38" ‚Üí "038", "5" ‚Üí "005"
    
    # ‚úÖ PR√âSERVATION: Ne plus vider si pas de chiffres trouv√©s
    logger.warning(f"üîç Commune sans chiffres: '{commune}' - Pr√©serv√©e tel quel")
    return commune.strip()

def safe_json_parse(content: str, context: str = "API response") -> Optional[Dict]:
    """
    Parse JSON de mani√®re robuste avec gestion d'erreurs
    
    Args:
        content: Contenu √† parser
        context: Contexte pour le logging d'erreur
    
    Returns:
        Dict pars√© ou None si √©chec
    """
    if not content or content.strip() == "":
        logger.warning(f"Contenu vide pour {context}")
        return None
    
    # Nettoyer le contenu
    content = content.strip()
    
    # Chercher un objet JSON dans la r√©ponse
    start_idx = content.find('{')
    end_idx = content.rfind('}')
    
    if start_idx == -1 or end_idx == -1:
        logger.warning(f"Pas de JSON trouv√© dans {context}: {content[:100]}...")
        return None
    
    try:
        json_content = content[start_idx:end_idx+1]
        result = json.loads(json_content)
        return result
    except json.JSONDecodeError as e:
        logger.warning(f"Erreur parsing JSON pour {context}: {e}")
        logger.debug(f"Contenu probl√©matique: {json_content[:200]}...")
        return None
    except Exception as e:
        logger.error(f"Erreur inattendue parsing JSON pour {context}: {e}")
        return None

class PDFPropertyExtractor:
    """Classe principale pour l'extraction d'informations de propri√©taires depuis des PDFs."""
    
    def __init__(self, input_dir: str = "input", output_dir: str = "output"):
        """
        Initialise l'extracteur.
        
        Args:
            input_dir: Dossier contenant les PDFs
            output_dir: Dossier de sortie pour les r√©sultats
        """
        # Charger les variables d'environnement
        load_dotenv()
        
        # R√©cup√©rer la cl√© API depuis les variables d'environnement
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("La cl√© API OpenAI n'est pas configur√©e. Veuillez d√©finir OPENAI_API_KEY dans le fichier .env")
        
        self.client = OpenAI(api_key=api_key)
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.default_section = os.getenv('DEFAULT_SECTION', 'A')
        self.default_plan_number = int(os.getenv('DEFAULT_PLAN_NUMBER', '123'))
        
        # Cr√©er les dossiers s'ils n'existent pas
        self.input_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        
        logger.info(f"Extracteur initialis√© - Input: {self.input_dir}, Output: {self.output_dir}")

    def clean_extraction_context(self, pdf_path: Path) -> None:
        """
        üßπ NETTOYAGE ANTI-CONTAMINATION ULTRA-S√âCURIS√â avant chaque PDF.
        
        VERSION 2.0 : Isolation compl√®te pour √©liminer toute contamination batch.
        """
        logger.info(f"üßπ NETTOYAGE ULTRA-S√âCURIS√â pour {pdf_path.name}")
        
        try:
            # 1. FORCER LE GARBAGE COLLECTOR AGRESSIVEMENT
            import gc
            gc.collect()
            gc.collect()  # Double nettoyage
            
            # 2. CR√âER UN NOUVEAU CLIENT OPENAI TOTALEMENT ISOL√â
            api_key = os.getenv('OPENAI_API_KEY')
            if api_key:
                # Fermer l'ancien client si possible
                if hasattr(self.client, '_client') and hasattr(self.client._client, 'close'):
                    try:
                        self.client._client.close()
                    except:
                        pass
                
                # Cr√©er un client compl√®tement nouveau
                self.client = OpenAI(api_key=api_key)
                logger.info("‚úÖ Nouveau client OpenAI cr√©√© (isolation compl√®te)")
            
            # 3. VIDER TOUS LES CACHES ET VARIABLES D'√âTAT
            cache_attrs = [
                '_image_cache', '_text_cache', '_extraction_cache', 
                '_prompt_cache', '_response_cache', '_header_cache'
            ]
            for attr in cache_attrs:
                if hasattr(self, attr):
                    getattr(self, attr).clear()
            
            # 4. R√âINITIALISER TOUTES LES VARIABLES DE TRAITEMENT
            reset_attrs = [
                '_last_processed_pdf', '_current_context', '_last_department',
                '_last_commune', '_extraction_history', '_contamination_context',
                '_temp_owners', '_temp_props', '_temp_results', '_current_extraction_state'
            ]
            for attr in reset_attrs:
                if hasattr(self, attr):
                    setattr(self, attr, None)
            
            # 5. NETTOYAGE SP√âCIAL POUR CONTAMINATION G√âOGRAPHIQUE
            self._geographic_cache = {}
            self._department_context = {}
            self._commune_context = {}
            
            # 6. FORCER LE NETTOYAGE DES VARIABLES GLOBALES POTENTIELLES
            globals_to_clear = [
                'CURRENT_PDF_CONTEXT', 'LAST_EXTRACTION', 'CURRENT_DEPARTMENT',
                'CURRENT_COMMUNE', 'EXTRACTION_MEMORY'
            ]
            for var_name in globals_to_clear:
                if var_name in globals():
                    globals()[var_name] = None
            
            # 7. NETTOYAGE TEMPORAIRE
            try:
                import tempfile
                temp_dir = tempfile.gettempdir()
                # Nettoyer les fichiers temporaires li√©s √† l'extraction PDF
                for temp_file in Path(temp_dir).glob("*pdf_extract*"):
                    try:
                        temp_file.unlink()
                    except:
                        pass
            except:
                pass
            
            logger.info(f"‚úÖ NETTOYAGE ULTRA-S√âCURIS√â TERMIN√â pour {pdf_path.name}")
            
        except Exception as e:
            logger.warning(f"Erreur lors du nettoyage: {e}")

    def batch_ultra_secure_cleanup(self, pdf_index: int, total_pdfs: int, pdf_path: Path) -> None:
        """
        üõ°Ô∏è NETTOYAGE BATCH ULTRA-S√âCURIS√â SP√âCIALIS√â
        
        M√©canismes d'isolation renforc√©s sp√©cifiquement pour le traitement par lots.
        """
        logger.info(f"üõ°Ô∏è BATCH CLEANUP [{pdf_index}/{total_pdfs}] - {pdf_path.name}")
        
        try:
            # 1. NETTOYAGE STANDARD ULTRA-S√âCURIS√â
            self.clean_extraction_context(pdf_path)
            
            # 2. ISOLATION BATCH SP√âCIALIS√âE
            
            # 2.1 Forcer l'oubli du contexte conversationnel OpenAI
            # En cr√©ant un d√©lai artificiel pour laisser le temps au serveur OpenAI d'oublier
            if pdf_index > 1:  # Pas pour le premier PDF
                import time
                time.sleep(0.5)  # Micro-pause pour isolation serveur
            
            # 2.2 R√©initialiser compl√®tement l'√©tat du processeur
            self.__dict__.update({
                '_batch_contamination_guard': pdf_path.name,
                '_current_pdf_isolation_id': f"{pdf_path.name}_{pdf_index}_{int(time.time())}",
                '_batch_processing_state': 'isolated'
            })
            
            # 2.3 Variables sp√©cifiques au batch √† nettoyer
            batch_vars = [
                '_batch_context', '_cross_pdf_memory', '_accumulated_data',
                '_batch_department_history', '_batch_commune_history'
            ]
            for var in batch_vars:
                if hasattr(self, var):
                    setattr(self, var, {})
            
            # 2.4 Nettoyage m√©moire Python agressif
            import gc
            collected = gc.collect()
            logger.debug(f"üóëÔ∏è Garbage collector: {collected} objets supprim√©s")
            
            # 2.5 Log de v√©rification isolation
            logger.info(f"‚úÖ ISOLATION BATCH ACTIV√âE - PDF {pdf_index}/{total_pdfs}")
            logger.info(f"üîí ID d'isolation: {getattr(self, '_current_pdf_isolation_id', 'unknown')}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur nettoyage batch ultra-s√©curis√©: {e}")

    def validate_extraction_consistency(self, owners: List[Dict], structured_data: Dict, pdf_path: Path) -> bool:
        """
        üîç VALIDATION CROIS√âE STRICT pour d√©tecter les contaminations et erreurs.
        
        V√©rifie la coh√©rence entre les diff√©rentes sources d'extraction.
        """
        logger.info(f"üîç VALIDATION CROIS√âE pour {pdf_path.name}")
        
        # 1. V√©rifier que les propri√©taires extraits sont coh√©rents avec le PDF
        valid_names = []
        suspicious_names = []
        
        for owner in owners:
            nom = owner.get('nom', '').strip()
            prenom = owner.get('prenom', '').strip()
            
            # D√©tecter les noms suspects (trop courts, num√©riques, etc.)
            if len(nom) < 2 or nom.isdigit():
                suspicious_names.append(f"{nom} {prenom}")
            else:
                valid_names.append(f"{nom} {prenom}")
        
        # 2. V√©rifier la coh√©rence g√©ographique
        departments = set()
        communes = set()
        for owner in owners:
            dept = owner.get('department', '').strip()
            comm = owner.get('commune', '').strip()
            if dept and dept.isdigit():
                departments.add(dept)
            if comm and comm.isdigit():
                communes.add(comm)
        
        # Alerte si trop de d√©partements/communes diff√©rents (signe de contamination)
        if len(departments) > 2 or len(communes) > 2:
            logger.warning(f"‚ö†Ô∏è CONTAMINATION G√âOGRAPHIQUE D√âTECT√âE:")
            logger.warning(f"   - D√©partements: {departments}")
            logger.warning(f"   - Communes: {communes}")
            logger.warning(f"   - PDF: {pdf_path.name}")
            return False
        
        # 3. V√©rifier la coh√©rence num√©rique
        expected_count = len(structured_data.get('prop_batie', [])) + len(structured_data.get('non_batie', []))
        actual_count = len(owners)
        
        if expected_count > 0 and actual_count > (expected_count * 3):  # Plus de 3x = suspect
            logger.warning(f"‚ö†Ô∏è EXPLOSION NUM√âRIQUE D√âTECT√âE:")
            logger.warning(f"   - Attendu: ~{expected_count} propri√©taires")
            logger.warning(f"   - Extrait: {actual_count} propri√©taires")
            logger.warning(f"   - Ratio: {actual_count/expected_count:.1f}x")
            return False
        
        logger.info(f"‚úÖ Validation r√©ussie: {len(valid_names)} propri√©taires valides, {len(suspicious_names)} suspects")
        return True

    def clean_contaminated_data(self, owners: List[Dict], pdf_path: Path) -> List[Dict]:
        """
        üßΩ NETTOYAGE DES DONN√âES CONTAMIN√âES d√©tect√©es.
        
        Supprime les propri√©taires qui ne correspondent pas au PDF actuel.
        """
        logger.info(f"üßΩ NETTOYAGE CONTAMINATION pour {pdf_path.name}")
        
        if not owners:
            return owners
        
        # 1. Extraire la r√©f√©rence g√©ographique du nom du fichier si possible
        filename_parts = pdf_path.stem.split()
        reference_codes = []
        for part in filename_parts:
            if part.isdigit() and len(part) >= 4:  # Codes comme "51179"
                reference_codes.append(part[:2])  # D√©partement
                reference_codes.append(part[2:])  # Commune
        
        # 2. Trouver la g√©ographie majoritaire dans les propri√©taires
        geo_counts = {}
        for owner in owners:
            dept = owner.get('department', '').strip()
            comm = owner.get('commune', '').strip()
            if dept and comm and dept.isdigit() and comm.isdigit():
                geo_key = f"{dept}-{comm}"
                geo_counts[geo_key] = geo_counts.get(geo_key, 0) + 1
        
        if not geo_counts:
            logger.warning("‚ö†Ô∏è Aucune g√©ographie d√©tect√©e dans les propri√©taires")
            return owners
        
        # 3. Prendre la g√©ographie majoritaire comme r√©f√©rence
        main_geo = max(geo_counts.items(), key=lambda x: x[1])[0]
        main_dept, main_comm = main_geo.split('-')
        
        logger.info(f"üéØ G√©ographie de r√©f√©rence: Dept {main_dept}, Commune {main_comm}")
        
        # 4. Filtrer les propri√©taires selon la g√©ographie de r√©f√©rence
        clean_owners = []
        contaminated_count = 0
        
        for owner in owners:
            dept = owner.get('department', '').strip()
            comm = owner.get('commune', '').strip()
            
            # Garder si g√©ographie correspond OU si g√©ographie vide (propagation possible)
            if (dept == main_dept and comm == main_comm) or (not dept and not comm):
                clean_owners.append(owner)
            else:
                contaminated_count += 1
                logger.info(f"‚ùå CONTAMIN√â: {owner.get('nom', '')} {owner.get('prenom', '')} (Dept {dept}, Commune {comm})")
        
        if contaminated_count > 0:
            logger.warning(f"üßΩ NETTOYAGE: {contaminated_count} propri√©taires contamin√©s supprim√©s")
            logger.info(f"‚úÖ PROPRE: {len(clean_owners)} propri√©taires conserv√©s")
        
        return clean_owners

    def list_pdf_files(self) -> List[Path]:
        """
        Liste tous les fichiers PDF dans le dossier input.
        
        Returns:
            Liste des chemins vers les fichiers PDF
        """
        pdf_files = list(self.input_dir.glob("*.pdf"))
        logger.info(f"Trouv√© {len(pdf_files)} fichier(s) PDF dans {self.input_dir}")
        return pdf_files

    def pdf_to_images(self, pdf_path: Path) -> List[bytes]:
        """
        Convertit toutes les pages d'un PDF en images PNG.
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            
        Returns:
            Liste des bytes des images PNG ou liste vide en cas d'erreur
        """
        try:
            logger.info(f"Conversion de toutes les pages de {pdf_path.name} en images")
            
            # Ouvrir le PDF
            doc = fitz.open(pdf_path)
            
            if len(doc) == 0:
                logger.error(f"Le PDF {pdf_path.name} est vide")
                return []
            
            images = []
            
            # Traiter chaque page
            for page_num in range(len(doc)):
                try:
                    page = doc[page_num]
                    
                    # Convertir chaque page en image avec une r√©solution MAXIMALE
                    mat = fitz.Matrix(5.0, 5.0)  # ULTRA-HAUTE r√©solution pour extraction optimale
                    pix = page.get_pixmap(matrix=mat)
                    
                    # Convertir en PNG
                    img_data = pix.tobytes("png")
                    images.append(img_data)
                    
                    logger.info(f"Page {page_num + 1}/{len(doc)} convertie pour {pdf_path.name}")
                    
                except Exception as e:
                    logger.error(f"Erreur lors de la conversion de la page {page_num + 1} de {pdf_path.name}: {str(e)}")
                    continue
            
            doc.close()
            logger.info(f"Conversion r√©ussie pour {pdf_path.name}: {len(images)} page(s) trait√©e(s)")
            return images
            
        except Exception as e:
            logger.error(f"Erreur lors de la conversion de {pdf_path.name}: {str(e)}")
            return []

    def extract_info_with_gpt4o(self, image_data: bytes, filename: str) -> Optional[Dict]:
        """
        EXTRACTION ULTRA-OPTIMIS√âE pour extraire TOUTES les informations possibles.
        
        Args:
            image_data: Donn√©es de l'image en bytes
            filename: Nom du fichier pour le logging
            
        Returns:
            Dictionnaire contenant les informations extraites ou None en cas d'erreur
        """
        try:
            logger.info(f"üîç Extraction ADAPTATIVE pour {filename}")
            
            # PHASE 1: D√âTECTION AUTOMATIQUE du format PDF
            format_info = self.detect_pdf_format(image_data)
            logger.info(f"üìä Format: {format_info.get('document_type')} | √âpoque: {format_info.get('format_era')} | Layout: {format_info.get('layout')}")
            
            # Encoder l'image en base64
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # PHASE 2: PROMPT ADAPTATIF selon le format d√©tect√©
            adapted_prompt = self.adapt_extraction_prompt(format_info)
            
            # PROMPT ADAPTATIF remplace l'ancien prompt fixe
            # (L'ancien prompt ultra-d√©taill√© est maintenant dans adapt_extraction_prompt)
            logger.info(f"üéØ Utilisation strat√©gie: {format_info.get('extraction_strategy')}")
            
            # Fallback: si la d√©tection √©choue, utiliser le prompt ultra-d√©taill√© original
            if not adapted_prompt:
                adapted_prompt = """
üéØ MISSION CRITIQUE: Tu es un expert en documents cadastraux fran√ßais. Tu DOIS extraire TOUTES les informations visibles avec une pr√©cision maximale.

üìã STRAT√âGIE D'EXTRACTION EXHAUSTIVE:

1Ô∏è‚É£ LOCALISATION (priorit√© absolue):
- Cherche en HAUT du document: codes comme "51179", "25227", "89238" 
- Format: DEPARTMENT(2 chiffres) + COMMUNE(3 chiffres)
- Exemples: "51179" = d√©partement 51, commune 179
- üö® R√àGLE ABSOLUE: "commune" = UNIQUEMENT 3 chiffres ("179", "424", "025"), JAMAIS noms/lieux
- ‚ùå INTERDIT dans commune: "LES PREMIERS SAPINS", "MAILLY-LE-CHATEAU", "91", "25"
- ‚úÖ CORRECT dans commune: "179", "424", "238" (exactement 3 chiffres)
- Le nom de commune va dans "commune_nom" s√©par√©ment

2Ô∏è‚É£ PROPRI√âTAIRES (scan complet):
- Noms en MAJUSCULES: [NOM1], [NOM2], [NOM3], etc.
- Pr√©noms: [Pr√©nom1], [Pr√©nom2], [Pr√©nom Multiple], etc.
- Codes MAJIC: M8BNF6, MB43HC, P7QR92 (alphanum√©riques 6 caract√®res)

3Ô∏è‚É£ PARCELLES (d√©tection fine):
- Sections: A, AB, ZY, 000ZD, etc.
- Num√©ros: 6, 0006, 123, 0123, etc.
- Contenance: 230040, 000150, 002300 (format chiffres)

4Ô∏è‚É£ ADRESSES (lecture compl√®te):
- Voies: "1 RUE D AVAT", "15 AVENUE DE LA PAIX"
- Codes postaux: 51240, 89660, 21000
- Villes: COUPEVILLE, AUXERRE, DIJON

5Ô∏è‚É£ DROITS R√âELS:
- PP = Pleine Propri√©t√©
- US = Usufruit  
- NU = Nue-propri√©t√©

üìä EXEMPLES CONCRETS DE DONN√âES R√âELLES:

EXEMPLE 1:
{
  "department": "51",
  "commune": "179",
  "commune_nom": "DAMPIERRE-SUR-MOIVRE",
  "section": "ZY",
  "numero": "0006",
  "contenance": "230040",
  "droit_reel": "US",
  "designation_parcelle": "LES ROULLIERS",
  "nom": "[NOM_PROPRIETAIRE1]",
  "prenom": "[PRENOM_MULTIPLE]",
  "numero_majic": "M8BNF6",
  "voie": "1 RUE D AVAT",
  "post_code": "51240",
  "city": "COUPEVILLE"
}

EXEMPLE 2:
{
  "department": "25",
  "commune": "227",
  "commune_nom": "BESANCON",
  "section": "000ZD",
  "numero": "0005",
  "contenance": "000150",
  "droit_reel": "PP",
  "designation_parcelle": "LE GRAND CHAMP",
  "nom": "[NOM_PROPRIETAIRE2]",
  "prenom": "[PRENOM_SIMPLE]",
  "numero_majic": "MB43HC",
  "voie": "15 RUE DE LA PAIX",
  "post_code": "25000",
  "city": "BESANCON"
}

üîç INSTRUCTIONS DE SCAN SYST√âMATIQUE:

1. Commence par scanner TOUT LE HAUT du document pour les codes d√©partement/commune
2. Lit TOUS les noms en majuscules (ce sont les propri√©taires)
3. Trouve TOUS les codes MAJIC (6 caract√®res alphanum√©riques)
4. R√©cup√®re TOUTES les adresses compl√®tes
5. Identifie TOUTES les sections et num√©ros de parcelles
6. Collecte TOUTES les contenances (surfaces)
7. ‚≠ê PR√âFIXE (TR√àS RARE mais CRUCIAL) : Cherche activement les pr√©fixes comme "ZY", "AB", "000AC" dans les tableaux "Propri√©t√©(s) non b√¢tie(s)" - ils apparaissent AVANT les sections !

‚ö†Ô∏è R√àGLES STRICTES:
- Si tu vois une information partiellement, INCLUS-LA quand m√™me
- Ne mets JAMAIS "N/A" - utilise "" si vraiment absent
- Scan CHAQUE ZONE du document m√©thodiquement
- IGNORE aucun d√©tail, m√™me petit
- Retourne TOUS les propri√©taires trouv√©s
- üö® S√âPARATION OBLIGATOIRE: "commune" = UNIQUEMENT code (ex: "208"), "commune_nom" = nom complet (ex: "DAMPIERRE-SUR-MOIVRE")

üì§ FORMAT DE R√âPONSE OBLIGATOIRE:
{
  "proprietes": [
    {
      "department": "XX",
      "commune": "XXX",
      "commune_nom": "NOM_COMMUNE",
      "prefixe": "",
      "section": "XXXX",
      "numero": "XXXX",
      "contenance": "XXXXXX",
      "droit_reel": "XX",
      "designation_parcelle": "LIEU-DIT",
      "nom": "NOM_PROPRIETAIRE",
      "prenom": "PRENOM_PROPRIETAIRE",
      "numero_majic": "XXXXXX",
      "voie": "ADRESSE_COMPLETE",
      "post_code": "XXXXX",
      "city": "VILLE"
    }
  ]
}

üéØ OBJECTIF: Z√âRO champ vide si l'info existe dans le document !
"""
            
            # PREMI√àRE PASSE: Extraction principale ultra-d√©taill√©e
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": adapted_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4000,
                temperature=0.0
            )
            
            # Parser la r√©ponse JSON
            response_text = response.choices[0].message.content.strip()
            
            # Nettoyer la r√©ponse
            if response_text.startswith('```json'):
                response_text = response_text[7:-3]
            elif response_text.startswith('```'):
                response_text = response_text[3:-3]
            
            main_result = safe_json_parse(response_text, f"extraction principale {filename}")
            
            if main_result and "proprietes" in main_result and main_result["proprietes"]:
                properties = main_result["proprietes"]
                logger.info(f"Extraction principale: {len(properties)} propri√©t√©(s) pour {filename}")
                
                # ‚úÖ NETTOYAGE IMM√âDIAT des codes commune (CORRECTION CRITIQUE)
                for prop in properties:
                    if prop.get('commune'):
                        original_commune = prop['commune']
                        cleaned_commune = clean_commune_code(original_commune)
                        if cleaned_commune != original_commune:
                            prop['commune'] = cleaned_commune
                            logger.debug(f"üßπ Commune nettoy√©e: '{original_commune}' ‚Üí '{cleaned_commune}'")
                
                # DEUXI√àME PASSE: R√©cup√©ration des champs manquants
                enhanced_properties = self.enhance_missing_fields(properties, base64_image, filename)
                
                if enhanced_properties:
                    logger.info(f"Extraction ULTRA-OPTIMIS√âE termin√©e: {len(enhanced_properties)} propri√©t√©(s) pour {filename}")
                    return {"proprietes": enhanced_properties}
                else:
                    return main_result
            else:
                logger.warning(f"Extraction principale sans r√©sultat pour {filename}")
                # PASSE DE SECOURS: Extraction d'urgence
                return self.emergency_extraction(base64_image, filename)
                
        except Exception as e:
            logger.error(f"Erreur extraction pour {filename}: {e}")
            return None

    def enhance_missing_fields(self, properties: List[Dict], base64_image: str, filename: str) -> List[Dict]:
        """
        DEUXI√àME PASSE: Am√©lioration cibl√©e des champs manquants.
        """
        try:
            # Analyser quels champs sont manquants
            missing_fields = {}
            critical_fields = ['department', 'commune', 'nom', 'prenom', 'section', 'numero']
            
            for prop in properties:
                for field in critical_fields:
                    if not prop.get(field) or prop.get(field) == "":
                        if field not in missing_fields:
                            missing_fields[field] = 0
                        missing_fields[field] += 1
            
            if not missing_fields:
                logger.info(f"‚úÖ Tous les champs critiques pr√©sents pour {filename}")
                return properties
            
            logger.info(f"üîç R√©cup√©ration cibl√©e pour {filename}: {missing_fields}")
            
            # Prompt cibl√© pour les champs manquants les plus critiques
            if 'department' in missing_fields or 'commune' in missing_fields:
                properties = self.extract_location_info(properties, base64_image, filename)
            
            if 'nom' in missing_fields or 'prenom' in missing_fields:
                properties = self.extract_owner_info(properties, base64_image, filename)
            
            return properties
            
        except Exception as e:
            logger.error(f"Erreur enhancement pour {filename}: {e}")
            return properties

    def extract_location_info(self, properties: List[Dict], base64_image: str, filename: str) -> List[Dict]:
        """
        üîß NOUVELLE VERSION: Extraction d√©partement/commune avec pdfplumber + ChatGPT.
        Beaucoup plus fiable que l'analyse d'image.
        """
        try:
            # Convertir filename en Path si n√©cessaire
            if isinstance(filename, str):
                # Si c'est juste un nom de fichier, chercher dans input/
                pdf_path = Path(self.input_dir) / filename
                if not pdf_path.exists():
                    # Essayer d'autres chemins possibles
                    for possible_path in [Path(filename), Path(self.input_dir) / f"{filename}.pdf"]:
                        if possible_path.exists():
                            pdf_path = possible_path
                            break
            else:
                pdf_path = filename
            
            if not pdf_path.exists():
                logger.warning(f"‚ö†Ô∏è Fichier PDF introuvable pour extraction en-t√™te: {pdf_path}")
                return properties
            
            # √âtape 1: Extraire le texte de l'en-t√™te avec pdfplumber
            header_text = self.extract_header_text_with_pdfplumber(pdf_path)
            
            if not header_text:
                logger.warning(f"‚ö†Ô∏è Impossible d'extraire l'en-t√™te textuel: {pdf_path.name}")
                return properties
            
            # √âtape 2: Analyser l'en-t√™te avec ChatGPT
            location_data = self.parse_header_text_with_gpt(header_text, pdf_path.name)
            
            if not location_data:
                logger.warning(f"‚ö†Ô∏è √âchec analyse en-t√™te: {pdf_path.name}")
                return properties
            
            # √âtape 3: Appliquer les donn√©es extraites aux propri√©t√©s
            dept = location_data.get("department")
            commune = location_data.get("commune")
            
            if dept or commune:
                # üö® PROPAGATION FORC√âE d√©partement/commune depuis en-t√™te
                missing_commune_count = 0
                missing_dept_count = 0
                
                for prop in properties:
                    if not prop.get("department") and dept:
                        prop["department"] = dept
                        missing_dept_count += 1
                    if not prop.get("commune") and commune:
                        # ‚úÖ NETTOYAGE imm√©diat du code commune
                        cleaned_commune = clean_commune_code(commune)
                        prop["commune"] = cleaned_commune
                        missing_commune_count += 1
                        if cleaned_commune != commune:
                            logger.debug(f"üßπ Commune nettoy√©e: '{commune}' ‚Üí '{cleaned_commune}'")
                
                if missing_commune_count > 0:
                    logger.info(f"üîÑ PROPAGATION FORC√âE: commune '{commune}' ajout√©e √† {missing_commune_count} propri√©t√©s")
                if missing_dept_count > 0:
                    logger.info(f"üîÑ PROPAGATION FORC√âE: d√©partement '{dept}' ajout√© √† {missing_dept_count} propri√©t√©s")
                
                logger.info(f"‚úÖ En-t√™te trait√© avec pdfplumber: dept={dept}, commune={commune} ‚Üí {len(properties)} propri√©t√©s")
            else:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e g√©ographique trouv√©e dans l'en-t√™te: {pdf_path.name}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction en-t√™te pdfplumber: {e}")
        
        return properties

    def extract_header_text_with_pdfplumber(self, pdf_path: Path) -> str:
        """
        Extrait le texte brut de l'en-t√™te du PDF avec pdfplumber.
        Beaucoup plus fiable que l'analyse d'image.
        """
        try:
            with pdfplumber.open(pdf_path) as pdf:
                if not pdf.pages:
                    return ""
                
                first_page = pdf.pages[0]
                full_text = first_page.extract_text()
                
                if not full_text:
                    return ""
                
                # Extraire seulement les premi√®res lignes (en-t√™te)
                lines = full_text.split('\n')
                header_lines = lines[:20]  # Plus de lignes pour capture √©largie
                
                # ‚úÖ NOUVEAU - Extraction beaucoup plus large
                relevant_lines = []
                import re
                
                for line in header_lines:
                    line_clean = line.strip()
                    # Garder TOUTES les lignes contenant des chiffres (codes g√©ographiques potentiels)
                    if (any(keyword in line_clean.upper() for keyword in 
                           ['D√âPARTEMENT', 'COMMUNE', 'RELEV√â', 'PROPRI√âT√â', 'ANN√âE', 'CADASTRE']) or
                        # OU lignes avec codes num√©riques (d√©partement/commune)
                        re.search(r'\b\d{2,5}\b', line_clean) or
                        # OU lignes avec noms de communes fran√ßaises typiques
                        any(word in line_clean.upper() for word in ['MAILLY', 'CH√ÇTEAU', 'SAINT', 'SUR', 'LES', 'LE'])):
                        relevant_lines.append(line_clean)
                
                header_text = '\n'.join(relevant_lines)
                logger.debug(f"üìÑ En-t√™te extrait: {header_text[:100]}...")
                return header_text
                
        except Exception as e:
            logger.warning(f"Erreur extraction en-t√™te pdfplumber: {e}")
            return ""

    def parse_header_text_with_gpt(self, header_text: str, filename: str) -> Dict:
        """
        Analyse le texte d'en-t√™te avec ChatGPT pour extraire d√©partement et commune.
        """
        if not header_text.strip():
            return {}
            
        try:
            header_prompt = f"""
üéØ ANALYSE INTELLIGENTE d'en-t√™te cadastral fran√ßais.

üìÑ TEXTE √Ä ANALYSER:
{header_text}

üîç RECHERCHE MULTI-FORMAT:
1. Format classique: "D√©partement : 89" + "Commune : 238"
2. Format condens√©: "89238" ou "89 238"  
3. Format avec nom: "238 MAILLY-LE-CHATEAU"
4. Format mixte: "D√©partement 89 Commune 238"

üìã STRAT√âGIE:
- Trouve TOUS les nombres de 2-5 chiffres
- Identifie le d√©partement (2 premiers chiffres)
- Identifie la commune (3 chiffres suivants OU s√©par√©s)
- Si nom de commune pr√©sent, extrais le code qui pr√©c√®de

üì§ EXEMPLES DE D√âTECTION:
Input: "D√©partement : 89   Commune : 238 MAILLY-LE-CHATEAU"
Output: {{"department": "89", "commune": "238"}}

Input: "89238 MAILLY-LE-CHATEAU Ann√©e 2024"  
Output: {{"department": "89", "commune": "238"}}

Input: "Cadastre 51179 ZY Propri√©t√©s"
Output: {{"department": "51", "commune": "179"}}

‚ö†Ô∏è R√àGLES:
- TOUJOURS chercher les codes num√©riques
- Si plusieurs possibilit√©s, prendre la premi√®re
- Si aucun code trouv√©, retourner {{"department": null, "commune": null}}
"""

            response = self.client.chat.completions.create(
                model="gpt-4o-mini",  # Plus rapide et moins cher pour analyse textuelle
                messages=[
                    {"role": "user", "content": header_prompt}
                ],
                max_tokens=200,
                temperature=0.0
            )
            
            response_text = response.choices[0].message.content.strip()
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0].strip()
            
            location_data = safe_json_parse(response_text, f"analyse en-t√™te {filename}")
            
            if location_data:
                dept = location_data.get("department")
                commune = location_data.get("commune")
                logger.info(f"‚úÖ En-t√™te analys√©: dept={dept}, commune={commune}")
                return location_data
            
        except Exception as e:
            logger.warning(f"Erreur analyse en-t√™te ChatGPT: {e}")
        
        return {}

    def extract_owner_info(self, properties: List[Dict], base64_image: str, filename: str) -> List[Dict]:
        """Extraction cibl√©e des informations propri√©taires."""
        try:
            owner_prompt = """
üéØ MISSION: Trouve TOUS les propri√©taires dans ce document cadastral.

üîç RECHERCHE SYST√âMATIQUE:
- Noms en MAJUSCULES: [NOM1], [NOM2], [NOM3], [NOM4], etc.
- Pr√©noms: [Pr√©nom1], [Pr√©nom2], [Pr√©nom3], [Pr√©nom Multiple], etc.
- Codes MAJIC: M8BNF6, MB43HC, P7QR92 (6 caract√®res alphanum√©riques)
- Adresses compl√®tes: "1 RUE D AVAT", "15 AVENUE DE LA PAIX"

üì§ FORMAT R√âPONSE:
{
  "owners": [
    {
      "nom": "NOM_MAJUSCULE",
      "prenom": "Pr√©nom Complet",
      "numero_majic": "XXXXXX",
      "voie": "Adresse compl√®te",
      "post_code": "XXXXX",
      "city": "VILLE"
    }
  ]
}

‚ö†Ô∏è Scan TOUT le document pour les propri√©taires !
"""
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": owner_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.0
            )
            
            owner_text = response.choices[0].message.content.strip()
            if "```json" in owner_text:
                owner_text = owner_text.split("```json")[1].split("```")[0].strip()
            
            owner_data = safe_json_parse(owner_text, f"extraction propri√©taires {filename}")
            
            if not owner_data:
                logger.warning(f"√âchec parsing propri√©taires pour {filename}")
                return properties
            
            if "owners" in owner_data and owner_data["owners"]:
                owners = owner_data["owners"]
                
                # Fusionner avec les propri√©t√©s existantes
                for i, prop in enumerate(properties):
                    if i < len(owners):
                        owner = owners[i]
                        for field in ['nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city']:
                            if not prop.get(field) and owner.get(field):
                                prop[field] = owner[field]
                
                logger.info(f"‚úÖ Propri√©taires r√©cup√©r√©s: {len(owners)}")
        
        except Exception as e:
            logger.warning(f"Erreur extraction propri√©taires: {e}")
        
        return properties

    def emergency_extraction(self, base64_image: str, filename: str) -> Optional[Dict]:
        """
        EXTRACTION D'URGENCE: Derni√®re tentative avec prompt ultra-simple.
        """
        try:
            logger.info(f"üö® Extraction d'urgence pour {filename}")
            
            emergency_prompt = """
Regarde ce document cadastral fran√ßais et trouve:
1. Tous les NOMS en MAJUSCULES
2. Tous les codes √† 5 chiffres (d√©partement+commune)
3. Toutes les sections (lettres comme A, ZY)
4. Tous les num√©ros de parcelles

Retourne TOUT ce que tu vois en JSON:
{
  "proprietes": [
    {
      "department": "",
      "commune": "",
      "commune_nom": "",
      "section": "",
      "numero": "",
      "nom": "",
      "prenom": "",
      "voie": "",
      "city": ""
    }
  ]
}
"""
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": emergency_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.1
            )
            
            emergency_text = response.choices[0].message.content.strip()
            if "```json" in emergency_text:
                emergency_text = emergency_text.split("```json")[1].split("```")[0].strip()
            
            result = safe_json_parse(emergency_text, f"extraction urgence {filename}")
            
            # ‚úÖ NETTOYAGE des codes commune dans les r√©sultats d'urgence
            if result and "proprietes" in result:
                for prop in result["proprietes"]:
                    if prop.get('commune'):
                        original_commune = prop['commune']
                        cleaned_commune = clean_commune_code(original_commune)
                        if cleaned_commune != original_commune:
                            prop['commune'] = cleaned_commune
                            logger.debug(f"üßπ Commune nettoy√©e (urgence): '{original_commune}' ‚Üí '{cleaned_commune}'")
            
            if not result:
                logger.error(f"√âchec total extraction pour {filename}")
                return None
            if "proprietes" in result and result["proprietes"]:
                logger.info(f"üÜò Extraction d'urgence r√©ussie: {len(result['proprietes'])} propri√©t√©(s)")
                return result
            
        except Exception as e:
            logger.error(f"√âchec extraction d'urgence pour {filename}: {e}")
        
        return None

    def generate_unique_id(self, department: str, commune: str, section: str, numero: str, prefixe: str = "") -> str:
        """
        G√âN√âRATION D'ID FORMAT CADASTRAL FRAN√áAIS - 14 CARACT√àRES (NOUVELLE VERSION)
        
        Format selon sp√©cifications client :
        D√âPARTEMENT(2) + COMMUNE(3) + SECTION_AVEC_PR√âFIXE(5) + NUM√âRO(4) = 14 caract√®res
        
        NOUVELLE R√àGLE : Le pr√©fixe s'int√®gre DANS la section (5 caract√®res total)
        - Sans pr√©fixe : Section "A" ‚Üí "0000A", Section "ZD" ‚Üí "000ZD"
        - Avec pr√©fixe : Pr√©fixe "302" + Section "A" ‚Üí "3020A"
        - Avec pr√©fixe : Pr√©fixe "12" + Section "A" ‚Üí "1200A"
        
        Args:
            department: Code d√©partement 
            commune: Code commune
            section: Section cadastrale (ex: "A", "ZC")
            numero: Num√©ro de parcelle
            prefixe: Pr√©fixe (peut √™tre vide, max 3 caract√®res)
            
        Returns:
            ID unique format√© sur EXACTEMENT 14 caract√®res
        """
        # √âTAPE 1: D√©partement - EXACTEMENT 2 caract√®res
        dept = str(department or "00").strip()
        if dept == "N/A" or not dept:
            dept = "00"
        dept = dept.zfill(2)[:2]  # Z√©ros √† gauche, max 2 caract√®res
        
        # √âTAPE 2: Commune - EXACTEMENT 3 caract√®res  
        comm = str(commune or "000").strip()
        if comm == "N/A" or not comm:
            comm = "000"
        comm = comm.zfill(3)[:3]  # Z√©ros √† gauche, max 3 caract√®res
        
        # √âTAPE 3: Section avec pr√©fixe int√©gr√© - EXACTEMENT 5 caract√®res
        # Nettoyer la section et d√©tecter si elle contient d√©j√† le pr√©fixe
        if section and str(section).strip() and section != "N/A":
            sect_raw = str(section).strip().upper()
            
            # Cas sp√©cial : section contient d√©j√† le pr√©fixe avec espace (ex: "302 A")
            if ' ' in sect_raw and prefixe and str(prefixe).strip():
                parts = sect_raw.split(' ', 1)
                if len(parts) == 2 and parts[0] == str(prefixe).strip():
                    # La section contient d√©j√† le pr√©fixe, extraire la vraie section
                    sect = parts[1]
                    logger.debug(f"üîç Section avec pr√©fixe d√©tect√©e: '{sect_raw}' ‚Üí section='{sect}' pr√©fixe='{prefixe}'")
                else:
                    sect = sect_raw
            else:
                sect = sect_raw
        else:
            sect = "A"  # Section par d√©faut
        
        # Nettoyer le pr√©fixe
        if prefixe and str(prefixe).strip() and prefixe != "N/A":
            pref = str(prefixe).strip()
        else:
            pref = ""  # Pas de pr√©fixe
        
        # Construire la section avec pr√©fixe (5 caract√®res total)
        if pref:
            # Avec pr√©fixe : pr√©fixe + z√©ros de padding + section = 5 caract√®res
            combined_length = len(pref) + len(sect)
            if combined_length <= 5:
                # Ajouter des z√©ros entre le pr√©fixe et la section
                padding_zeros = "0" * (5 - combined_length)
                section_final = f"{pref}{padding_zeros}{sect}"
            else:
                # Si trop long, tronquer la section
                available_for_section = 5 - len(pref)
                if available_for_section > 0:
                    section_final = f"{pref}{sect[:available_for_section]}"
                else:
                    # Pr√©fixe trop long, le tronquer
                    section_final = pref[:5]
        else:
            # Sans pr√©fixe : compl√©ter avec des z√©ros √† gauche (format cadastral)
            section_final = sect.zfill(5)
        
        # Validation section finale
        if len(section_final) != 5:
            logger.warning(f"üîß Section finale corrig√©e: '{section_final}' ‚Üí longueur ajust√©e")
            section_final = (section_final + "00000")[:5]  # Force 5 caract√®res
        
        # √âTAPE 4: Num√©ro - EXACTEMENT 4 caract√®res  
        if numero and str(numero).strip() and numero != "N/A":
            num = str(numero).strip()
            # Extraire les chiffres et compl√©ter avec des z√©ros √† gauche
            num_clean = ''.join(filter(str.isdigit, num))
            if num_clean:
                num = num_clean.zfill(4)[-4:]  # Derniers 4 chiffres si trop long
            else:
                num = "0001"  # Num√©ro par d√©faut si pas de chiffres
        else:
            num = "0001"  # Num√©ro par d√©faut
        
        # Validation num√©ro
        if len(num) != 4:
            logger.warning(f"üîß Num√©ro corrig√©: '{numero}' ‚Üí '{num}' (longueur: {len(num)})")
            num = (num + "0000")[:4]  # Force correction d'urgence
        
        # ASSEMBLAGE FINAL : DEPT(2) + COMM(3) + SECTION_AVEC_PR√âFIXE(5) + NUM√âRO(4) = 14 caract√®res
        unique_id = f"{dept}{comm}{section_final}{num}"
        expected_length = 14
        
        # VALIDATION FINALE
        if len(unique_id) != expected_length:
            logger.error(f"üö® ID LONGUEUR CRITIQUE: '{unique_id}' = {len(unique_id)} caract√®res (devrait √™tre {expected_length})")
            logger.error(f"üîç ANALYSE: dept='{dept}'({len(dept)}) comm='{comm}'({len(comm)}) section='{section_final}'({len(section_final)}) num='{num}'({len(num)})")
            
            # CORRECTION FORC√âE
            if len(unique_id) < expected_length:
                unique_id = unique_id.ljust(expected_length, '0')
                logger.warning(f"üîß ID COMPL√âT√â: '{unique_id}'")
            elif len(unique_id) > expected_length:
                unique_id = unique_id[:expected_length]
                logger.warning(f"üîß ID TRONQU√â: '{unique_id}'")
        
        # ASSERTION FINALE - Garantie absolue 14 caract√®res
        if len(unique_id) != expected_length:
            raise ValueError(f"ERREUR FATALE: ID '{unique_id}' = {len(unique_id)} caract√®res (devrait √™tre {expected_length})")
        
        logger.debug(f"‚úÖ ID 14 CARACT√àRES NOUVEAU FORMAT: '{unique_id}' (dept:{dept} comm:{comm} section_avec_pr√©fixe:{section_final} num:{num})")
        return unique_id

    def extract_tables_with_pdfplumber(self, pdf_path: Path) -> Dict:
        """
        EXTRACTION HYBRIDE √âTAPE 1: Extraction des tableaux structur√©s avec pdfplumber.
        R√©plique exactement l'approche du code Make/Python Anywhere.
        """
        logger.info(f"üìã Extraction tableaux pdfplumber pour {pdf_path.name}")
        
        try:
            prop_batie = []
            non_prop_batie = []
            contenance_totale = {}
            property_batie_in_new_page = False
            
            with pdfplumber.open(pdf_path) as pdf:
                # Parcourir toutes les pages
                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        if not table or not table[0]:
                            continue
                            
                        # D√©tecter les tableaux de propri√©t√©s b√¢ties
                        if table[0][0] == 'Propri√©t√©(s) b√¢tie(s)':
                            logger.info(f"üìä Trouv√© tableau propri√©t√©s b√¢ties")
                            property_batie_in_new_page = True
                            prop_batie = self.extract_property_batie(table)
                        
                        # D√©tecter les tableaux de propri√©t√©s non b√¢ties
                        elif table[0][0] == 'Propri√©t√©(s) non b√¢tie(s)':
                            logger.info(f"üìä Trouv√© tableau propri√©t√©s non b√¢ties")
                            prop_non_batie_dict = self.extract_property_non_batie(table)
                            non_prop_batie.extend(prop_non_batie_dict)
                        
                        # NOUVEAU: D√©tecter le tableau "Contenance totale"
                        elif any(row and 'Contenance totale' in str(row[0]) for row in table if row):
                            logger.info(f"üéØ Trouv√© tableau Contenance totale")
                            contenance_totale = self.extract_contenance_totale(table)
                            # Appliquer les valeurs HA, A, CA aux propri√©t√©s d√©j√† extraites
                            if contenance_totale:
                                self.apply_contenance_totale_to_properties(non_prop_batie, contenance_totale)
                
                # Fallback: chercher dans la premi√®re page si pas trouv√© ailleurs
                if not property_batie_in_new_page and pdf.pages:
                    first_page_tables = pdf.pages[0].extract_tables()
                    if first_page_tables:
                        for idx, row in enumerate(first_page_tables[0]):
                            if row and row[0] == 'Propri√©t√©(s) b√¢tie(s)':
                                property_batie_table = first_page_tables[0][idx:]
                                prop_batie = self.extract_property_batie(property_batie_table)
            
            logger.info(f"‚úÖ pdfplumber: {len(prop_batie)} b√¢ties, {len(non_prop_batie)} non b√¢ties")
            return {
                "prop_batie": prop_batie,
                "non_batie": non_prop_batie
            }
            
        except Exception as e:
            logger.error(f"Erreur pdfplumber pour {pdf_path.name}: {e}")
            return {"prop_batie": [], "non_batie": []}

    def extract_property_batie(self, table: List[List]) -> List[Dict]:
        """Extraction des propri√©t√©s b√¢ties (r√©plique du code Make)."""
        if len(table) < 3:
            return []
            
        property_rows = table[2:]
        headers = property_rows[0]
        clean_headers = [header.replace('\n', ' ').strip() if header is not None else '' for header in headers]
        property_dicts = []

        if len(property_rows) > 1 and property_rows[1] and "Total" not in str(property_rows[1][0]):
            for row in property_rows[1:]:
                if row and "Total" in str(row[0]):
                    break
                
                property_dict = {
                    clean_headers[i]: row[i] if i < len(row) else None 
                    for i in range(len(clean_headers))
                }
                property_dicts.append(property_dict)
        
        return property_dicts

    def extract_property_non_batie(self, table: List[List]) -> List[Dict]:
        """Extraction des propri√©t√©s non b√¢ties (r√©plique du code Make)."""
        if len(table) < 3:
            return []
            
        property_rows = table[2:]
        headers = property_rows[0]
        clean_headers = [header.replace('\n', ' ').strip() if header is not None else '' for header in headers]
        property_dicts = []

        # Chercher les en-t√™tes HA, A, CA suppl√©mentaires
        ha_pos = None
        a_pos = None
        ca_pos = None
        
        # V√©rifier s'il y a une ligne avec HA, A, CA apr√®s les en-t√™tes principaux
        for i, row in enumerate(property_rows):
            if row:
                for j, cell in enumerate(row):
                    if cell == 'HA':
                        ha_pos = j
                    elif cell == 'A':
                        a_pos = j
                    elif cell == 'CA':
                        ca_pos = j
                
                # Si on a trouv√© HA, A, CA dans cette ligne
                if ha_pos is not None and a_pos is not None and ca_pos is not None:
                    logger.info(f"üéØ En-t√™tes HA/A/CA trouv√©s dans tableau non b√¢ties aux positions {ha_pos}, {a_pos}, {ca_pos}")
                    break

        if len(property_rows) > 2 and property_rows[2] and "totale" not in str(property_rows[2][0]).lower():
            for row in property_rows[2:]:
                if row and "totale" in str(row[0]).lower():
                    break
                
                property_dict = {
                    clean_headers[i]: row[i] if i < len(row) else None 
                    for i in range(len(clean_headers))
                }
                
                # üîß CORRECTION SECTION : Debug et recherche am√©lior√©e
                section_value = None
                
                # Debug : afficher toute la ligne pour diagnostic
                logger.debug(f"DEBUG Ligne compl√®te: {row}")
                
                # M√©thode 1 : Par en-t√™te (existante)
                for i, header in enumerate(clean_headers):
                    if header and ('Sec' in header or 'Section' in header) and i < len(row):
                        if row[i]:
                            section_value = str(row[i]).strip()
                            logger.debug(f"Section par en-t√™te '{header}': '{section_value}'")
                            break
                
                # M√©thode 2 : Si section vide ou trop courte, chercher dans toutes les cellules
                import re
                if not section_value or len(section_value) < 2:
                    for cell in row:
                        if cell:
                            cell_str = str(cell).strip()
                            # Rechercher pattern : 1-2 lettres majuscules (ZK, AA, C, etc.)
                            if re.match(r'^[A-Z]{1,2}$', cell_str):
                                section_value = cell_str
                                logger.debug(f"Section trouv√©e par regex: '{section_value}'")
                                break
                
                if section_value:
                    property_dict['Sec'] = section_value
                    logger.info(f"Section finale extraite: '{section_value}'")
                else:
                    logger.warning(f"Section non trouv√©e dans la ligne")
                
                # Ajouter les valeurs HA, A, CA si elles existent
                if ha_pos is not None and ha_pos < len(row) and row[ha_pos]:
                    property_dict['HA'] = str(row[ha_pos]).strip()
                if a_pos is not None and a_pos < len(row) and row[a_pos]:
                    property_dict['A'] = str(row[a_pos]).strip()
                if ca_pos is not None and ca_pos < len(row) and row[ca_pos]:
                    property_dict['CA'] = str(row[ca_pos]).strip()
                
                property_dicts.append(property_dict)
        
        return property_dicts

    def extract_contenance_totale(self, table: List[List]) -> Dict:
        """Extraction du tableau 'Contenance totale' avec colonnes HA, A, CA"""
        try:
            # Chercher les en-t√™tes HA, A, CA ET la ligne de donn√©es correspondante
            contenance_data = {}
            
            for i, row in enumerate(table):
                if not row:
                    continue
                    
                # Chercher une ligne avec exactement HA, A, CA cons√©cutifs
                ha_pos = None
                a_pos = None  
                ca_pos = None
                
                for j, cell in enumerate(row):
                    if cell == 'HA':
                        ha_pos = j
                    elif cell == 'A' and j > 0 and row[j-1] == 'HA':  # A juste apr√®s HA
                        a_pos = j
                    elif cell == 'CA' and j > 1 and row[j-1] == 'A' and row[j-2] == 'HA':  # CA apr√®s A apr√®s HA
                        ca_pos = j
                
                # Si on a trouv√© les trois colonnes cons√©cutives
                if ha_pos is not None and a_pos is not None and ca_pos is not None:
                    logger.info(f"üéØ En-t√™tes HA/A/CA trouv√©s aux positions {ha_pos}, {a_pos}, {ca_pos}")
                    
                    # Chercher les donn√©es dans la m√™me ligne ou les lignes suivantes
                    for data_row_idx in range(i, min(i + 3, len(table))):  # Chercher dans les 3 lignes suivantes
                        data_row = table[data_row_idx]
                        if data_row and len(data_row) > ca_pos:
                            # V√©rifier si on a des valeurs num√©riques aux bonnes positions
                            ha_val = str(data_row[ha_pos]).strip() if ha_pos < len(data_row) and data_row[ha_pos] else ''
                            a_val = str(data_row[a_pos]).strip() if a_pos < len(data_row) and data_row[a_pos] else ''
                            ca_val = str(data_row[ca_pos]).strip() if ca_pos < len(data_row) and data_row[ca_pos] else ''
                            
                            # Si au moins une valeur est num√©rique, on prend cette ligne
                            if ha_val.isdigit() or a_val.isdigit() or ca_val.isdigit():
                                contenance_data = {
                                    'HA': ha_val,
                                    'A': a_val, 
                                    'CA': ca_val
                                }
                                logger.info(f"üéØ Contenance totale extraite: {contenance_data}")
                                return contenance_data
                    break  # On a trouv√© les en-t√™tes, pas besoin de continuer
            
            if not contenance_data:
                logger.warning("üéØ Tableau contenance totale d√©tect√© mais donn√©es non extraites")
            
            return contenance_data
            
        except Exception as e:
            logger.warning(f"Erreur extraction contenance totale: {e}")
            return {}

    def apply_contenance_totale_to_properties(self, properties: List[Dict], contenance_totale: Dict):
        """Applique les valeurs HA, A, CA du tableau contenance totale aux propri√©t√©s"""
        if not contenance_totale or not properties:
            return
            
        logger.info(f"üîÑ Application contenance totale √† {len(properties)} propri√©t√©(s)")
        
        for prop in properties:
            # Ajouter les valeurs HA, A, CA si elles n'existent pas d√©j√†
            if 'HA' not in prop and 'HA' in contenance_totale:
                prop['HA'] = contenance_totale['HA']
            if 'A' not in prop and 'A' in contenance_totale:
                prop['A'] = contenance_totale['A']
            if 'CA' not in prop and 'CA' in contenance_totale:
                prop['CA'] = contenance_totale['CA']

    def extract_owners_with_vision_simple(self, pdf_path: Path) -> List[Dict]:
        """
        EXTRACTION HYBRIDE √âTAPE 2: Extraction des propri√©taires avec OpenAI Vision.
        Utilise le prompt simplifi√© du style Make.
        """
        logger.info(f"üë§ Extraction propri√©taires OpenAI pour {pdf_path.name}")
        
        # Convertir PDF en images
        images = self.pdf_to_images(pdf_path)
        if not images:
            return []
        
        all_owners = []
        
        for page_num, image_data in enumerate(images, 1):
            try:
                # Encoder l'image
                base64_image = base64.b64encode(image_data).decode('utf-8')
                
                # PROMPT AM√âLIOR√â - EXTRACTION SYST√âMATIQUE COMPL√àTE
                simple_prompt = """üö® EXTRACTION SYST√âMATIQUE COMPL√àTE REQUISE üö®

MISSION CRITIQUE: Tu dois extraire TOUTES les lignes de donn√©es propri√©taires pr√©sentes dans ce tableau cadastral fran√ßais. 

üìã M√âTHODE SYST√âMATIQUE OBLIGATOIRE:
1. COMPTE d'abord le nombre total de lignes dans le(s) tableau(x)
2. LIS syst√©matiquement CHAQUE ligne de donn√©es de haut en bas
3. EXTRAIS TOUTES les informations pour CHAQUE ligne trouv√©e
4. Ne JAMAIS arr√™ter apr√®s quelques lignes - CONTINUE jusqu'√† la fin
5. V√âRIFIE que ton extraction contient le m√™me nombre d'entr√©es que de lignes dans le tableau

‚ö†Ô∏è ATTENTION: Ce document peut contenir des tableaux avec plusieurs dizaines de lignes. Tu DOIS toutes les extraire.

üéØ INFORMATIONS √Ä EXTRAIRE pour CHAQUE ligne:
- nom (en MAJUSCULES g√©n√©ralement)
- prenom (souvent apr√®s le nom)
- street_address (adresse compl√®te rue/num√©ro)
- city (ville)
- post_code (code postal)
- numero_proprietaire (code g√©n√©ralement 6 caract√®res)
- department (d√©partement, garder les z√©ros de d√©but)
- commune (üö® OBLIGATOIRE: UNIQUEMENT le code √† 3 chiffres, exemple "238", JAMAIS le nom "MAILLY-LE-CHATEAU")
- droit_reel (type de propri√©t√©: Propri√©taire, Usufruitier, Nu-propri√©taire, etc.)

üîç R√àGLES DE QUALIT√â:
- Si une ligne est incompl√®te, extrait quand m√™me ce qui est disponible
- Conserve TOUS les z√©ros de d√©but pour department et commune
- S√©pare correctement rue/ville/code postal dans l'adresse
- Ne jamais ignorer une ligne sous pr√©texte qu'elle manque d'info

üö® R√àGLE ABSOLUE COMMUNE - ANTI-CONTAMINATION:
- commune = EXCLUSIVEMENT LE CODE √Ä 3 CHIFFRES (ex: "424", "238", "179")
- ‚ùå INTERDIT: noms de lieux ("LES PREMIERS SAPINS", "MAILLY-LE-CHATEAU") 
- ‚ùå INTERDIT: codes de d√©partements ("25", "91") dans le champ commune
- ‚úÖ AUTORIS√â: uniquement codes num√©riques 3 chiffres ("424", "025", "001")
- SI tu vois "424 LES PREMIERS SAPINS", PRENDS SEULEMENT "424"
- SI tu vois "LES PREMIERS SAPINS" sans code, cherche dans les lignes autour
- V√âRIFICATION: commune doit avoir EXACTEMENT 3 chiffres, rien d'autre

R√âPONSE JSON OBLIGATOIRE (avec TOUTES les lignes trouv√©es):
{
  "owners": [
    {
      "nom": "[NOM_PROPRIETAIRE]",
      "prenom": "[PRENOM_MULTIPLE]", 
      "street_address": "2 RUE DE LA PAIX",
      "city": "MAILLY-LE-CHATEAU",
      "post_code": "89660",
      "numero_proprietaire": "MBRWL8",
      "department": "89",
      "commune": "238",
      "droit_reel": "Propri√©taire"
    }
  ]
}

üö® VALIDATION FINALE: V√©rifie que ton array "owners" contient UNE entr√©e pour CHAQUE ligne de donn√©es du tableau !"""
                
                # Appel OpenAI (param√®tres identiques √† Make)
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": simple_prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_image}",
                                        "detail": "high"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=2048,
                    temperature=1.0,  # Exactement comme Make
                    response_format={"type": "json_object"}
                )
                
                # Parser la r√©ponse
                response_text = response.choices[0].message.content.strip()
                result = safe_json_parse(response_text, f"vision simple page {page_num}")
                if result and "owners" in result and result["owners"]:
                    all_owners.extend(result["owners"])
                    logger.info(f"Page {page_num}: {len(result['owners'])} propri√©taire(s)")
                else:
                    logger.warning(f"Pas de propri√©taires trouv√©s page {page_num}")
                    
            except Exception as e:
                logger.error(f"Erreur extraction page {page_num}: {e}")
                continue
        
        logger.info(f"üë§ Total propri√©taires extraits: {len(all_owners)}")
        return all_owners

    def merge_structured_and_vision_data(self, structured_data: Dict, owners_data: List[Dict], filename: str) -> List[Dict]:
        """
        EXTRACTION HYBRIDE √âTAPE 3: Fusion intelligente des donn√©es tableaux + propri√©taires.
        """
        logger.info(f"üîó Fusion hybride pour {filename}")
        
        merged_properties = []
        
        # R√©cup√©rer les donn√©es structur√©es
        prop_batie = structured_data.get("prop_batie", [])
        non_prop_batie = structured_data.get("non_batie", [])
        
        # Combiner toutes les propri√©t√©s structur√©es
        all_structured = prop_batie + non_prop_batie
        
        if not all_structured and not owners_data:
            logger.warning(f"Aucune donn√©e extraite pour {filename}")
            return []
        
        # Strat√©gie de fusion
        if all_structured and owners_data:
            # CAS OPTIMAL: Les deux types de donn√©es
            logger.info(f"üéØ Fusion: {len(all_structured)} propri√©t√©s + {len(owners_data)} propri√©taires")
            
            for i, structured_prop in enumerate(all_structured):
                merged_prop = self.convert_structured_to_standard_format(structured_prop)
                
                # Associer avec un propri√©taire si disponible
                if i < len(owners_data):
                    owner = owners_data[i]
                    merged_prop.update({
                        'nom': owner.get('nom', ''),
                        'prenom': owner.get('prenom', ''),
                        'numero_majic': owner.get('numero_proprietaire', ''),
                        'voie': owner.get('street_address', ''),
                        'post_code': owner.get('post_code', ''),
                        'city': owner.get('city', ''),
                        'droit_reel': owner.get('droit_reel', ''),
                        'department': owner.get('department', ''),
                        'commune': owner.get('commune', '')
                    })
                
                merged_properties.append(merged_prop)
            
            # Ajouter les propri√©taires restants s'il y en a plus
            for j in range(len(all_structured), len(owners_data)):
                owner = owners_data[j]
                merged_prop = {
                    'department': owner.get('department', ''),
                    'commune': owner.get('commune', ''),
                    'prefixe': '',
                    'section': '',
                    'numero': '',
                    'contenance': '',
                    'droit_reel': owner.get('droit_reel', ''),
                    'designation_parcelle': '',
                    'nom': owner.get('nom', ''),
                    'prenom': owner.get('prenom', ''),
                    'numero_majic': owner.get('numero_proprietaire', ''),
                    'voie': owner.get('street_address', ''),
                    'post_code': owner.get('post_code', ''),
                    'city': owner.get('city', '')
                }
                merged_properties.append(merged_prop)
                
        elif all_structured:
            # Seulement des donn√©es structur√©es
            logger.info(f"üìä Seulement donn√©es structur√©es: {len(all_structured)}")
            for structured_prop in all_structured:
                merged_prop = self.convert_structured_to_standard_format(structured_prop)
                merged_properties.append(merged_prop)
                
        elif owners_data:
            # Seulement des propri√©taires
            logger.info(f"üë§ Seulement propri√©taires: {len(owners_data)}")
            for owner in owners_data:
                merged_prop = {
                    'department': owner.get('department', ''),
                    'commune': owner.get('commune', ''),
                    'prefixe': '',
                    'section': '',
                    'numero': '',
                    'contenance': '',
                    'droit_reel': owner.get('droit_reel', ''),
                    'designation_parcelle': '',
                    'nom': owner.get('nom', ''),
                    'prenom': owner.get('prenom', ''),
                    'numero_majic': owner.get('numero_proprietaire', ''),
                    'voie': owner.get('street_address', ''),
                    'post_code': owner.get('post_code', ''),
                    'city': owner.get('city', '')
                }
                merged_properties.append(merged_prop)
        
        logger.info(f"üéâ Fusion hybride termin√©e: {len(merged_properties)} propri√©t√©s")
        return merged_properties

    def convert_structured_to_standard_format(self, structured_prop: Dict) -> Dict:
        """Convertit les donn√©es pdfplumber au format standard."""
        return {
            'department': '',
            'commune': '',
            'prefixe': structured_prop.get('Pr√©fixe', ''),
            'section': structured_prop.get('Sec', structured_prop.get('Section', '')),
            'numero': structured_prop.get('N¬∞ Plan', structured_prop.get('Num√©ro', '')),
            'contenance': structured_prop.get('Contenance', ''),
            'droit_reel': '',
            'designation_parcelle': structured_prop.get('Adresse', structured_prop.get('D√©signation', '')),
            'nom': '',
            'prenom': '',
            'numero_majic': '',
            'voie': '',
            'post_code': '',
            'city': ''
        }

    def process_single_pdf_hybrid(self, pdf_path: Path) -> List[Dict]:
        """
        TRAITEMENT HYBRIDE PRINCIPAL : pdfplumber + OpenAI Vision
        R√©plique l'approche Make pour des r√©sultats optimaux.
        """
        logger.info(f"üöÄ TRAITEMENT HYBRIDE de {pdf_path.name}")
        
        # √âTAPE 1: Extraction tableaux avec pdfplumber
        structured_data = self.extract_tables_with_pdfplumber(pdf_path)
        
        # √âTAPE 2: Extraction propri√©taires avec OpenAI Vision
        owners_data = self.extract_owners_with_vision_simple(pdf_path)
        
        # √âTAPE 3: Fusion intelligente
        merged_properties = self.merge_structured_and_vision_data(structured_data, owners_data, pdf_path.name)
        
        # √âTAPE 4: Finalisation avec IDs uniques et propagation
        final_properties = []
        for prop in merged_properties:
            # G√©n√©rer l'ID unique
            unique_id = self.generate_unique_id(
                department=prop.get('department', '00'),
                commune=prop.get('commune', '000'),
                section=prop.get('section', 'A'),
                numero=prop.get('numero', '0000'),
                prefixe=prop.get('prefixe', '')
            )
            
            # Ajouter l'ID unique et le fichier source
            prop['id'] = unique_id
            prop['fichier_source'] = pdf_path.name
            
            final_properties.append(prop)
        
        # S√©paration automatique des pr√©fixes coll√©s
        if final_properties:
            final_properties = self.separate_stuck_prefixes(final_properties)
        
        # Propagation des valeurs
        if final_properties:
            final_properties = self.propagate_values_downward(final_properties, ['designation_parcelle', 'prefixe'])
        
        logger.info(f"‚úÖ HYBRIDE termin√©: {len(final_properties)} propri√©t√©(s) pour {pdf_path.name}")
        return final_properties

    def process_single_pdf(self, pdf_path: Path) -> List[Dict]:
        """
        Traite un PDF MULTI-PAGES avec fusion intelligente des donn√©es.
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            
        Returns:
            Liste des propri√©taires avec leurs informations fusionn√©es
        """
        logger.info(f"üîÑ Traitement MULTI-PAGES de {pdf_path.name}")
        
        # Convertir en images
        images = self.pdf_to_images(pdf_path)
        if not images:
            logger.error(f"‚ùå √âchec de la conversion en images pour {pdf_path.name}")
            return []
        
        # PHASE 1: Extraction de TOUTES les pages
        all_page_data = []
        for page_num, image_data in enumerate(images, 1):
            logger.info(f"üìÑ Extraction page {page_num}/{len(images)} pour {pdf_path.name}")
            extracted_data = self.extract_info_with_gpt4o(image_data, f"{pdf_path.name} (page {page_num})")
            if extracted_data and 'proprietes' in extracted_data:
                all_page_data.extend(extracted_data['proprietes'])
                logger.info(f"‚úÖ Page {page_num}: {len(extracted_data['proprietes'])} √©l√©ment(s) extraits")
            else:
                logger.warning(f"‚ö†Ô∏è Page {page_num}: aucune donn√©e extraite")
        
        if not all_page_data:
            logger.error(f"‚ùå Aucune donn√©e extraite de {pdf_path.name}")
            return []
        
        logger.info(f"üìä Total √©l√©ments bruts extraits: {len(all_page_data)}")
        
        # PHASE 2: FUSION INTELLIGENTE multi-pages
        merged_properties = self.smart_merge_multi_page_data(all_page_data, pdf_path.name)
        
        # PHASE 3: Finalisation avec IDs uniques
        final_properties = []
        for prop in merged_properties:
            # G√©n√©rer l'ID unique
            unique_id = self.generate_unique_id(
                department=prop.get('department', '00'),
                commune=prop.get('commune', '000'),
                section=prop.get('section', 'A'),
                numero=prop.get('numero', '0000'),
                prefixe=prop.get('prefixe', '')
            )
            
            # Ajouter l'ID unique et le fichier source
            prop['id'] = unique_id
            prop['fichier_source'] = pdf_path.name
            
            final_properties.append(prop)
        
        logger.info(f"üéâ {pdf_path.name} FUSIONN√â avec succ√®s - {len(final_properties)} propri√©t√©(s) compl√®te(s)")
        return final_properties

    def detect_pdf_format(self, image_data: bytes) -> Dict:
        """
        D√âTECTE automatiquement le type/format du PDF cadastral.
        Chaque d√©partement/commune a son propre format !
        """
        detection_prompt = """
        Tu es un expert en documents cadastraux fran√ßais. Analyse cette image et d√©termine :

        1. TYPE DE DOCUMENT :
           - Extrait cadastral (avec propri√©taires)
           - Matrice cadastrale  
           - √âtat de section
           - Plan cadastral
           - Autre

        2. FORMAT/√âPOQUE :
           - Moderne (post-2010) - tableaux structur√©s, codes MAJIC
           - Interm√©diaire (2000-2010) - semi-structur√©
           - Ancien (pr√©-2000) - format libre

        3. MISE EN PAGE :
           - Une seule page avec tout
           - Multi-pages (info dispers√©e)
           - Tableau structur√©
           - Texte libre

        4. INFORMATIONS VISIBLES :
           - D√©partement/commune en en-t√™te
           - Codes MAJIC visibles (6 chars alphanum√©riques)
           - Parcelles avec sections/num√©ros
           - Propri√©taires avec noms/pr√©noms
           - Adresses compl√®tes

        R√©ponds en JSON :
        {
            "document_type": "extrait|matrice|section|plan|autre",
            "format_era": "moderne|intermediaire|ancien", 
            "layout": "single_page|multi_page|tableau|texte_libre",
            "visible_info": {
                "location_header": true/false,
                "majic_codes": true/false,
                "parcels_listed": true/false,
                "owners_listed": true/false,
                "addresses_present": true/false
            },
            "extraction_strategy": "complete|location_focus|parcel_focus|owner_focus|mixed"
        }
        """

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": detection_prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64.b64encode(image_data).decode()}"}}
                        ]
                    }
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            content = response.choices[0].message.content
            if not content or content.strip() == "":
                logger.warning("R√©ponse API vide pour d√©tection format")
                raise ValueError("R√©ponse vide")
            
            # Nettoyer le contenu et extraire le JSON
            content = content.strip()
            
            # Chercher un objet JSON dans la r√©ponse
            start_idx = content.find('{')
            end_idx = content.rfind('}')
            
            if start_idx == -1 or end_idx == -1:
                logger.warning(f"Pas de JSON trouv√© dans la r√©ponse: {content[:100]}...")
                raise ValueError("Pas de JSON dans la r√©ponse")
            
            json_content = content[start_idx:end_idx+1]
            detection_result = json.loads(json_content)
            
            # V√©rifier que tous les champs requis sont pr√©sents
            required_fields = ["document_type", "format_era", "layout", "extraction_strategy"]
            for field in required_fields:
                if field not in detection_result:
                    logger.warning(f"Champ manquant dans d√©tection: {field}")
                    raise ValueError(f"Champ manquant: {field}")
            
            logger.info(f"Format d√©tect√©: {detection_result.get('document_type')} - {detection_result.get('format_era')} - Strat√©gie: {detection_result.get('extraction_strategy')}")
            return detection_result
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"√âchec d√©tection format: {e}")
            # Format par d√©faut pour extraction maximale
            return {
                "document_type": "extrait",
                "format_era": "moderne", 
                "layout": "multi_page",
                "visible_info": {
                    "location_header": True,
                    "majic_codes": True,
                    "parcels_listed": True,
                    "owners_listed": True,
                    "addresses_present": True
                },
                "extraction_strategy": "complete"
            }
        except Exception as e:
            logger.error(f"Erreur d√©tection format: {e}")
            return {
                "document_type": "extrait",
                "format_era": "moderne", 
                "layout": "multi_page",
                "visible_info": {
                    "location_header": True,
                    "majic_codes": True,
                    "parcels_listed": True,
                    "owners_listed": True,
                    "addresses_present": True
                },
                "extraction_strategy": "complete"
            }

    def adapt_extraction_prompt(self, format_info: Dict) -> str:
        """
        ADAPTE le prompt d'extraction selon le format d√©tect√©.
        Chaque type de PDF n√©cessite une approche diff√©rente !
        """
        base_examples = """
üéØ EXEMPLES ULTRA-PR√âCIS avec TOUTES les colonnes importantes:

EXEMPLE D√âPARTEMENT 51:
{
  "department": "51",
  "commune": "179",
  "commune_nom": "DAMPIERRE-SUR-MOIVRE",
  "prefixe": "",
  "section": "000ZE",
  "numero": "0025",
  "contenance": "001045",     ‚¨ÖÔ∏è CONTENANCE = SURFACE en m¬≤ (OBLIGATOIRE!)
  "droit_reel": "US",
  "designation_parcelle": "LES ROULLIERS",
  "nom": "[NOM_PROPRIETAIRE1]",
  "prenom": "[PRENOM_MULTIPLE]",
  "numero_majic": "M8BNF6",
  "voie": "1 RUE D AVAT",
  "post_code": "51240",
  "city": "COUPEVILLE"
}

EXEMPLE D√âPARTEMENT 25:
{
  "department": "25",
  "commune": "227",
  "commune_nom": "BESANCON",
  "prefixe": "",
  "section": "000ZD",
  "numero": "0005",
  "contenance": "000150",     ‚¨ÖÔ∏è SURFACE = 150m¬≤ (CHERCHE √ßa partout!)
  "droit_reel": "PP",
  "designation_parcelle": "LE GRAND CHAMP",
  "nom": "[NOM_PROPRIETAIRE2]",
  "prenom": "[PRENOM_SIMPLE]",
  "numero_majic": "MB43HC",
  "voie": "15 RUE DE LA PAIX", 
  "post_code": "25000",
  "city": "BESANCON"
}
"""

        # Adaptations selon le format d√©tect√©
        if format_info.get('document_type') == 'matrice':
            specific_instructions = """
üéØ DOCUMENT TYPE: MATRICE CADASTRALE
- Focus sur les TABLES avec colonnes structur√©es
- Propri√©taires souvent dans colonne de droite
- Parcelles dans colonnes de gauche
- ‚≠ê CONTENANCE dans colonne "Surface" ou "Cont." ou chiffres + m¬≤
- Scan ligne par ligne m√©thodiquement
"""
        elif format_info.get('layout') == 'tableau':
            specific_instructions = """
üéØ FORMAT: TABLEAU STRUCTUR√â
- Identifie les EN-T√äTES de colonnes
- Cherche colonnes: "Surface", "Contenance", "m¬≤", "ares", "ca"
- Lit chaque LIGNE du tableau
- Associe chaque valeur √† sa colonne
- ‚≠ê CONTENANCE = valeurs num√©riques dans colonnes surface
- Attention aux cellules fusionn√©es
"""
        elif format_info.get('format_era') == 'ancien':
            specific_instructions = """
üéØ FORMAT: ANCIEN/LIBRE
- Scan TOUT le texte zone par zone
- Cherche les motifs : "NOM Pr√©nom", codes, adresses
- Les infos peuvent √™tre DISPERS√âES
- Utilise le contexte pour regrouper
"""
        else:
            specific_instructions = """
üéØ FORMAT: MODERNE/STANDARD
- Scan syst√©matique de HAUT en BAS
- En-t√™tes pour d√©partement/commune
- Propri√©taires en MAJUSCULES
- Codes MAJIC (6 caract√®res)
- ‚≠ê CONTENANCE = recherche "m¬≤", "ares", "ca" + chiffres associ√©s
- Sections format "000XX" ou "XX" + num√©ros parcelles
"""

        # Instructions sp√©cifiques selon ce qui est visible
        if not format_info.get('visible_info', {}).get('majic_codes'):
            majic_note = "‚ö†Ô∏è CODES MAJIC probablement ABSENTS - concentre-toi sur noms/adresses"
        else:
            majic_note = "üéØ CODES MAJIC PR√âSENTS - scan tous les codes 6 caract√®res"

        if format_info.get('extraction_strategy') == 'location_focus':
            strategy_note = "üéØ PRIORIT√â: D√©partement/Commune/Localisation"
        elif format_info.get('extraction_strategy') == 'owner_focus':
            strategy_note = "üéØ PRIORIT√â: Propri√©taires/Noms/Adresses/MAJIC"
        elif format_info.get('extraction_strategy') == 'parcel_focus':
            strategy_note = "üéØ PRIORIT√â: Parcelles/Sections/Num√©ros/Contenances"
        else:
            strategy_note = "üéØ EXTRACTION COMPL√àTE de TOUTES les informations"

        # Prompt adaptatif final
        adapted_prompt = f"""
Tu es un EXPERT en extraction de donn√©es cadastrales fran√ßaises. Ce document a √©t√© analys√© automatiquement.

{specific_instructions}

{strategy_note}
{majic_note}

{base_examples}

üîç INSTRUCTIONS DE SCAN ADAPTATIF ULTRA-PR√âCIS:
1. Applique la strat√©gie d√©tect√©e ci-dessus
2. Scan M√âTHODIQUEMENT selon le type de mise en page  
3. ‚≠ê CONTENANCE = SURFACE : Cherche PARTOUT les chiffres suivis de "m¬≤", "ares", "ca" (centiares)
4. ‚≠ê SECTION + NUM√âRO : Formats "000ZE", "ZE", "003", toujours ensemble
5. ‚≠ê NOMS/PR√âNOMS : MAJUSCULES = nom, minuscules = pr√©nom  
6. ‚≠ê CODES MAJIC : Exactement 6 caract√®res alphanum√©riques
7. ‚≠ê ADRESSES : Num√©ro + nom de rue + code postal + ville
8. Collecte TOUTES les informations visibles
9. Regroupe intelligemment les donn√©es dispers√©es
10. Ne laisse RIEN passer, m√™me les valeurs partielles
11. ‚≠ê PR√âFIXE (TR√àS RARE mais CRUCIAL) : Le pr√©fixe n'appara√Æt que dans quelques PDFs sp√©ciaux, mais quand il existe, il est IMP√âRATIF de l'extraire !
    - Position : dans les tableaux "Propri√©t√©(s) non b√¢tie(s)" AVANT la d√©signation du lieu-dit
    - Format typique : "ZY 8", "AB 12", "000AC 5" ‚Üí pr√©fixe="ZY"/"AB"/"000AC", section="8"/"12"/"5"  
    - Colonne souvent nomm√©e "Pr√©fixe", "Pfxe" ou pr√©c√®de directement la section
    - Si AUCUN pr√©fixe visible = "", ne JAMAIS inventer
    - Si pr√©fixe trouv√© = l'extraire avec PR√âCISION ABSOLUE

‚ö†Ô∏è R√àGLES STRICTES:
- ADAPTE ta lecture au format d√©tect√©
- Si info partiellement visible, INCLUS-LA
- JAMAIS de "N/A" - utilise "" si vraiment absent
- IGNORE aucun d√©tail m√™me petit
- Retourne TOUS les propri√©taires trouv√©s

üì§ FORMAT R√âPONSE:
{{
  "proprietes": [
    {{
      "department": "",
      "commune": "",
      "commune_nom": "",
      "prefixe": "ZY",
      "section": "",
      "numero": "",
      "contenance": "",
      "droit_reel": "",
      "designation_parcelle": "",
      "nom": "",
      "prenom": "",
      "numero_majic": "",
      "voie": "",
      "post_code": "",
      "city": ""
    }}
  ]
}}
"""
        return adapted_prompt

    def propagate_values_downward(self, properties: List[Dict], fields: List[str]) -> List[Dict]:
        """
        Propage les valeurs des champs sp√©cifi√©s vers le bas si elles sont vides.
        M√©morise la derni√®re valeur vue pour chaque champ et la propage.
        
        ‚úÖ CORRECTION CONTAMINATION CROIS√âE: Isolation stricte par fichier source.
        """
        if not properties:
            return properties
        
        # Grouper les propri√©t√©s par fichier source pour √©viter la contamination crois√©e
        files_groups = {}
        for prop in properties:
            file_source = prop.get('fichier_source', 'unknown')
            if file_source not in files_groups:
                files_groups[file_source] = []
            files_groups[file_source].append(prop)
        
        all_updated_properties = []
        
        # Traiter chaque fichier s√©par√©ment avec sa propre m√©moire
        for file_source, file_props in files_groups.items():
            # ‚úÖ ISOLATION: Nouvelle m√©moire pour chaque fichier
            last_seen_values = {field: None for field in fields}
            
            for prop in file_props:
                updated_prop = prop.copy()
                for field in fields:
                    if updated_prop.get(field) is None or updated_prop.get(field) == "":
                        if last_seen_values[field] is not None:
                            updated_prop[field] = last_seen_values[field]
                    else:
                        last_seen_values[field] = updated_prop[field]
                all_updated_properties.append(updated_prop)
        
        return all_updated_properties

    def separate_stuck_prefixes(self, properties: List[Dict]) -> List[Dict]:
        """
        S√âPARATION AUTOMATIQUE des pr√©fixes coll√©s aux sections.
        D√©tecte et s√©pare les patterns comme "302A" ‚Üí pr√©fixe="302", section="A"
        """
        import re
        
        updated_properties = []
        separated_count = 0
        
        for prop in properties:
            updated_prop = prop.copy()
            section = str(updated_prop.get('section', '')).strip()
            current_prefixe = str(updated_prop.get('prefixe', '')).strip()
            
            # Ne traiter que si la section n'est pas vide et le pr√©fixe est vide
            if section and not current_prefixe:
                # Pattern pour d√©tecter pr√©fixe num√©rique coll√© √† section alphab√©tique
                # Exemples: "302A", "302 A", "302AB", "001ZD", "123AC", etc.
                pattern = r'^(\d+)\s*([A-Z]+)$'  # \s* permet les espaces optionnels
                match = re.match(pattern, section)
                
                if match:
                    detected_prefixe = match.group(1)  # La partie num√©rique (302)
                    detected_section = match.group(2)  # La partie alphab√©tique (A)
                    
                    # Mettre √† jour les champs
                    updated_prop['prefixe'] = detected_prefixe
                    updated_prop['section'] = detected_section
                    
                    separated_count += 1
                    logger.info(f"üîç Pr√©fixe s√©par√©: '{section}' ‚Üí pr√©fixe='{detected_prefixe}' section='{detected_section}'")
            
            updated_properties.append(updated_prop)
        
        if separated_count > 0:
            logger.info(f"‚úÇÔ∏è S√©paration automatique: {separated_count} pr√©fixe(s) d√©tect√©(s) et s√©par√©(s)")
        
        return updated_properties

    def parse_contenance_value(self, value: str) -> str:
        """
        ‚úÖ CORRECTION CONTENANCE : Parse les valeurs de contenance avec gestion des formats fran√ßais.
        
        G√®re les cas :
        - "1 216,05" ‚Üí "1216" (supprime espaces et virgules)
        - "10,98" ‚Üí "10" (partie enti√®re)
        - "1098" ‚Üí "1098" (d√©j√† correct)
        - "" ou None ‚Üí ""
        """
        if not value:
            return ""
        
        try:
            # Convertir en string et nettoyer
            clean_value = str(value).strip()
            
            if not clean_value or clean_value.lower() in ['n/a', 'null', 'none']:
                return ""
            
            # ‚úÖ CORRECTION FORMAT FRAN√áAIS : Supprimer espaces dans les milliers
            clean_value = clean_value.replace(' ', '')  # "1 216" ‚Üí "1216"
            
            # ‚úÖ CORRECTION VIRGULES : Remplacer virgules par points
            clean_value = clean_value.replace(',', '.')  # "1216,05" ‚Üí "1216.05"
            
            # ‚úÖ EXTRAIRE PARTIE ENTI√àRE : Prendre seulement la partie avant le point
            if '.' in clean_value:
                clean_value = clean_value.split('.')[0]  # "1216.05" ‚Üí "1216"
            
            # ‚úÖ VALIDATION : V√©rifier que c'est num√©rique
            if clean_value.isdigit():
                return clean_value
            else:
                # Extraire seulement les chiffres
                digits_only = ''.join(filter(str.isdigit, clean_value))
                return digits_only if digits_only else ""
                
        except Exception as e:
            logger.warning(f"Erreur parsing contenance '{value}': {e}")
            return ""

    def split_name_intelligently(self, nom: str, prenom: str) -> tuple:
        """
        ‚úÖ CORRECTION NOMS/PR√âNOMS : S√©pare intelligemment les noms mal fusionn√©s.
        
        G√®re les cas :
        - nom="ALEXIS MOURADOFF", prenom="ALEXIS" ‚Üí nom="MOURADOFF", prenom="ALEXIS"
        - nom="MOURADOFF", prenom="MONIQUE ALEXIS" ‚Üí nom="MOURADOFF", prenom="MONIQUE ALEXIS"
        """
        nom_clean = str(nom).strip()
        prenom_clean = str(prenom).strip()
        
        # Cas 1: Pr√©nom dupliqu√© au d√©but du nom
        if prenom_clean and nom_clean.startswith(prenom_clean + ' '):
            # "ALEXIS MOURADOFF" avec pr√©nom "ALEXIS" ‚Üí nom="MOURADOFF"
            nom_without_prenom = nom_clean[len(prenom_clean):].strip()
            logger.debug(f"üîß Nom corrig√©: '{nom_clean}' ‚Üí '{nom_without_prenom}' (pr√©nom dupliqu√© supprim√©)")
            return nom_without_prenom, prenom_clean
        
        # Cas 2: V√©rifier AVANT TOUT si c'est une personne morale (priorit√© absolue)
        nom_parts = nom_clean.split()
        if len(nom_parts) >= 2 and not prenom_clean:
            # ‚úÖ CORRECTION: D√©tecter les personnes morales EN PREMIER
            legal_keywords = ['COM', 'COMMUNE', 'VILLE', 'MAIRIE', '√âTAT', 'D√âPARTEMENT', 'R√âGION', 'SCI', 'SARL', 'SA', 'EURL']
            if any(keyword in nom_clean.upper() for keyword in legal_keywords):
                logger.debug(f"üèõÔ∏è Personne morale conserv√©e: '{nom_clean}'")
                return nom_clean, prenom_clean
        
        # Cas 3: Nom compos√© mal s√©par√© (plus de 2 mots dans nom) - apr√®s v√©rification personne morale
        if len(nom_parts) > 2 and not prenom_clean:
            # "[PRENOM_MULTIPLE] [NOM_FAMILLE]" sans pr√©nom ‚Üí nom="[NOM_FAMILLE]", prenom="[PRENOM_MULTIPLE]"
            potential_nom = nom_parts[-1]  # Dernier mot = nom de famille
            potential_prenom = ' '.join(nom_parts[:-1])  # Reste = pr√©nom
            logger.debug(f"üîß S√©paration automatique: '{nom_clean}' ‚Üí nom='{potential_nom}' prenom='{potential_prenom}'")
            return potential_nom, potential_prenom
            
        # Cas 4: Nom simple (2 mots) - probable pr√©nom+nom
        if len(nom_parts) == 2 and not prenom_clean:
            # "[PRENOM] [NOM_FAMILLE]" sans pr√©nom ‚Üí nom="[NOM_FAMILLE]", prenom="[PRENOM]"
            potential_nom = nom_parts[1]
            potential_prenom = nom_parts[0]
            logger.debug(f"üîß S√©paration simple: '{nom_clean}' ‚Üí nom='{potential_nom}' prenom='{potential_prenom}'")
            return potential_nom, potential_prenom
        
        # Cas par d√©faut : garder tel quel
        return nom_clean, prenom_clean

    def clean_address(self, address: str) -> str:
        """
        ‚úÖ CORRECTION ADRESSES : Nettoie et valide les adresses.
        
        Supprime les caract√®res parasites et valide le format.
        """
        if not address:
            return ""
        
        address_clean = str(address).strip()
        
        # Supprimer les caract√®res parasites courants
        parasites = ['<', '>', '|', '_', '+', '=']
        for parasite in parasites:
            address_clean = address_clean.replace(parasite, ' ')
        
        # Nettoyer les espaces multiples
        address_clean = ' '.join(address_clean.split())
        
        # Validation basique : doit contenir au moins une lettre
        if not any(c.isalpha() for c in address_clean):
            return ""
        
        # Validation longueur
        if len(address_clean) < 3 or len(address_clean) > 100:
            return ""
        
        return address_clean

    def smart_merge_multi_page_data(self, all_page_data: List[Dict], filename: str) -> List[Dict]:
        """
        FUSION INTELLIGENTE des donn√©es multi-pages.
        Combine les informations dispers√©es sur plusieurs pages en propri√©t√©s compl√®tes.
        """
        logger.info(f"üß† Fusion intelligente de {len(all_page_data)} √©l√©ments pour {filename}")
        
        # S√©parer les donn√©es par type d'information
        location_data = {}  # d√©partement/commune communs
        owners_data = []    # propri√©taires avec infos compl√®tes
        parcels_data = []   # parcelles avec sections/num√©ros
        mixed_data = []     # donn√©es mixtes
        
        # PHASE 1: Classification des donn√©es extraites
        for item in all_page_data:
            has_owner = bool(item.get('nom') or item.get('prenom'))
            has_parcel = bool(item.get('section') or item.get('numero'))
            has_location = bool(item.get('department') or item.get('commune'))
            
            if has_location and not location_data:
                # Stocker les infos de localisation (d√©partement/commune)
                location_data = {
                    'department': item.get('department', ''),
                    'commune': item.get('commune', ''),
                    'post_code': item.get('post_code', ''),
                    'city': item.get('city', '')
                }
                logger.info(f"üìç Localisation trouv√©e: {location_data}")
            
            if has_owner and has_parcel:
                # Donn√©es compl√®tes
                mixed_data.append(item)
            elif has_owner:
                # Donn√©es propri√©taire uniquement
                owners_data.append(item)
            elif has_parcel:
                # Donn√©es parcelle uniquement  
                parcels_data.append(item)
        
        logger.info(f"üìã Classification: {len(mixed_data)} compl√®tes, {len(owners_data)} propri√©taires, {len(parcels_data)} parcelles")
        
        # PHASE 2: Strat√©gie de fusion selon les donn√©es disponibles
        merged_properties = []
        
        if mixed_data:
            # Cas optimal: donn√©es d√©j√† compl√®tes
            for item in mixed_data:
                # Enrichir avec les infos de localisation si manquantes
                if not item.get('department') and location_data.get('department'):
                    item['department'] = location_data['department']
                if not item.get('commune') and location_data.get('commune'):
                    item['commune'] = location_data['commune']
                if not item.get('post_code') and location_data.get('post_code'):
                    item['post_code'] = location_data['post_code']
                if not item.get('city') and location_data.get('city'):
                    item['city'] = location_data['city']
                
                merged_properties.append(item)
                
        elif owners_data and parcels_data:
            # Fusion propri√©taires + parcelles
            logger.info(f"üîó Fusion {len(owners_data)} propri√©taires avec {len(parcels_data)} parcelles")
            
            # Strat√©gie: croiser chaque propri√©taire avec chaque parcelle
            for owner in owners_data:
                for parcel in parcels_data:
                    merged_prop = self.merge_owner_parcel(owner, parcel, location_data)
                    merged_properties.append(merged_prop)
                    
        elif owners_data:
            # Seulement des propri√©taires - enrichir avec localisation
            for owner in owners_data:
                merged_prop = owner.copy()
                # Ajouter les infos de localisation
                for key, value in location_data.items():
                    if not merged_prop.get(key) and value:
                        merged_prop[key] = value
                merged_properties.append(merged_prop)
                
        elif parcels_data:
            # Seulement des parcelles - enrichir avec localisation  
            for parcel in parcels_data:
                merged_prop = parcel.copy()
                # Ajouter les infos de localisation
                for key, value in location_data.items():
                    if not merged_prop.get(key) and value:
                        merged_prop[key] = value
                merged_properties.append(merged_prop)
        
        else:
            # Cas de secours: utiliser toutes les donn√©es brutes
            merged_properties = all_page_data
        
        # PHASE 3: Nettoyage et d√©duplication
        cleaned_properties = self.clean_and_deduplicate(merged_properties, filename)
        
        # S√©paration automatique des pr√©fixes coll√©s
        cleaned_properties = self.separate_stuck_prefixes(cleaned_properties)
        
        # Propager les valeurs de designation_parcelle et prefixe vers le bas
        cleaned_properties = self.propagate_values_downward(cleaned_properties, ['designation_parcelle', 'prefixe'])
        
        logger.info(f"‚ú® Fusion termin√©e: {len(cleaned_properties)} propri√©t√©s finales")
        return cleaned_properties

    def merge_owner_parcel(self, owner: Dict, parcel: Dict, location: Dict) -> Dict:
        """
        Fusionne intelligemment les donn√©es propri√©taire + parcelle + localisation.
        """
        merged = {}
        
        # Priorit√© aux donn√©es propri√©taire pour les champs propri√©taire
        owner_fields = ['nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city']
        for field in owner_fields:
            merged[field] = owner.get(field, '')
        
        # Priorit√© aux donn√©es parcelle pour les champs parcelle
        parcel_fields = ['section', 'numero', 'contenance', 'droit_reel', 'designation_parcelle', 'prefixe']
        for field in parcel_fields:
            merged[field] = parcel.get(field, '')
        
        # Utiliser les donn√©es de localisation pour les champs manquants
        location_fields = ['department', 'commune', 'post_code', 'city']
        for field in location_fields:
            if not merged.get(field) and location.get(field):
                merged[field] = location[field]
        
        # Si encore des champs manquants, essayer de les r√©cup√©rer de l'autre source
        for field in ['department', 'commune']:
            if not merged.get(field):
                merged[field] = owner.get(field, parcel.get(field, ''))
        
        return merged

    def clean_and_deduplicate(self, properties: List[Dict], filename: str) -> List[Dict]:
        """
        üßπ NETTOYAGE ET D√âDUPLICATION INTELLIGENTE avec validation g√©ographique.
        
        √âvite de supprimer des propri√©taires l√©gitimes diff√©rents.
        """
        if not properties:
            return []
        
        logger.info(f"üßπ NETTOYAGE INTELLIGENT pour {filename} - {len(properties)} propri√©t√©s")
        
        cleaned = []
        seen_combinations = set()
        
        # 1. VALIDATION G√âOGRAPHIQUE PR√âALABLE
        geo_stats = {}
        for prop in properties:
            dept = prop.get('department', '').strip()
            comm = prop.get('commune', '').strip()
            if dept and comm:
                geo_key = f"{dept}-{comm}"
                geo_stats[geo_key] = geo_stats.get(geo_key, 0) + 1
        
        # D√©tecter si on a une g√©ographie dominante
        main_geo = None
        if geo_stats:
            main_geo = max(geo_stats.items(), key=lambda x: x[1])[0]
            logger.info(f"üéØ G√©ographie dominante: {main_geo} ({geo_stats[main_geo]} occurrences)")
        
        contaminated_removed = 0
        for prop in properties:
            # 2. FILTRAGE ANTI-CONTAMINATION G√âOGRAPHIQUE
            dept = prop.get('department', '').strip()
            comm = prop.get('commune', '').strip()
            
            if main_geo and dept and comm:
                prop_geo = f"{dept}-{comm}"
                if prop_geo != main_geo:
                    contaminated_removed += 1
                    logger.info(f"‚ùå CONTAMINATION: {prop.get('nom', '')} {prop.get('prenom', '')} (Geo: {prop_geo} vs {main_geo})")
                    continue  # Skip cette propri√©t√© contamin√©e
            
            # 3. CR√âATION CL√â UNIQUE INTELLIGENTE pour d√©duplication fine
            key_fields = [
                prop.get('nom', '').strip().upper(),
                prop.get('prenom', '').strip().upper(),
                prop.get('section', '').strip(),
                prop.get('numero', '').strip(),
                prop.get('numero_majic', '').strip()
            ]
            unique_key = '|'.join(str(f) for f in key_fields)
            
            # 4. IGNORER LES ENTR√âES COMPL√àTEMENT VIDES OU INVALIDES
            if not any(key_fields) or unique_key == '||||':
                continue
            
            # V√©rification sp√©ciale pour noms suspects
            nom = prop.get('nom', '').strip()
            prenom = prop.get('prenom', '').strip()
            if not self.is_likely_real_owner(nom, prenom):
                logger.info(f"‚ùå NOM SUSPECT REJET√â: {nom} {prenom}")
                continue
            
            # 5. D√âDUPLICATION INTELLIGENTE
            if unique_key not in seen_combinations:
                seen_combinations.add(unique_key)
                
                # 6. ASSURER LA COMPL√âTUDE DES CHAMPS
                required_fields = [
                    'department', 'commune', 'prefixe', 'section', 'numero', 
                    'contenance', 'droit_reel', 'designation_parcelle', 
                    'nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city'
                ]
                
                for field in required_fields:
                    if field not in prop:
                        prop[field] = ''
                
                cleaned.append(prop)
            else:
                logger.debug(f"üîÑ DOUBLON IGNOR√â: {nom} {prenom} (d√©j√† trait√©)")
        
        if contaminated_removed > 0:
            logger.warning(f"üßΩ CONTAMINATION NETTOY√âE: {contaminated_removed} propri√©taires d'autres PDFs supprim√©s")
        
        logger.info(f"‚úÖ NETTOYAGE TERMIN√â: {len(properties)} ‚Üí {len(cleaned)} propri√©t√©s valides apr√®s d√©duplication intelligente")
        return cleaned

    def export_to_csv(self, all_properties: List[Dict], output_filename: str = "output.csv") -> None:
        """
        Exporte toutes les donn√©es vers un fichier CSV avec s√©parateur point-virgule.
        """
        if not all_properties:
            logger.warning("Aucune donn√©e √† exporter")
            return
        # Nettoyage du code commune avant export
        for prop in all_properties:
            prop['commune'] = clean_commune_code(prop.get('commune', ''))
        # Cr√©er le DataFrame
        df = pd.DataFrame(all_properties)
        
        # Colonnes selon les sp√©cifications du client (avec contenance d√©taill√©e)
        columns_order = [
            'department', 'commune', 'prefixe', 'section', 'numero', 
            'contenance_ha', 'contenance_a', 'contenance_ca',
            'droit_reel', 'designation_parcelle', 'nom', 'prenom', 'numero_majic', 
            'voie', 'post_code', 'city', 'id', 'fichier_source'
        ]
        
        # Renommer les colonnes pour plus de clart√©
        column_mapping = {
            'department': 'D√©partement',
            'commune': 'Commune', 
            'prefixe': 'Pr√©fixe',
            'section': 'Section',
            'numero': 'Num√©ro',
            'contenance_ha': 'Contenance HA',
            'contenance_a': 'Contenance A', 
            'contenance_ca': 'Contenance CA',
            'droit_reel': 'Droit r√©el',
            'designation_parcelle': 'Designation Parcelle',
            'nom': 'Nom Propri',
            'prenom': 'Pr√©nom Propri',
            'numero_majic': 'N¬∞MAJIC',
            'voie': 'Voie',
            'post_code': 'CP',
            'city': 'Ville',
            'id': 'id',
            'fichier_source': 'Fichier source'
        }
        
        # R√©organiser et renommer
        df = df.reindex(columns=columns_order, fill_value='')
        df = df.rename(columns=column_mapping)
        
        # Export CSV avec s√©parateur point-virgule (meilleur pour Excel fran√ßais)
        output_path = self.output_dir / output_filename
        df.to_csv(output_path, index=False, encoding='utf-8-sig', sep=';')
        
        logger.info(f"üìä Donn√©es CSV export√©es vers {output_path} (s√©parateur: ;)")
        logger.info(f"üìà Total: {len(all_properties)} propri√©t√©(s) dans {len(df['Fichier source'].unique())} fichier(s)")
        
        return output_path

    def export_to_excel(self, all_properties: List[Dict], output_filename: str = "output.xlsx") -> None:
        """
        Exporte toutes les donn√©es vers un fichier Excel (.xlsx).
        """
        if not all_properties:
            logger.warning("Aucune donn√©e √† exporter en Excel")
            return
        # Nettoyage du code commune avant export
        for prop in all_properties:
            prop['commune'] = clean_commune_code(prop.get('commune', ''))
        # Cr√©er le DataFrame
        df = pd.DataFrame(all_properties)
        
        # Colonnes selon les sp√©cifications du client (avec contenance d√©taill√©e)
        columns_order = [
            'department', 'commune', 'prefixe', 'section', 'numero', 
            'contenance_ha', 'contenance_a', 'contenance_ca',
            'droit_reel', 'designation_parcelle', 'nom', 'prenom', 'numero_majic', 
            'voie', 'post_code', 'city', 'id', 'fichier_source'
        ]
        
        # Renommer les colonnes pour plus de clart√©
        column_mapping = {
            'department': 'D√©partement',
            'commune': 'Commune', 
            'prefixe': 'Pr√©fixe',
            'section': 'Section',
            'numero': 'Num√©ro',
            'contenance_ha': 'Contenance HA',
            'contenance_a': 'Contenance A', 
            'contenance_ca': 'Contenance CA',
            'droit_reel': 'Droit r√©el',
            'designation_parcelle': 'Designation Parcelle',
            'nom': 'Nom Propri',
            'prenom': 'Pr√©nom Propri',
            'numero_majic': 'N¬∞MAJIC',
            'voie': 'Voie',
            'post_code': 'CP',
            'city': 'Ville',
            'id': 'id',
            'fichier_source': 'Fichier source'
        }
        
        # R√©organiser et renommer
        df = df.reindex(columns=columns_order, fill_value='')
        df = df.rename(columns=column_mapping)
        
        # Export Excel
        output_path = self.output_dir / output_filename
        df.to_excel(output_path, index=False, engine='openpyxl')
        
        logger.info(f"üìä Donn√©es Excel export√©es vers {output_path}")
        logger.info(f"üìà Total: {len(all_properties)} propri√©t√©(s) dans {len(df['Fichier source'].unique())} fichier(s)")
        
        return output_path

    def run(self) -> None:
        """
        TRAITEMENT PAR LOTS OPTIMIS√â pour extraction maximale.
        """
        logger.info("üöÄ D√©marrage de l'extraction BATCH OPTIMIS√âE")
        
        # Lister les fichiers PDF
        pdf_files = self.list_pdf_files()
        
        if not pdf_files:
            logger.warning("‚ùå Aucun fichier PDF trouv√© dans le dossier input/")
            return
        
        logger.info(f"üìÑ {len(pdf_files)} PDF(s) d√©tect√©(s) pour traitement par lots")
        
        # PHASE 1: PR√â-ANALYSE du lot pour strat√©gie globale
        batch_strategy = self.analyze_pdf_batch(pdf_files)
        logger.info(f"üß† Strat√©gie globale: {batch_strategy.get('approach', 'standard')}")
        
        # PHASE 2: TRAITEMENT OPTIMIS√â par lots
        all_properties = self.process_pdf_batch_optimized(pdf_files, batch_strategy)
        
        # PHASE 3: POST-TRAITEMENT pour combler les trous
        if all_properties:
            enhanced_properties = self.post_process_batch_results(all_properties, pdf_files)
            
            # PHASE 4: EXPORT avec statistiques d√©taill√©es
            self.export_to_csv_with_stats(enhanced_properties)
            
            # Statistiques finales
            total_props = len(enhanced_properties)
            total_pdfs = len(pdf_files)
            avg_per_pdf = total_props / total_pdfs if total_pdfs > 0 else 0
            
            logger.info(f"‚úÖ EXTRACTION BATCH TERMIN√âE!")
            logger.info(f"üìä {total_props} propri√©t√©s extraites de {total_pdfs} PDFs")
            logger.info(f"üìà Moyenne: {avg_per_pdf:.1f} propri√©t√©s/PDF")
        else:
            logger.warning("‚ùå Aucune donn√©e extraite du lot")

    def analyze_pdf_batch(self, pdf_files: List[Path]) -> Dict:
        """
        PR√â-ANALYSE du lot de PDFs pour d√©terminer la strat√©gie optimale.
        """
        logger.info(f"üîç Pr√©-analyse de {len(pdf_files)} PDFs...")
        
        batch_info = {
            'total_files': len(pdf_files),
            'formats_detected': {},
            'total_pages': 0,
            'approach': 'standard',
            'common_location': {},
            'estimated_properties': 0
        }
        
        # Analyser un √©chantillon pour d√©tecter les patterns
        sample_size = min(3, len(pdf_files))  # Analyser max 3 PDFs pour la strat√©gie
        
        for i, pdf_file in enumerate(pdf_files[:sample_size]):
            logger.info(f"üîç Analyse √©chantillon {i+1}/{sample_size}: {pdf_file.name}")
            
            # Convertir premi√®re page pour analyse rapide
            images = self.pdf_to_images(pdf_file)
            if images:
                batch_info['total_pages'] += len(images)
                
                # D√©tecter le format de ce PDF
                format_info = self.detect_pdf_format(images[0])
                format_key = f"{format_info.get('document_type')}_{format_info.get('format_era')}"
                
                if format_key in batch_info['formats_detected']:
                    batch_info['formats_detected'][format_key] += 1
                else:
                    batch_info['formats_detected'][format_key] = 1
        
        # D√©terminer la strat√©gie globale bas√©e sur l'analyse
        if len(batch_info['formats_detected']) == 1:
            # Format homog√®ne - strat√©gie sp√©cialis√©e
            batch_info['approach'] = 'homogeneous_optimized'
        elif len(pdf_files) > 10:
            # Beaucoup de PDFs - strat√©gie volume
            batch_info['approach'] = 'high_volume_batch'
        else:
            # Format mixte - strat√©gie adaptative
            batch_info['approach'] = 'mixed_adaptive'
        
        logger.info(f"üìä Formats d√©tect√©s: {batch_info['formats_detected']}")
        logger.info(f"üéØ Strat√©gie choisie: {batch_info['approach']}")
        
        return batch_info

    def process_pdf_batch_optimized(self, pdf_files: List[Path], batch_strategy: Dict) -> List[Dict]:
        """
        TRAITEMENT PAR LOTS OPTIMIS√â selon la strat√©gie d√©termin√©e.
        """
        all_properties = []
        approach = batch_strategy.get('approach', 'standard')
        
        logger.info(f"‚öôÔ∏è Traitement par lots - Approche: {approach}")
        
        if approach == 'homogeneous_optimized':
            # Format homog√®ne - traitement optimis√©
            all_properties = self.process_homogeneous_batch(pdf_files)
        elif approach == 'high_volume_batch':
            # Volume √©lev√© - traitement parall√®le simul√©
            all_properties = self.process_high_volume_batch(pdf_files)
        else:
            # Approche adaptative mixte (par d√©faut)
            all_properties = self.process_mixed_adaptive_batch(pdf_files)
        
        logger.info(f"üìä {len(all_properties)} propri√©t√©s extraites au total")
        return all_properties

    def process_homogeneous_batch(self, pdf_files: List[Path]) -> List[Dict]:
        """
        Traitement optimis√© pour un lot de PDFs homog√®nes avec isolation ultra-s√©curis√©e.
        """
        logger.info("üîÑ Traitement homog√®ne optimis√© STYLE MAKE - MODE ULTRA-S√âCURIS√â")
        all_properties = []
        
        # Traiter avec approche Make exacte + isolation batch
        for i, pdf_file in enumerate(pdf_files, 1):
            logger.info(f"üìÑ Traitement Make [{i}/{len(pdf_files)}]: {pdf_file.name}")
            
            # üõ°Ô∏è NETTOYAGE BATCH ULTRA-S√âCURIS√â avant chaque PDF
            self.batch_ultra_secure_cleanup(i, len(pdf_files), pdf_file)
            
            properties = self.process_like_make(pdf_file)
            all_properties.extend(properties)
            
            # Log interm√©diaire pour suivi
            if i % 5 == 0:
                logger.info(f"üìä Progr√®s: {len(all_properties)} propri√©t√©s extraites jusqu'ici")
        
        return all_properties

    def process_high_volume_batch(self, pdf_files: List[Path]) -> List[Dict]:
        """
        Traitement optimis√© pour gros volume avec style Make et isolation ultra-s√©curis√©e.
        """
        logger.info("üöÄ Traitement haut volume STYLE MAKE - MODE ULTRA-S√âCURIS√â")
        all_properties = []
        
        for i, pdf_file in enumerate(pdf_files, 1):
            logger.info(f"üìÑ Volume Make [{i}/{len(pdf_files)}]: {pdf_file.name}")
            
            # üõ°Ô∏è NETTOYAGE BATCH ULTRA-S√âCURIS√â avant chaque PDF
            self.batch_ultra_secure_cleanup(i, len(pdf_files), pdf_file)
            
            properties = self.process_like_make(pdf_file)
            all_properties.extend(properties)
            
            # Logs de progression
            if i % 10 == 0:
                logger.info(f"üìä Progression: {i}/{len(pdf_files)} fichiers trait√©s")
        
        return all_properties

    def process_mixed_adaptive_batch(self, pdf_files: List[Path]) -> List[Dict]:
        """
        Traitement adaptatif mixte avec style Make et isolation ultra-s√©curis√©e.
        """
        logger.info("üéØ Traitement adaptatif mixte STYLE MAKE - MODE ULTRA-S√âCURIS√â")
        all_properties = []
        
        for i, pdf_file in enumerate(pdf_files, 1):
            logger.info(f"üìÑ Adaptatif Make [{i}/{len(pdf_files)}]: {pdf_file.name}")
            
            # üõ°Ô∏è NETTOYAGE BATCH ULTRA-S√âCURIS√â avant chaque PDF
            self.batch_ultra_secure_cleanup(i, len(pdf_files), pdf_file)
            
            properties = self.process_like_make(pdf_file)
            all_properties.extend(properties)
            
            # Suivi adaptatif
            if len(properties) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune propri√©t√© extraite pour {pdf_file.name}")
        
        return all_properties

    def post_process_batch_results(self, all_properties: List[Dict], pdf_files: List[Path]) -> List[Dict]:
        """
        POST-TRAITEMENT S√âCURIS√â - AUCUN cross-r√©f√©rencement pour garantir la fiabilit√©.
        Mode extraction PURE uniquement.
        """
        logger.info(f"üîß Post-traitement S√âCURIS√â de {len(all_properties)} propri√©t√©s")
        logger.info("üõ°Ô∏è Mode FIABILIT√â MAXIMALE - Aucun cross-r√©f√©rencement")
        
        if not all_properties:
            return all_properties
        
        # Analyser les champs manquants √† l'√©chelle du lot (pour statistiques seulement)
        missing_stats = self.analyze_missing_fields_batch(all_properties)
        logger.info(f"üìä Champs incomplets d√©tect√©s: {len(missing_stats)} types")
        
        # ‚ùå CROSS-R√âF√âRENCEMENT D√âSACTIV√â pour √©viter les erreurs
        # Les colonnes vides restent vides = FIABILIT√â GARANTIE
        logger.info("üö´ Cross-r√©f√©rencement D√âSACTIV√â - Extraction pure uniquement")
        
        # D√©duplication finale √† l'√©chelle du lot (s√©curis√©e)
        final_properties = self.deduplicate_batch_results(all_properties)
        
        # Statistiques de qualit√©
        removed_duplicates = len(all_properties) - len(final_properties)
        if removed_duplicates > 0:
            logger.info(f"üßπ {removed_duplicates} doublons supprim√©s")
        
        logger.info(f"‚úÖ Post-traitement termin√©: {len(final_properties)} propri√©t√©s FIABLES")
        logger.info("üéØ Toutes les donn√©es sont extraites directement des PDFs (AUCUNE invention)")
        
        return final_properties

    def analyze_missing_fields_batch(self, properties: List[Dict]) -> Dict:
        """
        Analyse les champs manquants √† l'√©chelle du lot complet.
        """
        missing_stats = {}
        total_props = len(properties)
        
        required_fields = [
            'department', 'commune', 'section', 'numero', 'contenance',
            'nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city'
        ]
        
        for field in required_fields:
            empty_count = sum(1 for prop in properties if not prop.get(field))
            if empty_count > 0:
                missing_stats[field] = {
                    'empty_count': empty_count,
                    'completion_rate': ((total_props - empty_count) / total_props) * 100
                }
        
        for field, stats in missing_stats.items():
            logger.info(f"üìä {field}: {stats['completion_rate']:.1f}% compl√©t√© ({stats['empty_count']} manquants)")
        
        return missing_stats

    def deduplicate_batch_results(self, properties: List[Dict]) -> List[Dict]:
        """
        ‚úÖ D√âDUPLICATION STRICTE CORRIG√âE - √âlimine les vrais doublons m√™me entre fichiers.
        """
        seen_keys = set()
        deduplicated = []
        
        for prop in properties:
            # ‚úÖ CL√â UNIQUE STRICTE - SANS fichier source pour √©liminer les vrais doublons
            key_parts = [
                prop.get('nom', ''),
                prop.get('prenom', ''),
                prop.get('section', ''),
                prop.get('numero', ''),
                prop.get('department', ''),
                prop.get('commune', ''),
                prop.get('numero_majic', ''),
                prop.get('droit_reel', '')  # Inclure le droit r√©el pour distinguer usufruitier/nu-prop
            ]
            unique_key = '|'.join(str(p).strip().upper() for p in key_parts)
            
            # ‚úÖ √âVITER les entr√©es compl√®tement vides ET les vrais doublons
            if unique_key not in seen_keys and unique_key != '|||||||':
                seen_keys.add(unique_key)
                deduplicated.append(prop)
            else:
                logger.debug(f"üóëÔ∏è Doublon √©limin√©: {prop.get('nom', '')}-{prop.get('section', '')}-{prop.get('numero', '')}")
        
        removed = len(properties) - len(deduplicated)
        if removed > 0:
            logger.info(f"üßπ {removed} doublons STRICTS supprim√©s (m√™me entre fichiers diff√©rents)")
        
        return deduplicated

    def export_to_csv_with_stats(self, all_properties: List[Dict]) -> None:
        """
        üßπ EXPORT CSV ET EXCEL AVEC VALIDATION FINALE ET STATISTIQUES D√âTAILL√âES.
        """
        if not all_properties:
            logger.warning("Aucune donn√©e √† exporter")
            return
        
        # üîç VALIDATION FINALE STRICTE avant export
        logger.info("üîç VALIDATION FINALE avant export...")
        validated_properties = self.final_validation_before_export(all_properties)
        
        if len(validated_properties) != len(all_properties):
            removed = len(all_properties) - len(validated_properties)
            logger.warning(f"üßΩ VALIDATION FINALE: {removed} propri√©t√©s contamin√©es supprim√©es")
        
        # Export CSV (avec point-virgule) ET Excel
        csv_path = self.export_to_csv(validated_properties)
        excel_path = self.export_to_excel(validated_properties, "output.xlsx")
        
        # G√©n√©rer des statistiques de qualit√©
        self.generate_quality_report(validated_properties)
        
        logger.info(f"‚úÖ EXPORTS TERMIN√âS AVEC VALIDATION:")
        logger.info(f"üìÑ CSV: {csv_path}")
        logger.info(f"üìä Excel: {excel_path}")
        logger.info(f"üõ°Ô∏è Donn√©es valid√©es: {len(validated_properties)} propri√©t√©s finales")

    def generate_quality_report(self, properties: List[Dict]) -> None:
        """
        G√©n√®re un rapport de qualit√© d√©taill√© pour le lot trait√©.
        MODE S√âCURIS√â - Extraction pure uniquement.
        """
        total_props = len(properties)
        
        # Calculer les taux de compl√©tion par champ
        required_fields = [
            'department', 'commune', 'section', 'numero', 'contenance',
            'nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city'
        ]
        
        completion_stats = {}
        for field in required_fields:
            filled_count = sum(1 for prop in properties if prop.get(field))
            completion_rate = (filled_count / total_props) * 100 if total_props > 0 else 0
            completion_stats[field] = completion_rate
        
        # Log du rapport de qualit√©
        logger.info("üìä RAPPORT DE QUALIT√â - MODE S√âCURIS√â")
        logger.info("=" * 50)
        logger.info(f"üõ°Ô∏è EXTRACTION PURE - Fiabilit√© 100% garantie")
        logger.info(f"Total propri√©t√©s extraites: {total_props}")
        logger.info("\nTaux de compl√©tion par champ:")
        
        for field, rate in completion_stats.items():
            status = "üü¢" if rate >= 90 else "üü°" if rate >= 70 else "üî¥"
            logger.info(f"  {status} {field:<20}: {rate:5.1f}%")
        
        # Moyenne globale
        avg_completion = sum(completion_stats.values()) / len(completion_stats)
        overall_status = "üü¢" if avg_completion >= 90 else "üü°" if avg_completion >= 70 else "üî¥"
        logger.info(f"\n{overall_status} TAUX GLOBAL DE COMPL√âTION: {avg_completion:.1f}%")
        
        # Message de s√©curit√©
        logger.info("\nüéØ GARANTIES DE FIABILIT√â:")
        logger.info("  ‚úÖ Toutes les donn√©es proviennent directement des PDFs")
        logger.info("  ‚úÖ Aucune invention ou interpolation de donn√©es")
        logger.info("  ‚úÖ Colonnes vides = vraiment absentes du PDF original")
        logger.info("  ‚úÖ Aucun risque de m√©lange entre propri√©taires/adresses")

    def process_like_make(self, pdf_path: Path) -> List[Dict]:
        """
        R√âPLIQUE EXACTE DU WORKFLOW MAKE - CORRIG√âE ANTI-DUPLICATION
        
        Suit exactement la m√™me logique que l'automatisation Make :
        1. pdfplumber pour les tableaux (comme Python Anywhere)
        2. OpenAI Vision simple pour les propri√©taires (prompt Make)
        3. D√âTECTION TYPE PDF et traitement adapt√©
        4. G√©n√©ration ID avec OpenAI (comme Make)
        5. Fusion 1:1 intelligente
        """
        logger.info(f"üéØ TRAITEMENT STYLE MAKE pour {pdf_path.name}")
        
        try:
            # üßπ √âTAPE 0: NETTOYAGE ANTI-CONTAMINATION (ultra-s√©curis√© si pas d√©j√† fait en batch)
            if not hasattr(self, '_batch_processing_state') or self._batch_processing_state != 'isolated':
                # Mode single PDF - utiliser nettoyage ultra-s√©curis√©
                self.clean_extraction_context(pdf_path)
            else:
                # Mode batch - nettoyage d√©j√† fait par batch_ultra_secure_cleanup
                logger.debug("üîí Nettoyage batch d√©j√† effectu√© - isolation pr√©serv√©e")
            
            # √âTAPE 1: Extraction tableaux (comme Python Anywhere)
            structured_data = self.extract_tables_with_pdfplumber(pdf_path)
            logger.info(f"üìã Tableaux extraits: {len(structured_data.get('prop_batie', []))} b√¢tis, {len(structured_data.get('non_batie', []))} non-b√¢tis")
            
            # √âTAPE 2: Extraction propri√©taires (prompt Make exact)
            owners = self.extract_owners_make_style(pdf_path)
            logger.info(f"Proprietaires extraits: {len(owners)}")
            
            # üîç √âTAPE 2.1: VALIDATION CROIS√âE ANTI-CONTAMINATION
            if not self.validate_extraction_consistency(owners, structured_data, pdf_path):
                logger.warning(f"‚ö†Ô∏è CONTAMINATION D√âTECT√âE - Application du nettoyage")
                owners = self.clean_contaminated_data(owners, pdf_path)
                logger.info(f"üßΩ Propri√©taires apr√®s nettoyage: {len(owners)}")
            
            # üîç VALIDATION EXTRACTION COMPL√àTE: V√©rifier si extraction semble incompl√®te
            if len(owners) > 0:
                # Estimer le nombre attendu de lignes bas√© sur les tableaux structur√©s
                expected_lines = len(structured_data.get('prop_batie', [])) + len(structured_data.get('non_batie', []))
                
                # Si diff√©rence significative, alerter et possiblement re-extraire
                if expected_lines > 0 and len(owners) < (expected_lines * 0.5):  # Si moins de 50% du attendu
                    logger.warning(f"‚ö†Ô∏è EXTRACTION POTENTIELLEMENT INCOMPL√àTE:")
                    logger.warning(f"   - Propri√©taires extraits: {len(owners)}")
                    logger.warning(f"   - Lignes tableaux d√©tect√©es: {expected_lines}")
                    logger.warning(f"   - Ratio: {len(owners)}/{expected_lines} = {len(owners)/expected_lines*100:.1f}%")
                    
                    # Strat√©gie de secours : extraction d'urgence
                    if len(owners) <= 2 and expected_lines > 5:
                        logger.warning(f"üÜò ACTIVATION EXTRACTION DE SECOURS pour donn√©es manquantes")
                        try:
                            # Re-extraction avec prompt ultra-directif sur donn√©es manquantes
                            images = self.pdf_to_images(pdf_path)
                            if images:
                                base64_image = base64.b64encode(images[0]).decode('utf-8')
                                backup_owners = self.extract_line_by_line_debug(base64_image, 1)
                                if len(backup_owners) > len(owners):
                                    logger.info(f"‚úÖ SECOURS R√âUSSI: {len(backup_owners)} vs {len(owners)} propri√©taires")
                                    owners = backup_owners
                        except Exception as e:
                            logger.error(f"‚ùå Erreur extraction de secours: {e}")
                
                logger.info(f"üë§ PROPRI√âTAIRES FINAUX APR√àS VALIDATION: {len(owners)}")
            
            # ‚úÖ CORRECTION CRITIQUE: Filtrage des VRAIS propri√©taires uniquement
            filtered_owners = []
            for owner in owners:
                nom = owner.get('nom', '').strip()
                prenom = owner.get('prenom', '').strip()
                if self.is_likely_real_owner(nom, prenom):
                    filtered_owners.append(owner)
                    logger.info(f"‚úÖ Vrai propri√©taire: {nom} {prenom}")
                else:
                    logger.info(f"‚ùå Rejet√© (lieu-dit/adresse): {nom} {prenom}")
            
            owners = filtered_owners
            logger.info(f"üë§ Propri√©taires valides apr√®s filtrage: {len(owners)}")
            
            # üö® D√âTECTION EXPLOSION COMBINATOIRE ULTRA-STRICTE
            if len(owners) > 50:  # Seuil tr√®s strict
                logger.error(f"üö® EXPLOSION D√âTECT√âE: {len(owners)} propri√©taires extraits (limite: 50)")
                logger.error(f"üí° CAUSE PROBABLE: Contamination ou explosion combinatoire")
                
                # Filtrer UNIQUEMENT les propri√©taires avec donn√©es g√©ographiques compl√®tes
                filtered_owners = []
                for owner in owners:
                    dept = owner.get('department', '').strip()
                    comm = owner.get('commune', '').strip()
                    nom = owner.get('nom', '').strip()
                    
                    # Ne garder QUE si : d√©partement ET commune ET nom valide
                    if dept and comm and nom and self.is_likely_real_owner(nom, owner.get('prenom', '')):
                        filtered_owners.append(owner)
                
                logger.warning(f"üßΩ FILTRAGE STRICT: {len(filtered_owners)} propri√©taires conserv√©s sur {len(owners)}")
                owners = filtered_owners[:20]  # Limite de s√©curit√© absolue
                logger.info(f"‚úÖ S√âCURIT√â: Limitation √† {len(owners)} propri√©taires maximum")
            
            if not owners and not structured_data.get('prop_batie') and not structured_data.get('non_batie'):
                logger.warning(f"Aucune donn√©e extraite pour {pdf_path.name}")
                return []
            
            # √âTAPE 3: üéØ D√âTECTION TYPE PDF ET TRAITEMENT ADAPT√â
            pdf_type = self.detect_pdf_ownership_type(owners, structured_data)
            logger.info(f"üîç Type PDF d√©tect√©: {pdf_type}")
            
            final_results = []
            
            # Traiter les propri√©t√©s non b√¢ties
            non_batie_props = structured_data.get('non_batie', [])
            if non_batie_props and owners:
                logger.info("üèûÔ∏è Traitement propri√©t√©s non b√¢ties style Make")
                
                if pdf_type == "single_owner":
                    # TYPE 2: Un seul propri√©taire pour toutes les propri√©t√©s
                    main_owner = self.select_main_owner(owners)
                    logger.info(f"üë§ Propri√©taire unique s√©lectionn√©: {main_owner.get('nom', '')} {main_owner.get('prenom', '')}")
                    
                    for prop in non_batie_props:
                        if prop.get('Adresse'):  # Filtre comme Make
                            # G√©n√©ration ID avec OpenAI (comme Make)
                            unique_id = self.generate_id_with_openai_like_make(main_owner, prop)
                            
                            # Fusion 1:1 simple (comme Make)
                            combined = self.merge_like_make(main_owner, prop, unique_id, 'non_batie', pdf_path.name)
                            final_results.append(combined)
                
                else:
                    # TYPE 1: Plusieurs propri√©taires (logique originale)
                    logger.info("üë• Multiple propri√©taires - association complexe")
                    
                    # üö® LIMITE ANTI-EXPLOSION : √âviter le produit cart√©sien massif
                    potential_combinations = len(owners) * len([p for p in non_batie_props if p.get('Adresse')])
                    if potential_combinations > 100:
                        logger.warning(f"üö® EXPLOSION D√âTECT√âE: {potential_combinations} combinaisons potentielles ({len(owners)} propri√©taires √ó {len([p for p in non_batie_props if p.get('Adresse')])} parcelles)")
                        logger.warning(f"üõ°Ô∏è LIMITE ACTIV√âE: Traitement en mode single_owner pour √©viter les donn√©es artificielles")
                        
                        # Basculer en mode single_owner avec le premier propri√©taire
                        main_owner = self.select_main_owner(owners)
                        for prop in non_batie_props:
                            if prop.get('Adresse'):
                                unique_id = self.generate_id_with_openai_like_make(main_owner, prop)
                                combined = self.merge_like_make(main_owner, prop, unique_id, 'non_batie', pdf_path.name)
                                final_results.append(combined)
                    else:
                        for owner in owners:
                            for prop in non_batie_props:
                                if prop.get('Adresse'):  # Filtre comme Make
                                    # G√©n√©ration ID avec OpenAI (comme Make)
                                    unique_id = self.generate_id_with_openai_like_make(owner, prop)
                                    
                                    # Fusion 1:1 simple (comme Make)
                                    combined = self.merge_like_make(owner, prop, unique_id, 'non_batie', pdf_path.name)
                                    final_results.append(combined)
            
            # Traiter les propri√©t√©s b√¢ties
            prop_batie = structured_data.get('prop_batie', [])
            if prop_batie and owners:
                logger.info("üè† Traitement propri√©t√©s b√¢ties style Make")
                
                if pdf_type == "single_owner":
                    # TYPE 2: Un seul propri√©taire pour toutes les propri√©t√©s
                    main_owner = self.select_main_owner(owners)
                    
                    for prop in prop_batie:
                        if prop.get('Adresse'):  # Filtre comme Make
                            # G√©n√©ration ID avec OpenAI (comme Make)
                            unique_id = self.generate_id_with_openai_like_make(main_owner, prop)
                            
                            # Fusion 1:1 simple (comme Make)
                            combined = self.merge_like_make(main_owner, prop, unique_id, 'batie', pdf_path.name)
                            final_results.append(combined)
                            
                else:
                    # TYPE 1: Plusieurs propri√©taires (logique originale)
                    
                    # üö® LIMITE ANTI-EXPLOSION : √âviter le produit cart√©sien massif
                    potential_combinations = len(owners) * len([p for p in prop_batie if p.get('Adresse')])
                    if potential_combinations > 100:
                        logger.warning(f"üö® EXPLOSION D√âTECT√âE (b√¢ties): {potential_combinations} combinaisons potentielles ({len(owners)} propri√©taires √ó {len([p for p in prop_batie if p.get('Adresse')])} parcelles)")
                        logger.warning(f"üõ°Ô∏è LIMITE ACTIV√âE: Traitement en mode single_owner pour √©viter les donn√©es artificielles")
                        
                        # Basculer en mode single_owner avec le premier propri√©taire
                        main_owner = self.select_main_owner(owners)
                        for prop in prop_batie:
                            if prop.get('Adresse'):
                                unique_id = self.generate_id_with_openai_like_make(main_owner, prop)
                                combined = self.merge_like_make(main_owner, prop, unique_id, 'batie', pdf_path.name)
                                final_results.append(combined)
                    else:
                        for owner in owners:
                            for prop in prop_batie:
                                if prop.get('Adresse'):  # Filtre comme Make
                                    # G√©n√©ration ID avec OpenAI (comme Make)
                                    unique_id = self.generate_id_with_openai_like_make(owner, prop)
                                    
                                    # Fusion 1:1 simple (comme Make)
                                    combined = self.merge_like_make(owner, prop, unique_id, 'batie', pdf_path.name)
                                    final_results.append(combined)
            
            # Si pas de structured data, juste les propri√©taires
            if not non_batie_props and not prop_batie and owners:
                logger.info("üë§ Seulement propri√©taires (pas de tableaux)")
                if pdf_type == "single_owner":
                    main_owner = self.select_main_owner(owners)
                    combined = self.merge_like_make(main_owner, {}, "", 'owners_only', pdf_path.name)
                    final_results.append(combined)
                else:
                    for owner in owners:
                        combined = self.merge_like_make(owner, {}, "", 'owners_only', pdf_path.name)
                        final_results.append(combined)
            
            # √âTAPE 4: S√©paration automatique des pr√©fixes coll√©s
            final_results = self.separate_stuck_prefixes(final_results)
            
            # √âTAPE 5: Propagation des valeurs manquantes (prefixe, contenance d√©taill√©e)
            final_results = self.propagate_values_downward(final_results, ['prefixe', 'contenance_ha', 'contenance_a', 'contenance_ca'])
            
            # √âTAPE 6: Suppression des lignes sans num√©ro de parcelle
            final_results = self.remove_empty_parcel_numbers(final_results, pdf_path.name)
            
            # √âTAPE 6.5: PROPAGATION G√âOGRAPHIQUE FORC√âE (ANTI-CONTAMINATION)
            logger.info("üéØ √âTAPE 6.5: Propagation g√©ographique forc√©e (anti-contamination)")
            
            if final_results:
                # Extraire g√©ographie depuis en-t√™te PDF si disponible
                location_info = self.extract_location_info(final_results, "", pdf_path.name)
                if location_info and location_info[0] if isinstance(location_info, list) else location_info:
                    header_data = location_info[0] if isinstance(location_info, list) else location_info
                    header_dept = str(header_data.get('department', '')).strip()
                    header_commune = str(header_data.get('commune', '')).strip()
                    
                    # V√©rifier si la g√©ographie de l'en-t√™te est valide
                    if (header_dept.isdigit() and len(header_dept) == 2 and 
                        header_commune.isdigit() and len(header_commune) == 3):
                        
                        # PROPAGATION FORC√âE sur toutes les lignes Unknown/invalides
                        propagated_count = 0
                        for prop in final_results:
                            dept = str(prop.get('department', '')).strip()
                            comm = str(prop.get('commune', '')).strip()
                            
                            # Si g√©ographie manquante/invalide ‚Üí forcer avec en-t√™te
                            if (not dept or not comm or 
                                dept in ['Unknown', 'XX', 'COMMUNE'] or 
                                comm in ['Unknown', 'XX', 'COMMUNE'] or
                                not dept.isdigit() or not comm.isdigit()):
                                
                                prop['department'] = header_dept
                                prop['commune'] = header_commune
                                propagated_count += 1
                        
                        logger.info(f"   ‚úÖ Propagation forc√©e: {propagated_count} lignes corrig√©es avec {header_dept}/{header_commune}")
                    else:
                        logger.warning(f"   ‚ö†Ô∏è En-t√™te g√©ographique invalide: {header_dept}/{header_commune}")
                else:
                    logger.warning(f"   ‚ö†Ô∏è Impossible d'extraire g√©ographie depuis en-t√™te PDF")
            
            # √âTAPE 7: Filtrage g√©ographique par r√©f√©rence (premi√®re ligne valide)
            final_results = self.filter_by_geographic_reference(final_results, pdf_path.name)
            
            logger.info(f"Traitement Make termine: {len(final_results)} proprietes finales")
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement Make {pdf_path.name}: {e}")
            return []

    def detect_pdf_ownership_type(self, owners: List[Dict], structured_data: Dict) -> str:
        """
        D√©tecte le type de PDF :
        - "single_owner" : Un seul propri√©taire r√©el (Type 2)
        - "multiple_owners" : Plusieurs propri√©taires distincts (Type 1)
        
        ‚úÖ AM√âLIOR√â : D√©tection plus fine des cas multi-propri√©taires
        """
        if not owners:
            return "multiple_owners"
        
        # Analyse d√©taill√©e des propri√©taires
        unique_owners = set()
        valid_owners = []
        family_names = {}
        droit_types = set()
        
        for owner in owners:
            nom = owner.get('nom', '').strip().upper()
            prenom = owner.get('prenom', '').strip().upper()
            droit = owner.get('droit_reel', '').strip().upper()
            owner_key = f"{nom}|{prenom}"
            unique_owners.add(owner_key)
            
            # Filtrer les vrais propri√©taires
            if self.is_likely_real_owner(nom, prenom):
                valid_owners.append(owner)
                
                # Analyser les noms de famille
                if nom:
                    if nom not in family_names:
                        family_names[nom] = set()
                    family_names[nom].add(prenom)
                
                # Collecter les types de droits
                if droit:
                    droit_types.add(droit)
        
        num_properties = len(structured_data.get('non_batie', [])) + len(structured_data.get('prop_batie', []))
        num_unique_owners = len(unique_owners)
        num_valid_owners = len(valid_owners)
        num_families = len(family_names)
        
        logger.info(f"üìä Analyse d√©taill√©e: {num_unique_owners} extraits, {num_valid_owners} valides, {num_families} familles, {num_properties} propri√©t√©s")
        logger.info(f"üìä Types de droits: {list(droit_types)}")
        
        # ‚úÖ SIGNAL FORT MULTI-PROPRI√âTAIRES : Usufruitier + Nu-propri√©taires
        critical_patterns = ['USUFRUITIER', 'NU-PROPRI√âTAIRE', 'NU-PROP', 'USUFRUIT']
        has_usufruit_pattern = any(pattern in ' '.join(droit_types) for pattern in critical_patterns)
        
        if has_usufruit_pattern:
            logger.info(f"üéØ D√âTECTION FORTE: Types de droits usufruitiers d√©tect√©s ‚Üí multiple_owners")
            return "multiple_owners"
        
        # ‚úÖ SIGNAL FORT : Plusieurs membres d'une m√™me famille
        multi_family_members = any(len(prenoms) > 1 for prenoms in family_names.values())
        if multi_family_members and num_families <= 3:
            logger.info(f"üéØ D√âTECTION FORTE: Plusieurs membres de famille(s) ‚Üí multiple_owners")
            return "multiple_owners"
        
        # ‚úÖ SIGNAL FORT : Noms de famille tr√®s diff√©rents
        if num_families >= 3 and num_valid_owners >= 3:
            logger.info(f"üéØ D√âTECTION FORTE: {num_families} familles diff√©rentes ‚Üí multiple_owners")
            return "multiple_owners"
        
        # ‚úÖ CRIT√àRE VOLUME : Si tr√®s peu de vrais propri√©taires pour beaucoup de propri√©t√©s
        if num_valid_owners <= 2 and num_properties > 50:
            logger.info(f"üéØ D√©tection: PDF type single_owner ({num_valid_owners} vrais propri√©taires pour {num_properties} propri√©t√©s)")
            return "single_owner"
        
        # ‚úÖ CRIT√àRE RATIO : Si ratio propri√©taires/propri√©t√©s tr√®s faible 
        ratio_valid = num_valid_owners / max(num_properties, 1)
        if ratio_valid < 0.05:  # Moins de 5% (plus strict pour √©viter erreurs)
            logger.info(f"üéØ D√©tection: PDF type single_owner (ratio {ratio_valid:.3f} tr√®s faible)")
            return "single_owner"
        
        # ‚úÖ CAS MULTIPLE : Si beaucoup de propri√©taires valides OU ratio √©lev√©
        if num_valid_owners > 5 or ratio_valid > 0.3:
            logger.info(f"üéØ D√©tection: PDF type multiple_owners ({num_valid_owners} propri√©taires, ratio {ratio_valid:.3f})")
            return "multiple_owners"
        
        # ‚úÖ PAR D√âFAUT CONSERVATEUR : En cas de doute, privil√©gier multiple_owners
        # (√âvite de rater des propri√©taires en assumant single_owner)
        logger.info(f"üéØ D√©tection conservatrice: PDF type multiple_owners par d√©faut (s√©curit√©)")
        return "multiple_owners"

    def select_main_owner(self, owners: List[Dict]) -> Dict:
        """
        S√©lectionne le propri√©taire principal quand il n'y en a qu'un seul r√©el.
        Priorise les personnes morales (communes, etc.) et les plus fr√©quents.
        """
        if not owners:
            return {}
        
        # Compter les occurrences de chaque propri√©taire
        owner_counts = {}
        for owner in owners:
            nom = owner.get('nom', '').strip()
            prenom = owner.get('prenom', '').strip()
            key = f"{nom}|{prenom}"
            
            if key not in owner_counts:
                owner_counts[key] = {'owner': owner, 'count': 0}
            owner_counts[key]['count'] += 1
        
        # Prioriser les personnes morales
        legal_entity_keywords = [
            'COM', 'COMMUNE', 'VILLE', 'MAIRIE', '√âTAT', 'D√âPARTEMENT', 'R√âGION',
            'SCI', 'SARL', 'SASU', 'EURL', 'SA', 'SOCI√âT√â', 'ENTERPRISE'
        ]
        
        for key, data in owner_counts.items():
            nom = data['owner'].get('nom', '').upper()
            for keyword in legal_entity_keywords:
                if keyword in nom:
                    logger.info(f"üè¢ Propri√©taire principal s√©lectionn√© (personne morale): {nom}")
                    return data['owner']
        
        # Sinon, prendre le plus fr√©quent
        most_frequent = max(owner_counts.values(), key=lambda x: x['count'])
        main_owner = most_frequent['owner']
        nom = main_owner.get('nom', '')
        prenom = main_owner.get('prenom', '')
        logger.info(f"üë§ Propri√©taire principal s√©lectionn√© (plus fr√©quent): {nom} {prenom}")
        
        return main_owner

    def extract_owners_make_style(self, pdf_path: Path) -> List[Dict]:
        """
        ‚úÖ EXTRACTION ULTRA-ROBUSTE - Strat√©gies multiples pour capturer TOUS les propri√©taires.
        
        Strat√©gies successives :
        1. Prompt standard am√©lior√©
        2. Extraction sp√©cialis√©e usufruitier/nu-propri√©taire
        3. Mode debugging avec extraction ligne par ligne
        4. Extraction d'urgence simplifi√©e
        """
        logger.info(f"üéØ EXTRACTION ULTRA-ROBUSTE pour {pdf_path.name}")
        
        # Convertir PDF en images
        images = self.pdf_to_images(pdf_path)
        if not images:
            return []
        
        all_owners = []
        
        for page_num, image_data in enumerate(images, 1):
            logger.info(f"üìÑ Traitement page {page_num}/{len(images)}")
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # üéØ STRAT√âGIE 1: Prompt ultra-directif (version am√©lior√©e)
            page_owners = self.extract_with_ultra_directive_prompt(base64_image, page_num)
            
            # ‚úÖ Si extraction insuffisante, essayer strat√©gies de secours
            if len(page_owners) <= 1:
                logger.warning(f"‚ö†Ô∏è Page {page_num}: Seulement {len(page_owners)} propri√©taire(s) - Activation strat√©gies de secours")
                
                # üéØ STRAT√âGIE 2: Extraction sp√©cialis√©e usufruitiers/nu-propri√©taires
                backup_owners = self.extract_usufruit_nu_propriete_specialized(base64_image, page_num)
                if len(backup_owners) > len(page_owners):
                    logger.info(f"üîÑ Strat√©gie usufruitier meilleure: {len(backup_owners)} vs {len(page_owners)}")
                    page_owners = backup_owners
                
                # üéØ STRAT√âGIE 3: Mode debugging ligne par ligne
                if len(page_owners) <= 1:
                    debug_owners = self.extract_line_by_line_debug(base64_image, page_num)
                    if len(debug_owners) > len(page_owners):
                        logger.info(f"üîÑ Mode debug meilleur: {len(debug_owners)} vs {len(page_owners)}")
                        page_owners = debug_owners
                
                # üéØ STRAT√âGIE 4: Extraction d'urgence ultra-simple
                if len(page_owners) <= 1:
                    emergency_owners = self.extract_emergency_all_names(base64_image, page_num)
                    if len(emergency_owners) > len(page_owners):
                        logger.info(f"üÜò Mode urgence meilleur: {len(emergency_owners)} vs {len(page_owners)}")
                        page_owners = emergency_owners
            
            # Ajouter les propri√©taires trouv√©s
            if page_owners:
                all_owners.extend(page_owners)
                logger.info(f"‚úÖ Page {page_num}: {len(page_owners)} propri√©taire(s) finalement extraits")
            else:
                logger.warning(f"‚ùå Page {page_num}: Aucun propri√©taire extrait malgr√© toutes les strat√©gies")
        
        logger.info(f"üìä TOTAL APR√àS TOUTES STRAT√âGIES: {len(all_owners)} propri√©taire(s)")
        
        # Post-traitement classique
        all_owners = self.detect_and_fix_legal_entities(all_owners)
        validated_owners = self.validate_complete_extraction(all_owners, pdf_path.name)
        
        return validated_owners
    
    def extract_with_ultra_directive_prompt(self, base64_image: str, page_num: int) -> List[Dict]:
        """Strat√©gie 1: Prompt ultra-directif avec emphase sur la multiplicit√©"""
        try:
            prompt = """üö® ALERTE CRITIQUE: Ce document peut contenir PLUSIEURS PROPRI√âTAIRES avec DIFF√âRENTS TYPES DE DROITS !

üéØ MISSION ABSOLUE: Trouve et extrait CHAQUE PERSONNE mentionn√©e dans ce document cadastral fran√ßais.

‚≠ê TYPES DE DROITS CRITIQUES √Ä IDENTIFIER:
- USUFRUITIER (personne qui a l'usufruit)
- NU-PROPRI√âTAIRE (personne qui a la nue-propri√©t√©)
- PROPRI√âTAIRE (pleine propri√©t√©)
- INDIVISAIRE (propri√©t√© en indivision)

üîç M√âTHODE DE SCAN SYST√âMATIQUE:

1Ô∏è‚É£ CHERCHE PARTOUT LES MOTS "TITULAIRE", "PROPRI√âTAIRE", "USUFRUITIER", "NU-PROPRI√âTAIRE"
2Ô∏è‚É£ POUR CHAQUE BLOC TROUV√â, LIS TOUS LES NOMS ET PR√âNOMS
3Ô∏è‚É£ NE T'ARR√äTE PAS apr√®s le premier - CONTINUE jusqu'√† la fin du document
4Ô∏è‚É£ REGARDE SP√âCIALEMENT S'IL Y A DES LISTES DE PERSONNES
5Ô∏è‚É£ ATTENTION aux h√©ritiers multiples (m√™me famille, pr√©noms diff√©rents)

‚ö†Ô∏è EXEMPLE TYPIQUE DE CE QUE TU DOIS TROUVER:
- 1 Usufruitier: [NOM_USUFRUITIER] [PRENOM_USUFRUITIER]
- 3 Nu-propri√©taires: [NOM_NU_PROP_1] [PRENOM_NU_PROP_1], [NOM_NU_PROP_2] [PRENOM_NU_PROP_2], [NOM_NU_PROP_3] [PRENOM_NU_PROP_3]

üö® R√àGLE VITALE: Si tu vois "Usufruitier" ET "Nu-propri√©taire", il y a FORC√âMENT PLUSIEURS PERSONNES !

R√âPONSE JSON:
{"owners": [
    {"nom": "[NOM_USUFRUITIER]", "prenom": "[PRENOM_USUFRUITIER]", "droit_reel": "Usufruitier", "street_address": "...", "city": "...", "post_code": "...", "numero_proprietaire": "...", "department": "...", "commune": "..."},
    {"nom": "[NOM_NU_PROP_1]", "prenom": "[PRENOM_NU_PROP_1]", "droit_reel": "Nu-propri√©taire", "street_address": "...", "city": "...", "post_code": "...", "numero_proprietaire": "...", "department": "...", "commune": "..."},
    {"nom": "[NOM_NU_PROP_2]", "prenom": "[PRENOM_NU_PROP_2]", "droit_reel": "Nu-propri√©taire", "street_address": "...", "city": "...", "post_code": "...", "numero_proprietaire": "...", "department": "...", "commune": "..."},
    {"nom": "[NOM_NU_PROP_3]", "prenom": "[PRENOM_NU_PROP_3]", "droit_reel": "Nu-propri√©taire", "street_address": "...", "city": "...", "post_code": "...", "numero_proprietaire": "...", "department": "...", "commune": "..."}
]}

üö® JAMAIS moins de propri√©taires qu'il n'y en a r√©ellement dans le document !"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}", "detail": "high"}}
                    ]
                }],
                max_tokens=3000,
                temperature=0.1,  # Plus d√©terministe
                response_format={"type": "json_object"}
            )
            
            result = safe_json_parse(response.choices[0].message.content, f"ultra-directif page {page_num}")
            return result.get("owners", []) if result else []
            
        except Exception as e:
            logger.error(f"Erreur strat√©gie ultra-directive page {page_num}: {e}")
            return []
    
    def extract_usufruit_nu_propriete_specialized(self, base64_image: str, page_num: int) -> List[Dict]:
        """Strat√©gie 2: Extraction sp√©cialis√©e pour les cas usufruitier/nu-propri√©taire"""
        try:
            prompt = """üéØ MISSION SP√âCIALIS√âE: Tu es un expert en droits d'usufruit et nue-propri√©t√©.

üìã TON OBJECTIF: Identifier TOUS les usufruitiers ET TOUS les nu-propri√©taires dans ce document.

üîç INDICES √Ä CHERCHER:
- Mots "USUFRUITIER", "USUFRUIT" ‚Üí personne qui a l'usufruit
- Mots "NU-PROPRI√âTAIRE", "NUE-PROPRI√âT√â" ‚Üí personne(s) qui ont la nue-propri√©t√©
- Souvent: 1 usufruitier + plusieurs nu-propri√©taires (enfants, h√©ritiers)

‚öñÔ∏è R√àGLE JURIDIQUE: L'usufruit + nue-propri√©t√© = propri√©t√© compl√®te
- USUFRUITIER = peut utiliser le bien (souvent le parent survivant)
- NU-PROPRI√âTAIRES = propri√©taires "en attente" (souvent les enfants)

üîç M√âTHODE DE RECHERCHE:
1. Cherche le mot "USUFRUITIER" - note la personne associ√©e
2. Cherche le mot "NU-PROPRI√âTAIRE" - note TOUTES les personnes associ√©es
3. Cherche dans les tableaux, listes, sections du document
4. Ne manque AUCUN nom mentionn√© avec ces droits

EXEMPLE TYPIQUE:
- [PRENOM_USUFRUITIER] [NOM_USUFRUITIER] (veuve) = Usufruitier
- Ses 3 enfants = Nu-propri√©taires

{"owners": [
    {"nom": "...", "prenom": "...", "droit_reel": "Usufruitier", "numero_proprietaire": "...", "street_address": "...", "city": "...", "post_code": "...", "department": "...", "commune": "..."},
    {"nom": "...", "prenom": "...", "droit_reel": "Nu-propri√©taire", "numero_proprietaire": "...", "street_address": "...", "city": "...", "post_code": "...", "department": "...", "commune": "..."}
]}"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}", "detail": "high"}}
                    ]
                }],
                max_tokens=2500,
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            
            result = safe_json_parse(response.choices[0].message.content, f"usufruit sp√©cialis√© page {page_num}")
            return result.get("owners", []) if result else []
            
        except Exception as e:
            logger.error(f"Erreur strat√©gie usufruit page {page_num}: {e}")
            return []
    
    def extract_line_by_line_debug(self, base64_image: str, page_num: int) -> List[Dict]:
        """Strat√©gie 3: Mode debugging - extraction ligne par ligne"""
        try:
            prompt = """üîç MODE DEBUGGING: Lis ce document ligne par ligne et trouve tous les noms de personnes.

üìã INSTRUCTIONS DE DEBUG:
1. Scanne le document de haut en bas
2. Pour CHAQUE ligne, note s'il y a un nom de personne
3. Ignore les adresses, lieux-dits, mais garde les vrais noms
4. Cherche particuli√®rement apr√®s les mots: TITULAIRE, PROPRI√âTAIRE, USUFRUITIER, NU-PROPRI√âTAIRE

üéØ PATTERN DE NOMS √Ä CHERCHER:
- NOM en MAJUSCULES + pr√©nom en minuscules
- Exemples: [NOM1] [Pr√©nom1], [NOM2] [Pr√©nom2], [NOM3] [Pr√©nom3]
- Codes associ√©s (6 caract√®res): M8BNF6, N7QX21, etc.

‚ö†Ô∏è √Ä IGNORER:
- Noms de rues: RUE DE..., AVENUE..., PLACE...
- Lieux-dits: MONT DE..., COTE DE..., VAL DE...

Retourne TOUS les noms trouv√©s:
{"owners": [
    {"nom": "NOM1", "prenom": "Pr√©nom1", "droit_reel": "...", "numero_proprietaire": "...", "street_address": "...", "city": "...", "post_code": "...", "department": "...", "commune": "..."}
]}"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}", "detail": "high"}}
                    ]
                }],
                max_tokens=2000,
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            
            result = safe_json_parse(response.choices[0].message.content, f"debug ligne-par-ligne page {page_num}")
            return result.get("owners", []) if result else []
            
        except Exception as e:
            logger.error(f"Erreur mode debug page {page_num}: {e}")
            return []
    
    def extract_emergency_all_names(self, base64_image: str, page_num: int) -> List[Dict]:
        """Strat√©gie 4: Extraction d'urgence - trouve tous les noms possibles"""
        try:
            prompt = """üÜò MODE URGENCE: Trouve TOUS les noms de personnes dans ce document, m√™me partiellement.

MISSION SIMPLE: Liste TOUS les noms que tu vois, m√™me si les informations sont incompl√®tes.

Cherche:
- Noms en MAJUSCULES
- Pr√©noms associ√©s  
- N'importe quel pattern de personne

{"owners": [
    {"nom": "TOUS_LES_NOMS_TROUV√âS", "prenom": "TOUS_LES_PR√âNOMS", "droit_reel": "", "numero_proprietaire": "", "street_address": "", "city": "", "post_code": "", "department": "", "commune": ""}
]}"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}", "detail": "high"}}
                    ]
                }],
                max_tokens=1500,
                temperature=0.2,
                response_format={"type": "json_object"}
            )
            
            result = safe_json_parse(response.choices[0].message.content, f"urgence page {page_num}")
            return result.get("owners", []) if result else []
            
        except Exception as e:
            logger.error(f"Erreur mode urgence page {page_num}: {e}")
            return []

    def generate_id_with_openai_like_make(self, owner: Dict, prop: Dict) -> str:
        """
        G√âN√âRATION D'ID CORRIG√âE - Utilise notre m√©thode locale ultra-robuste
        au lieu du prompt OpenAI d√©faillant qui g√©n√©rait des IDs √† 13 caract√®res.
        
        GARANTIT exactement 14 caract√®res √† chaque fois.
        """
        # Extraire les donn√©es comme Make
        department = owner.get('department', '')
        commune = owner.get('commune', '')
        raw_section = prop.get('Sec', '')
        plan_number = prop.get('N¬∞ Plan', '')
        raw_prefixe = prop.get('Pr√©fixe', prop.get('Pfxe', ''))  # Support des deux variantes
        
        # üîß NETTOYAGE PR√âALABLE: S√©parer pr√©fixe et section si coll√©s avec espace
        import re
        final_section = raw_section
        final_prefixe = raw_prefixe
        
        # Si pas de pr√©fixe et section contient pattern num√©rique+alphab√©tique avec espace
        if not raw_prefixe and raw_section:
            pattern = r'^(\d+)\s+([A-Z]+)$'  # \s+ pour d√©tecter les espaces
            match = re.match(pattern, raw_section)
            if match:
                final_prefixe = match.group(1)  # 302
                final_section = match.group(2)  # A (sans espace)
                logger.debug(f"üîß Section nettoy√©e pour ID: '{raw_section}' ‚Üí section='{final_section}' prefixe='{final_prefixe}'")
        
        # ‚úÖ UTILISATION DIRECTE de notre m√©thode locale CORRIG√âE
        # Plus fiable, plus rapide, et √©conomise les tokens OpenAI
        generated_id = self.generate_unique_id(
            str(department), str(commune), 
            str(final_section), str(plan_number), str(final_prefixe)
        )
        
        logger.debug(f"ID g√©n√©r√© localement (14 car. garantis): {generated_id}")
        return generated_id

    def detect_and_fix_legal_entities(self, owners: List[Dict]) -> List[Dict]:
        """
        D√©tecte et corrige les personnes morales (entreprises, communes, etc.)
        qui auraient pu √™tre mal extraites comme personnes physiques.
        """
        if not owners:
            return owners
            
        # Mots-cl√©s pour d√©tecter les personnes morales
        legal_entity_keywords = [
            'COM', 'COMMUNE DE', 'VILLE DE', 'MAIRIE DE',
            'SCI', 'SARL', 'SASU', 'EURL', 'SA ', 'SAS ',
            'SOCI√âT√â', 'ENTREPRISE', 'COMPAGNIE',
            'ASSOCIATION', 'FONDATION',
            '√âTAT', 'D√âPARTEMENT', 'R√âGION',
            'SYNDICAT', 'COLLECTIVIT√â',
            '√âTABLISSEMENT', 'INSTITUTION',
            'COOP√âRATIVE', 'MUTUELLE'
        ]
        
        corrected_owners = []
        
        for owner in owners:
            nom = str(owner.get('nom', '')).upper().strip()
            prenom = str(owner.get('prenom', '')).strip()
            
            # V√©rifier si c'est une personne morale
            is_legal_entity = False
            full_entity_name = nom
            
            # Cas 1: Le nom contient d√©j√† un mot-cl√© de personne morale
            for keyword in legal_entity_keywords:
                if keyword in nom:
                    is_legal_entity = True
                    # Si il y a aussi un pr√©nom, reconstruire le nom complet
                    if prenom:
                        full_entity_name = f"{nom} {prenom}".strip()
                    break
            
            # Cas 2: Le pr√©nom contient un mot-cl√© (mal extrait)
            if not is_legal_entity and prenom:
                prenom_upper = prenom.upper()
                for keyword in legal_entity_keywords:
                    if keyword in prenom_upper:
                        is_legal_entity = True
                        # Reconstruire le nom complet
                        full_entity_name = f"{nom} {prenom}".strip()
                        break
            
            # Cas 3: Nom + pr√©nom forment ensemble une personne morale
            if not is_legal_entity and nom and prenom:
                combined = f"{nom} {prenom}".upper()
                for keyword in legal_entity_keywords:
                    if keyword in combined:
                        is_legal_entity = True
                        full_entity_name = combined
                        break
            
            # Cr√©er l'entr√©e corrig√©e
            corrected_owner = owner.copy()
            
            if is_legal_entity:
                corrected_owner['nom'] = full_entity_name
                corrected_owner['prenom'] = ''  # Vider le pr√©nom pour les personnes morales
                logger.info(f"üè¢ Personne morale d√©tect√©e: '{full_entity_name}'")
            
            corrected_owners.append(corrected_owner)
        
        return corrected_owners

    def validate_complete_extraction(self, owners: List[Dict], filename: str) -> List[Dict]:
        """
        ‚úÖ VALIDATION CRITIQUE: V√©rification que l'extraction est compl√®te.
        
        Analyse les signaux d'extraction incompl√®te et alerte si n√©cessaire.
        """
        if not owners:
            logger.warning(f"üö® EXTRACTION INCOMPL√àTE: Aucun propri√©taire extrait pour {filename}")
            return owners
        
        logger.info(f"üîç Validation extraction: {len(owners)} propri√©taire(s) pour {filename}")
        
        # Analyser les patterns qui sugg√®rent une extraction incompl√®te
        unique_names = set()
        name_counts = {}
        droit_types = set()
        
        for owner in owners:
            nom = owner.get('nom', '').strip()
            prenom = owner.get('prenom', '').strip()
            droit = owner.get('droit_reel', '').strip()
            
            full_name = f"{nom} {prenom}".strip()
            unique_names.add(full_name)
            
            if full_name in name_counts:
                name_counts[full_name] += 1
            else:
                name_counts[full_name] = 1
            
            if droit:
                droit_types.add(droit.upper())
        
        # ‚úÖ SIGNAL 1: Tr√®s peu de propri√©taires uniques
        if len(unique_names) == 1 and len(owners) > 5:
            logger.warning(f"üö® POTENTIEL PROBL√àME: Un seul nom unique ({list(unique_names)[0]}) r√©p√©t√© {len(owners)} fois")
            logger.warning(f"üí° SUGGESTION: V√©rifier si le document contient d'autres propri√©taires")
        
        # ‚úÖ SIGNAL 2: M√©lange de types de droits sugg√®re multi-propri√©taires
        critical_droit_patterns = ['USUFRUITIER', 'NU-PROPRI√âTAIRE', 'NU-PROP', 'USUFRUIT']
        has_critical_patterns = any(pattern in ' '.join(droit_types) for pattern in critical_droit_patterns)
        
        if has_critical_patterns and len(unique_names) == 1:
            logger.warning(f"üö® ALERTE CRITIQUE: Types de droits multiples d√©tect√©s ({droit_types}) mais un seul propri√©taire")
            logger.warning(f"üí° CONSEIL: Un usufruitier implique g√©n√©ralement plusieurs nu-propri√©taires")
        
        # ‚úÖ SIGNAL 3: Noms de famille identiques avec pr√©noms diff√©rents
        family_names = {}
        for owner in owners:
            nom = owner.get('nom', '').strip().upper()
            prenom = owner.get('prenom', '').strip()
            if nom:
                if nom not in family_names:
                    family_names[nom] = set()
                family_names[nom].add(prenom)
        
        for family_name, prenoms in family_names.items():
            if len(prenoms) > 1:
                logger.info(f"‚úÖ Famille {family_name}: {len(prenoms)} pr√©noms trouv√©s ‚Üí Extraction multi-h√©ritiers probable")
        
        # ‚úÖ RAPPORT DE VALIDATION
        logger.info(f"üìä RAPPORT VALIDATION {filename}:")
        logger.info(f"   üë• {len(owners)} propri√©taires extraits")
        logger.info(f"   üî§ {len(unique_names)} noms uniques")
        logger.info(f"   ‚öñÔ∏è {len(droit_types)} types de droits: {list(droit_types)}")
        
        # Si extraction semble compl√®te
        if len(unique_names) > 1 or (len(unique_names) == 1 and len(owners) == 1):
            logger.info(f"‚úÖ Validation r√©ussie: extraction semble compl√®te")
        else:
            logger.warning(f"‚ö†Ô∏è Validation incertaine: v√©rifier manuellement le PDF")
        
        return owners

    def merge_like_make(self, owner: Dict, prop: Dict, unique_id: str, prop_type: str, pdf_path_name: str) -> Dict:
        """
        ‚úÖ FUSION CORRIG√âE avec gestion optimis√©e des contenances et adresses.
        """
        
        # S√âPARATION AUTOMATIQUE DES PR√âFIXES COLL√âS
        import re
        raw_section = str(prop.get('Sec', ''))
        raw_prefixe = str(prop.get('Pr√©fixe', prop.get('Pfxe', '')))
        
        # Si pas de pr√©fixe et section contient pattern num√©rique+alphab√©tique
        if not raw_prefixe and raw_section:
            pattern = r'^(\d+)\s*([A-Z]+)$'  # \s* permet les espaces optionnels
            match = re.match(pattern, raw_section)
            if match:
                final_prefixe = match.group(1)  # 302
                final_section = match.group(2)  # A
                logger.info(f"üîç PR√âFIXE S√âPAR√â: '{raw_section}' ‚Üí pr√©fixe='{final_prefixe}' section='{final_section}'")
            else:
                final_prefixe = raw_prefixe
                final_section = raw_section
        else:
            final_prefixe = raw_prefixe
            final_section = raw_section
        
        # ‚úÖ CORRECTION CONTENANCE : Gestion des formats fran√ßais et parsing robuste
        contenance_ha = self.parse_contenance_value(prop.get('HA', prop.get('Contenance', '')))
        contenance_a = self.parse_contenance_value(prop.get('A', ''))
        contenance_ca = self.parse_contenance_value(prop.get('CA', ''))
        
        # ‚úÖ CORRECTION NOMS/PR√âNOMS : S√©paration intelligente des noms compos√©s
        nom_final, prenom_final = self.split_name_intelligently(
            owner.get('nom', ''), owner.get('prenom', '')
        )
        
        # ‚úÖ CORRECTION ADRESSES : Nettoyage et validation des adresses
        voie_cleaned = self.clean_address(owner.get('street_address', ''))
        
        # Mapping exact comme dans Make Google Sheets (CORRIG√â)
        merged = {
            # Colonnes A-E (informations parcelle)
            'department': str(owner.get('department', '')),  # Colonne A
            'commune': clean_commune_code(str(owner.get('commune', ''))),        # Colonne B - CORRIG√â avec nettoyage  
            'prefixe': final_prefixe,                        # Colonne C (CORRIG√â - s√©paration auto)
            'section': final_section,                        # Colonne D (CORRIG√â - s√©paration auto)
            'numero': str(prop.get('N¬∞ Plan', '')),         # Colonne E
            
            # Colonnes F-H (gestion/demande - vides dans Make)
            'demandeur': '',    # Colonne F
            'date': '',         # Colonne G  
            'envoye': '',       # Colonne H
            
            # Colonne I (designation + contenance d√©taill√©e CORRIG√âE)
            'designation_parcelle': str(prop.get('Adresse', '')),  # Colonne I
            'contenance_ha': contenance_ha,     # ‚úÖ CORRIG√â - Parsing fran√ßais
            'contenance_a': contenance_a,       # ‚úÖ CORRIG√â - Parsing fran√ßais  
            'contenance_ca': contenance_ca,     # ‚úÖ CORRIG√â - Parsing fran√ßais
            
            # Colonnes J-O (propri√©taire CORRIG√âES)
            'nom': nom_final,                                    # ‚úÖ CORRIG√â - S√©paration intelligente
            'prenom': prenom_final,                             # ‚úÖ CORRIG√â - S√©paration intelligente
            'numero_majic': str(owner.get('numero_proprietaire', '')),  # Colonne L
            'voie': voie_cleaned,                               # ‚úÖ CORRIG√â - Adresse nettoy√©e
            'post_code': str(owner.get('post_code', '')),       # Colonne N
            'city': str(owner.get('city', '')),                 # Colonne O
            
            # Colonnes P-R (statuts - vides dans Make)
            'identifie': '',    # Colonne P
            'rdp': '',          # Colonne Q
            'sig': '',          # Colonne R
            
            # Colonnes S-T (ID et droit)
            'id': unique_id,                                    # Colonne S
            'droit_reel': str(owner.get('droit_reel', '')),    # Colonne T - ‚úÖ CORRIG√â: cl√© avec underscore
            
            # M√©tadonn√©es internes
            'fichier_source': pdf_path_name,
            'type_propriete': prop_type
        }
        
        return merged

    def generate_parcel_id(self, department: str, commune: str, section: str = None, plan_number: int = None) -> str:
        """
        G√©n√®re un identifiant parcellaire selon la formule sp√©cifi√©e.
        
        Args:
            department: Code d√©partement (2 chiffres)
            commune: Code commune (3 chiffres)
            section: Section (5 caract√®res, optionnel)
            plan_number: Num√©ro de plan (4 chiffres, optionnel)
            
        Returns:
            Identifiant parcellaire format√©
        """
        # Utiliser les valeurs par d√©faut si non fournies
        if section is None:
            section = self.default_section
        if plan_number is None:
            plan_number = self.default_plan_number
        
        # Formater selon les r√®gles
        dept_formatted = department.zfill(2)
        commune_formatted = commune.zfill(3)
        section_formatted = str(section).ljust(5, '0')[:5]
        plan_formatted = str(plan_number).zfill(4)
        
        parcel_id = f"{dept_formatted}{commune_formatted}{section_formatted}{plan_formatted}"
        return parcel_id

    def decompose_contenance(self, contenance: str) -> Dict[str, str]:
        """
        D√©compose une contenance de 7 chiffres en hectares, ares et centiares.
        
        Args:
            contenance: Cha√Æne de 7 chiffres (ex: "0130221")
            
        Returns:
            Dictionnaire avec les cl√©s HA, A, CA
        """
        if not contenance or contenance == "N/A" or len(contenance) != 7 or not contenance.isdigit():
            return {"HA": "N/A", "A": "N/A", "CA": "N/A"}
        
        # D√©composer selon le format : HHAACC
        ha = contenance[:2]  # 2 premiers chiffres (hectares)
        a = contenance[2:4]  # 2 suivants (ares)
        ca = contenance[4:7]  # 3 derniers (centiares)
        
        return {"HA": ha, "A": a, "CA": ca}

    def is_likely_real_owner(self, nom: str, prenom: str) -> bool:
        """
        D√©termine si un nom/pr√©nom correspond √† un vrai propri√©taire
        ou √† une adresse/lieu confondu par GPT-4 Vision.
        
        RENFORC√â : Filtre strictement les r√©sidus parasites et lignes artificielles.
        """
        if not nom.strip():
            return False
            
        nom_upper = nom.upper().strip()
        prenom_clean = prenom.strip()
        
        # üö® FILTRE STRICT: √âliminer imm√©diatement les mots parasites sp√©cifiques
        parasitic_words = [
            'AUX', 'LAVES', 'ECHASSIR', 'ECURIE', 'GABOIS', 'NAUX', 'MANDE', 
            'NOIX', 'MONTANT', 'PUISEAU', 'REMEMBRES', 'PRINCESSES', 'PARC',
            'MARECHAUX', 'AUXERRE', 'FORETS', 'BLANC', 'MALVOISINE', 'LONGEVAS',
            'VAL', 'COTE', 'MONT', 'CHAMPS', 'PRES', 'BOIS', 'DESSUS', 'DESSOUS'
        ]

        # Si le nom exact correspond √† un mot parasite, REJETER imm√©diatement
        if nom_upper in parasitic_words:
            logger.debug(f"üóëÔ∏è REJET√â (mot parasite): {nom}")
            return False

        # Si le nom commence par un pattern suspect, REJETER
        suspicious_starts = ['LE ', 'LA ', 'LES ', 'DU ', 'DE ', 'AU ', 'AUX ']
        if any(nom_upper.startswith(pattern) for pattern in suspicious_starts):
            logger.debug(f"üóëÔ∏è REJET√â (pr√©fixe suspect): {nom}")
            return False
        
        # ‚úÖ CRIT√àRE 1: Personnes morales (communes, soci√©t√©s) - PRIORIT√â ABSOLUE
        legal_entity_keywords = [
            'COM', 'COMMUNE', 'VILLE', 'MAIRIE', '√âTAT', 'D√âPARTEMENT', 'R√âGION',
            'SCI', 'SARL', 'SASU', 'EURL', 'SA', 'SOCI√âT√â', 'ENTERPRISE',
            'ASSOCIATION', 'SYNDICAT', 'FEDERATION', 'UNION'
        ]
        
        for keyword in legal_entity_keywords:
            if keyword in nom_upper and len(nom.strip()) >= 8:  # Nom suffisamment long
                return True
        
        # ‚úÖ CRIT√àRE 2: Rejet des adresses/lieux-dits (APR√àS v√©rification personnes morales)
        if self.looks_like_address(nom_upper):
            return False
        
        # ‚úÖ CRIT√àRE 3: Personnes physiques avec pr√©nom
        if prenom_clean and len(prenom_clean) >= 2:
            # Nom de famille classique (pas trop court, pas d'adresse)
            if len(nom.strip()) >= 3 and not self.looks_like_address(nom_upper):
                return True
        
        # ‚úÖ CRIT√àRE 4: Noms de famille seuls mais plausibles
        if not prenom_clean:
            # Doit ressembler √† un nom de famille (pas d'adresse, longueur raisonnable)
            if (len(nom.strip()) >= 5 and 
                not self.looks_like_address(nom_upper) and
                not any(char.isdigit() for char in nom) and  # Pas de chiffres
                nom_upper not in ['N/A', 'NULL', 'VIDE', 'INCONNU']):
                return True
        
        # ‚úÖ CRIT√àRE 5: Patterns de noms classiques fran√ßais
        classic_patterns = ['MC', 'MAC', 'DE ', 'DU ', 'LE ', 'LA ']
        for pattern in classic_patterns:
            if pattern in nom_upper and len(nom.strip()) >= 5:
                return True
        
        # Par d√©faut : REJETER si aucun crit√®re positif
        return False

    def looks_like_address(self, nom_upper: str) -> bool:
        """
        D√©termine si un nom ressemble √† une adresse plut√¥t qu'√† un propri√©taire.
        """
        # Mots-cl√©s d'adresses (tr√®s √©tendu)
        address_keywords = [
            'RUE', 'AVENUE', 'PLACE', 'CHEMIN', 'ROUTE', 'LIEU-DIT', 'IMPASSE',
            'AU VILLAGE', 'AU ', 'LA ', 'LE ', 'LES ', 'DE LA', 'DU ', 'DES ',
            'CHAMPS', 'PRES', 'BOIS', 'FORET', 'COTE', 'SUR ', 'SOUS ', 'HAUTE',
            'DESSUS', 'DESSOUS', 'HAUT', 'BAS', 'GRAND', 'PETIT', 'VIEUX', 'NOUVEAU',
            'GRANDE', 'PETITE', 'VIEILLE', 'NOUVELLE', 'RANG', 'TETE', 'BOUT',
            'MILIEU', 'ENTRE', 'VERS', 'PRES DE', 'PROCHE', 'CUDRET', 'SEUT',
            'ROCHE', 'PIERRE', 'MONT', 'COL', 'VALLEE', 'PLAINE', 'PLATEAU',
            # ‚úÖ AJOUTS SP√âCIFIQUES aux donn√©es utilisateur
            'NOIX', 'MANDE', 'NAUX', 'GOBAIN', 'MONTANT', 'REMEMBRES', 'PUISEAU',
            'GABOIS', 'PRINCESSES', 'PARC', 'MARECHAUX', 'AUXERRE', 'FORETS',
            'BLANC', 'MALVOISINE', 'LONGEVAS',
            # üö® AJOUTS ANTI-R√âSIDUS STRICTS
            'CUDRET', 'SEUT', 'GIBELIN', 'VALLON', 'TERRES', 'CHAMP', 'PRE',
            'VIGNE', 'VIGNOBLE', 'ETANG', 'MARE', 'SOURCE', 'FONTAINE',
            'CROIX', 'CALVAIRE', 'CHAPELLE', 'MOULIN', 'FERME', 'GRANGE'
        ]
        
        # Si le nom contient des mots-cl√©s d'adresse
        for keyword in address_keywords:
            if keyword in nom_upper:
                return True
        
        # Patterns d'adresses typiques (enrichis)
        address_patterns = [
            'GIRARDET', 'HAUTETERRE', 'HAUTEPIERRE', 'REISSILLE',
            # ‚úÖ Patterns sp√©cifiques aux donn√©es utilisateur
            'MONT DE NOIX', 'COTE DE MANDE', 'SUR LES NAUX', 'MONTANT DU NOYER',
            'VAL DE PUISEAU', 'LA VALLEE DE', 'CHE DES VIGNES', 'RUE D EN HAUT',
            'RENVERS DES FORETS', 'HAM DE MALVOISINE', 'VIEILLE RUE D'
        ]
        
        for pattern in address_patterns:
            if pattern in nom_upper:
                return True
        
        return False

    def clean_inconsistent_location_data(self, properties: List[Dict], filename: str) -> List[Dict]:
        """
        Nettoie les incoh√©rences g√©ographiques par fichier source.
        
        Pour chaque fichier :
        1. Analyser TOUTES les lignes pour identifier le couple (d√©partement, commune) de r√©f√©rence
        2. En cas d'√©galit√©, prendre la premi√®re localisation dans l'ordre du fichier
        3. Supprimer toutes les lignes avec un couple diff√©rent ou des valeurs vides
        
        Args:
            properties: Liste des propri√©t√©s √† nettoyer
            filename: Nom du fichier pour les logs
            
        Returns:
            Liste filtr√©e sans les lignes g√©ographiquement incoh√©rentes
        """
        if not properties:
            return properties
        
        initial_count = len(properties)
        
        # Grouper par fichier source
        files_groups = {}
        for prop in properties:
            file_source = prop.get('fichier_source', 'unknown')
            if file_source not in files_groups:
                files_groups[file_source] = []
            files_groups[file_source].append(prop)
        
        cleaned_properties = []
        total_removed = 0
        
        # Traiter chaque fichier s√©par√©ment
        for file_source, file_props in files_groups.items():
            logger.info(f"üåç Nettoyage g√©ographique pour {file_source}: {len(file_props)} lignes")
            
            # √âTAPE 1: Analyser TOUTES les lignes pour identifier la r√©f√©rence g√©ographique
            location_counts = {}
            location_first_occurrence = {}  # M√©moriser l'ordre d'apparition
            
            for index, prop in enumerate(file_props):
                dept = str(prop.get('department', '')).strip()
                commune = str(prop.get('commune', '')).strip()
                
                # Ignorer les valeurs vides pour la r√©f√©rence
                if dept and commune and dept != 'N/A' and commune != 'N/A':
                    location_key = f"{dept}-{commune}"
                    
                    # Compter les occurrences
                    location_counts[location_key] = location_counts.get(location_key, 0) + 1
                    
                    # M√©moriser la premi√®re occurrence pour le d√©partage
                    if location_key not in location_first_occurrence:
                        location_first_occurrence[location_key] = index
            
            # Si aucune r√©f√©rence valide trouv√©e, garder toutes les lignes
            if not location_counts:
                logger.warning(f"‚ö†Ô∏è {file_source}: Aucune r√©f√©rence g√©ographique trouv√©e, conservation de toutes les lignes")
                cleaned_properties.extend(file_props)
                continue
            
            # Couple de r√©f√©rence = le premier dans l'ordre du fichier (pour respecter votre demande)
            # Modification : au lieu de prendre le plus fr√©quent, prendre le premier
            reference_location = min(location_counts.keys(), key=lambda loc: location_first_occurrence[loc])
            
            # ‚úÖ CORRECTION CRITIQUE: Gestion des tirets multiples dans la cl√©
            try:
                # Utiliser split avec maxsplit=1 pour ne s√©parer que sur le premier tiret
                parts = reference_location.split('-', 1)
                if len(parts) == 2:
                    ref_dept, ref_commune = parts
                else:
                    # Fallback si le format est inattendu
                    logger.warning(f"‚ö†Ô∏è Format de r√©f√©rence inattendu: {reference_location}")
                    ref_dept = parts[0] if parts else ""
                    ref_commune = ""
            except Exception as e:
                logger.error(f"‚ùå Erreur parsing r√©f√©rence g√©ographique '{reference_location}': {e}")
                # En cas d'erreur, garder toutes les lignes de ce fichier
                cleaned_properties.extend(file_props)
                continue
            
            logger.info(f"   - R√©f√©rence identifi√©e: d√©partement={ref_dept}, commune={ref_commune}")
            logger.info(f"   - Bas√©e sur la premi√®re occurrence dans le fichier")
            
            # √âTAPE 2: Filtrer selon la r√©f√©rence
            file_cleaned = []
            file_removed = 0
            
            for prop in file_props:
                dept = str(prop.get('department', '')).strip()
                commune = str(prop.get('commune', '')).strip()
                
                # Supprimer si valeurs vides
                if not dept or not commune or dept == 'N/A' or commune == 'N/A':
                    file_removed += 1
                    logger.debug(f"      üóëÔ∏è Supprim√© (valeurs vides): {prop.get('nom', 'N/A')} - dept={dept}, commune={commune}")
                    continue
                
                # Supprimer si diff√©rent de la r√©f√©rence
                if dept != ref_dept or commune != ref_commune:
                    file_removed += 1
                    logger.debug(f"      üóëÔ∏è Supprim√© (incoh√©rent): {prop.get('nom', 'N/A')} - {dept}-{commune} vs {reference_location}")
                    continue
                
                # Garder si coh√©rent
                file_cleaned.append(prop)
            
            cleaned_properties.extend(file_cleaned)
            total_removed += file_removed
            
            logger.info(f"   - ‚úÖ {len(file_cleaned)} lignes conserv√©es, {file_removed} supprim√©es")
        
        final_count = len(cleaned_properties)
        logger.info(f"üéØ √âTAPE 7 TERMIN√âE: {total_removed} ligne(s) supprim√©e(s) au total, {final_count} conserv√©e(s)")
        
        return cleaned_properties

    def filter_by_geographic_reference(self, properties: List[Dict], filename: str) -> List[Dict]:
        """
        Filtrage g√©ographique par r√©f√©rence : prend la premi√®re ligne valide comme r√©f√©rence
        et supprime toutes les lignes avec un d√©partement/commune diff√©rent.
        
        Args:
            properties: Liste des propri√©t√©s √† filtrer  
            filename: Nom du fichier pour les logs
            
        Returns:
            Liste filtr√©e selon la r√©f√©rence g√©ographique de la premi√®re ligne valide
        """
        if not properties:
            return properties
        
        initial_count = len(properties)
        
        # Grouper par fichier source pour traiter chaque PDF s√©par√©ment
        files_groups = {}
        for prop in properties:
            file_source = prop.get('fichier_source', 'unknown')
            if file_source not in files_groups:
                files_groups[file_source] = []
            files_groups[file_source].append(prop)
        
        filtered_properties = []
        total_removed = 0
        
        # Traiter chaque fichier s√©par√©ment
        for file_source, file_props in files_groups.items():
            logger.info(f"üéØ Filtrage g√©ographique par r√©f√©rence pour {file_source}: {len(file_props)} lignes")
            
            # √âTAPE 1: Trouver la PREMI√àRE g√©ographie R√âELLEMENT VALIDE (ANTI-CONTAMINATION ULTRA-STRICT)
            reference_dept = None
            reference_commune = None
            reference_found_at = -1
            
            # Chercher la premi√®re g√©ographie avec crit√®res ultra-stricts
            for index, prop in enumerate(file_props):
                dept = str(prop.get('department', '')).strip()
                commune = str(prop.get('commune', '')).strip()
                
                # ‚úÖ CRIT√àRES ULTRA-STRICTS pour g√©ographie valide
                if (dept and commune and 
                    dept not in ['', 'N/A', 'None', 'XX', 'Unknown'] and 
                    commune not in ['', 'N/A', 'None', 'COMMUNE', 'Unknown'] and
                    # OBLIGATOIRE : codes num√©riques seulement
                    dept.isdigit() and len(dept) == 2 and
                    commune.isdigit() and len(commune) == 3):
                    
                    reference_dept = dept
                    reference_commune = commune
                    reference_found_at = index
                    logger.info(f"   üìç R√©f√©rence VALIDE trouv√©e: {dept}/{commune} (ligne {index+1})")
                    break
            
            # MODE DE SECOURS si aucune g√©ographie parfaitement valide
            if reference_dept is None:
                logger.warning(f"‚ö†Ô∏è AUCUNE g√©ographie ultra-valide trouv√©e - Mode de secours activ√©")
                for index, prop in enumerate(file_props):
                    dept = str(prop.get('department', '')).strip()
                    commune = str(prop.get('commune', '')).strip()
                    if (dept and commune and 
                        any(c.isdigit() for c in dept) and any(c.isdigit() for c in commune) and
                        dept not in ['XX', 'COMMUNE', 'Unknown'] and commune not in ['XX', 'COMMUNE', 'Unknown']):
                        reference_dept = dept
                        reference_commune = commune
                        logger.warning(f"   üÜò Mode de secours: {dept}/{commune}")
                        break
            
            # Si aucune r√©f√©rence trouv√©e, ignorer le filtrage pour ce fichier
            if reference_dept is None or reference_commune is None:
                logger.warning(f"‚ö†Ô∏è {file_source}: Aucune r√©f√©rence g√©ographique valide trouv√©e - conservation de toutes les lignes")
                filtered_properties.extend(file_props)
                continue
            
            # √âTAPE 2: Filtrer selon la r√©f√©rence
            file_filtered = []
            file_removed = 0
            
            for index, prop in enumerate(file_props):
                dept = str(prop.get('department', '')).strip()
                commune = str(prop.get('commune', '')).strip()
                
                # Ignorer le filtrage si d√©partement/commune vides ou sans chiffres (comme demand√©)
                if (not dept or not commune or dept in ['', 'N/A', 'None'] or commune in ['', 'N/A', 'None'] or
                    not any(c.isdigit() for c in dept) or not any(c.isdigit() for c in commune)):
                    file_filtered.append(prop)
                    logger.debug(f"      ‚è≠Ô∏è Ignor√© (valeurs vides ou sans chiffres): ligne {index + 1}")
                    continue
                
                # Garder si correspond √† la r√©f√©rence
                if dept == reference_dept and commune == reference_commune:
                    file_filtered.append(prop)
                    logger.debug(f"      ‚úÖ Conserv√© (r√©f√©rence): ligne {index + 1}")
                else:
                    # Supprimer si diff√©rent de la r√©f√©rence
                    file_removed += 1
                    logger.debug(f"      üóëÔ∏è Supprim√© (hors r√©f√©rence): ligne {index + 1} - {dept}-{commune} vs {reference_dept}-{reference_commune}")
            
            filtered_properties.extend(file_filtered)
            total_removed += file_removed
            
            logger.info(f"   ‚úÖ {len(file_filtered)} lignes conserv√©es, {file_removed} supprim√©es")
        
        final_count = len(filtered_properties)
        logger.info(f"üéØ FILTRAGE G√âOGRAPHIQUE TERMIN√â: {total_removed} ligne(s) supprim√©e(s) au total, {final_count} conserv√©e(s)")
        
        return filtered_properties

    def remove_empty_parcel_numbers(self, properties: List[Dict], filename: str) -> List[Dict]:
        """
        Supprime les lignes o√π la colonne 'numero' (num√©ro de parcelle) est vide.
        
        Args:
            properties: Liste des propri√©t√©s √† filtrer
            filename: Nom du fichier pour les logs
            
        Returns:
            Liste filtr√©e sans les lignes avec num√©ro de parcelle vide
        """
        if not properties:
            return properties
        
        initial_count = len(properties)
        
        # Filtrer les propri√©t√©s avec un num√©ro de parcelle non vide
        filtered_properties = []
        for prop in properties:
            numero = str(prop.get('numero', '')).strip()
            
            # Garder seulement les lignes avec un num√©ro de parcelle valide
            if numero and numero not in ['', 'N/A', 'None', 'null', '0']:
                filtered_properties.append(prop)
            else:
                logger.debug(f"üóëÔ∏è Ligne supprim√©e (num√©ro de parcelle vide): {prop.get('nom', 'N/A')} - {prop.get('designation_parcelle', 'N/A')}")
        
        removed_count = initial_count - len(filtered_properties)
        
        if removed_count > 0:
            logger.info(f"üßπ √âTAPE 6: Suppression des lignes sans num√©ro de parcelle")
            logger.info(f"   - {removed_count} ligne(s) supprim√©e(s) sur {initial_count}")
            logger.info(f"   - {len(filtered_properties)} ligne(s) conserv√©e(s)")
        else:
            logger.info(f"‚úÖ √âTAPE 6: Aucune ligne sans num√©ro de parcelle trouv√©e ({initial_count} lignes v√©rifi√©es)")
        
        return filtered_properties

    def final_validation_before_export(self, all_properties: List[Dict]) -> List[Dict]:
        """
        üîç VALIDATION FINALE STRICTE avant export - Derni√®re v√©rification qualit√©.
        
        D√©tecte et corrige les derniers probl√®mes avant l'export final.
        """
        logger.info(f"üîç VALIDATION FINALE - {len(all_properties)} propri√©t√©s √† valider")
        
        if not all_properties:
            return all_properties
        
        # 1. STATISTIQUES PAR FICHIER SOURCE
        file_stats = {}
        geo_stats = {}
        
        # üö® NETTOYAGE FINAL ANTI-R√âSIDUS
        logger.info("üßπ NETTOYAGE FINAL ANTI-R√âSIDUS...")

        # Filtrer ligne par ligne avec crit√®res stricts
        final_clean = []
        removed_count = 0

        for prop in all_properties:
            nom = prop.get('nom', '').strip()
            prenom = prop.get('prenom', '').strip()
            dept = prop.get('department', '').strip()
            comm = prop.get('commune', '').strip()
            
            # CRIT√àRE 1: Nom valide requis
            if not self.is_likely_real_owner(nom, prenom):
                removed_count += 1
                logger.debug(f"üóëÔ∏è SUPPRIM√â (nom invalide): {nom} {prenom}")
                continue
            
            # CRIT√àRE 2: Donn√©es g√©ographiques requises (STRICT)
            if not dept or not comm or dept == 'N/A' or comm == 'N/A':
                removed_count += 1 
                logger.debug(f"üóëÔ∏è SUPPRIM√â (g√©o manquante): {nom} - dept={dept}, comm={comm}")
                continue
            
            # CRIT√àRE 3: REJET G√âOGRAPHIES CONTAMIN√âES (ULTRA-STRICT)
            if (dept in ['XX', 'COMMUNE', 'Unknown'] or 
                comm in ['XX', 'COMMUNE', 'Unknown'] or
                not dept.isdigit() or not comm.isdigit() or
                len(dept) != 2 or len(comm) != 3):
                removed_count += 1
                logger.debug(f"üóëÔ∏è SUPPRIM√â (g√©o contamin√©e): {nom} - dept={dept}, comm={comm}")
                continue
            
            # CRIT√àRE 4: Longueur minimale du nom
            if len(nom) < 3:
                removed_count += 1
                logger.debug(f"üóëÔ∏è SUPPRIM√â (nom trop court): {nom}")
                continue
            
            final_clean.append(prop)

        if removed_count > 0:
            logger.warning(f"üßΩ NETTOYAGE FINAL: {removed_count} lignes parasites supprim√©es")
            logger.info(f"‚úÖ R√âSULTAT FINAL: {len(final_clean)} propri√©t√©s valides")

        all_properties = final_clean
        
        for prop in all_properties:
            fichier = prop.get('fichier_source', 'INCONNU')
            dept = prop.get('department', '').strip()
            comm = prop.get('commune', '').strip()
            
            # Stats par fichier
            if fichier not in file_stats:
                file_stats[fichier] = {'count': 0, 'geo': set()}
            file_stats[fichier]['count'] += 1
            
            # Stats g√©ographiques
            if dept and comm:
                geo_key = f"{dept}-{comm}"
                file_stats[fichier]['geo'].add(geo_key)
                geo_stats[geo_key] = geo_stats.get(geo_key, 0) + 1
        
        # 2. D√âTECTION DE CONTAMINATION CROIS√âE
        contaminated_files = []
        for fichier, stats in file_stats.items():
            if len(stats['geo']) > 1:  # Plus d'1 g√©ographie = suspect
                contaminated_files.append(fichier)
                logger.warning(f"‚ö†Ô∏è FICHIER SUSPECT: {fichier} - {len(stats['geo'])} g√©ographies diff√©rentes: {stats['geo']}")
        
        # 3. NETTOYAGE FINAL SI CONTAMINATION D√âTECT√âE
        if contaminated_files:
            logger.warning(f"üßΩ NETTOYAGE FINAL - {len(contaminated_files)} fichiers avec contamination")
            
            cleaned_properties = []
            removed_count = 0
            
            for prop in all_properties:
                fichier = prop.get('fichier_source', '')
                dept = prop.get('department', '').strip()
                comm = prop.get('commune', '').strip()
                
                if fichier in contaminated_files and dept and comm:
                    # Pour les fichiers contamin√©s, garder seulement la g√©ographie majoritaire
                    geo_key = f"{dept}-{comm}"
                    main_geo = max(geo_stats.items(), key=lambda x: x[1])[0]
                    
                    if geo_key == main_geo:
                        cleaned_properties.append(prop)
                    else:
                        removed_count += 1
                        logger.info(f"‚ùå SUPPRIM√â: {prop.get('nom', '')} {prop.get('prenom', '')} (Geo {geo_key} != {main_geo})")
                else:
                    cleaned_properties.append(prop)
            
            if removed_count > 0:
                logger.warning(f"üßΩ CONTAMINATION FINALE NETTOY√âE: {removed_count} propri√©t√©s supprim√©es")
                all_properties = cleaned_properties
        
        # 4. RAPPORT FINAL DE QUALIT√â
        logger.info(f"‚úÖ VALIDATION FINALE TERMIN√âE:")
        logger.info(f"   - Propri√©t√©s finales: {len(all_properties)}")
        logger.info(f"   - Fichiers trait√©s: {len(file_stats)}")
        logger.info(f"   - G√©ographies uniques: {len(geo_stats)}")
        
        for geo, count in geo_stats.items():
            logger.info(f"   - {geo}: {count} propri√©t√©s")
        
        return all_properties

    def export_to_csv(self, all_properties: List[Dict], output_filename: str = "output.csv") -> None:
        """
        Exporte toutes les donn√©es vers un fichier CSV avec s√©parateur point-virgule.
        """
        if not all_properties:
            logger.warning("Aucune donn√©e √† exporter")
            return
        # Nettoyage du code commune avant export
        for prop in all_properties:
            prop['commune'] = clean_commune_code(prop.get('commune', ''))
        # Cr√©er le DataFrame
        df = pd.DataFrame(all_properties)
        
        # Colonnes selon les sp√©cifications du client (avec contenance d√©taill√©e)
        columns_order = [
            'department', 'commune', 'prefixe', 'section', 'numero', 
            'contenance_ha', 'contenance_a', 'contenance_ca',
            'droit_reel', 'designation_parcelle', 'nom', 'prenom', 'numero_majic', 
            'voie', 'post_code', 'city', 'id', 'fichier_source'
        ]
        
        # Renommer les colonnes pour plus de clart√©
        column_mapping = {
            'department': 'D√©partement',
            'commune': 'Commune', 
            'prefixe': 'Pr√©fixe',
            'section': 'Section',
            'numero': 'Num√©ro',
            'contenance_ha': 'Contenance HA',
            'contenance_a': 'Contenance A', 
            'contenance_ca': 'Contenance CA',
            'droit_reel': 'Droit r√©el',
            'designation_parcelle': 'Designation Parcelle',
            'nom': 'Nom Propri',
            'prenom': 'Pr√©nom Propri',
            'numero_majic': 'N¬∞MAJIC',
            'voie': 'Voie',
            'post_code': 'CP',
            'city': 'Ville',
            'id': 'id',
            'fichier_source': 'Fichier source'
        }
        
        # R√©organiser et renommer
        df = df.reindex(columns=columns_order, fill_value='')
        df = df.rename(columns=column_mapping)
        
        # Export CSV avec s√©parateur point-virgule (meilleur pour Excel fran√ßais)
        output_path = self.output_dir / output_filename
        df.to_csv(output_path, index=False, encoding='utf-8-sig', sep=';')
        
        logger.info(f"üìä Donn√©es CSV export√©es vers {output_path} (s√©parateur: ;)")
        logger.info(f"üìà Total: {len(all_properties)} propri√©t√©(s) dans {len(df['Fichier source'].unique())} fichier(s)")
        
        return output_path


def main():
    """Fonction principale."""
    # Charger les variables d'environnement
    load_dotenv()
    
    # Cr√©er et lancer l'extracteur
    extractor = PDFPropertyExtractor()
    extractor.run()


if __name__ == "__main__":
    main() 