#!/usr/bin/env python3
"""
Script d'extraction automatique d'informations de propri√©taires depuis des PDFs.

Auteur: Assistant IA
Date: 2025
"""

import os
import json
import logging
import base64
from pathlib import Path
from typing import List, Dict, Optional
import fitz  # PyMuPDF
import pdfplumber
import pandas as pd
from openai import OpenAI
from dotenv import load_dotenv
from PIL import Image
import io
import logging.handlers
import sys
import tempfile
import shutil

# Configuration du logging avec encodage UTF-8 pour Windows
def setup_logging():
    """Configure le logging avec support UTF-8 pour Windows"""
    # Cr√©er un formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Logger principal
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    
    # Supprimer les handlers existants
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Handler pour fichier avec encodage UTF-8
    try:
        file_handler = logging.FileHandler('extraction.log', encoding='utf-8', mode='a')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    except Exception as e:
        print(f"Erreur configuration fichier log: {e}")
    
    # Handler pour console avec gestion des erreurs d'encodage
    try:
        # Cr√©er un stream qui ignore les erreurs d'encodage
        console_stream = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        console_handler = logging.StreamHandler(console_stream)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
    except Exception:
        # Fallback vers un handler console simple sans emojis
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
    
    return logger

# Initialiser le logger
logger = setup_logging()

def safe_json_parse(content: str, context: str = "API response") -> Optional[Dict]:
    """
    Parse JSON de mani√®re robuste avec gestion d'erreurs
    
    Args:
        content: Contenu √† parser
        context: Contexte pour le logging d'erreur
    
    Returns:
        Dict pars√© ou None si √©chec
    """
    if not content or content.strip() == "":
        logger.warning(f"Contenu vide pour {context}")
        return None
    
    # Nettoyer le contenu
    content = content.strip()
    
    # Chercher un objet JSON dans la r√©ponse
    start_idx = content.find('{')
    end_idx = content.rfind('}')
    
    if start_idx == -1 or end_idx == -1:
        logger.warning(f"Pas de JSON trouv√© dans {context}: {content[:100]}...")
        return None
    
    try:
        json_content = content[start_idx:end_idx+1]
        result = json.loads(json_content)
        return result
    except json.JSONDecodeError as e:
        logger.warning(f"Erreur parsing JSON pour {context}: {e}")
        logger.debug(f"Contenu probl√©matique: {json_content[:200]}...")
        return None
    except Exception as e:
        logger.error(f"Erreur inattendue parsing JSON pour {context}: {e}")
        return None

class PDFPropertyExtractor:
    """Classe principale pour l'extraction d'informations de propri√©taires depuis des PDFs."""
    
    def __init__(self, input_dir: str = "input", output_dir: str = "output"):
        """
        Initialise l'extracteur.
        
        Args:
            input_dir: Dossier contenant les PDFs
            output_dir: Dossier de sortie pour les r√©sultats
        """
        # Charger les variables d'environnement
        load_dotenv()
        
        # R√©cup√©rer la cl√© API depuis les variables d'environnement
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("La cl√© API OpenAI n'est pas configur√©e. Veuillez d√©finir OPENAI_API_KEY dans le fichier .env")
        
        self.client = OpenAI(api_key=api_key)
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.default_section = os.getenv('DEFAULT_SECTION', 'A')
        self.default_plan_number = int(os.getenv('DEFAULT_PLAN_NUMBER', '123'))
        
        # Cr√©er les dossiers s'ils n'existent pas
        self.input_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        
        logger.info(f"Extracteur initialis√© - Input: {self.input_dir}, Output: {self.output_dir}")

    def list_pdf_files(self) -> List[Path]:
        """
        Liste tous les fichiers PDF dans le dossier input.
        
        Returns:
            Liste des chemins vers les fichiers PDF
        """
        pdf_files = list(self.input_dir.glob("*.pdf"))
        logger.info(f"Trouv√© {len(pdf_files)} fichier(s) PDF dans {self.input_dir}")
        return pdf_files

    def pdf_to_images(self, pdf_path: Path) -> List[bytes]:
        """
        Convertit toutes les pages d'un PDF en images PNG.
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            
        Returns:
            Liste des bytes des images PNG ou liste vide en cas d'erreur
        """
        try:
            logger.info(f"Conversion de toutes les pages de {pdf_path.name} en images")
            
            # Ouvrir le PDF
            doc = fitz.open(pdf_path)
            
            if len(doc) == 0:
                logger.error(f"Le PDF {pdf_path.name} est vide")
                return []
            
            images = []
            
            # Traiter chaque page
            for page_num in range(len(doc)):
                try:
                    page = doc[page_num]
                    
                    # Convertir chaque page en image avec une r√©solution MAXIMALE
                    mat = fitz.Matrix(5.0, 5.0)  # ULTRA-HAUTE r√©solution pour extraction optimale
                    pix = page.get_pixmap(matrix=mat)
                    
                    # Convertir en PNG
                    img_data = pix.tobytes("png")
                    images.append(img_data)
                    
                    logger.info(f"Page {page_num + 1}/{len(doc)} convertie pour {pdf_path.name}")
                    
                except Exception as e:
                    logger.error(f"Erreur lors de la conversion de la page {page_num + 1} de {pdf_path.name}: {str(e)}")
                    continue
            
            doc.close()
            logger.info(f"Conversion r√©ussie pour {pdf_path.name}: {len(images)} page(s) trait√©e(s)")
            return images
            
        except Exception as e:
            logger.error(f"Erreur lors de la conversion de {pdf_path.name}: {str(e)}")
            return []

    def extract_info_with_gpt4o(self, image_data: bytes, filename: str) -> Optional[Dict]:
        """
        EXTRACTION ULTRA-OPTIMIS√âE pour extraire TOUTES les informations possibles.
        
        Args:
            image_data: Donn√©es de l'image en bytes
            filename: Nom du fichier pour le logging
            
        Returns:
            Dictionnaire contenant les informations extraites ou None en cas d'erreur
        """
        try:
            logger.info(f"üîç Extraction ADAPTATIVE pour {filename}")
            
            # PHASE 1: D√âTECTION AUTOMATIQUE du format PDF
            format_info = self.detect_pdf_format(image_data)
            logger.info(f"üìä Format: {format_info.get('document_type')} | √âpoque: {format_info.get('format_era')} | Layout: {format_info.get('layout')}")
            
            # Encoder l'image en base64
            base64_image = base64.b64encode(image_data).decode('utf-8')
            
            # PHASE 2: PROMPT ADAPTATIF selon le format d√©tect√©
            adapted_prompt = self.adapt_extraction_prompt(format_info)
            
            # PROMPT ADAPTATIF remplace l'ancien prompt fixe
            # (L'ancien prompt ultra-d√©taill√© est maintenant dans adapt_extraction_prompt)
            logger.info(f"üéØ Utilisation strat√©gie: {format_info.get('extraction_strategy')}")
            
            # Fallback: si la d√©tection √©choue, utiliser le prompt ultra-d√©taill√© original
            if not adapted_prompt:
                adapted_prompt = """
üéØ MISSION CRITIQUE: Tu es un expert en documents cadastraux fran√ßais. Tu DOIS extraire TOUTES les informations visibles avec une pr√©cision maximale.

üìã STRAT√âGIE D'EXTRACTION EXHAUSTIVE:

1Ô∏è‚É£ LOCALISATION (priorit√© absolue):
- Cherche en HAUT du document: codes comme "51179", "25227", "75001" 
- Format: DEPARTMENT(2 chiffres) + COMMUNE(3 chiffres)
- Exemples: "51179" = d√©partement 51, commune 179

2Ô∏è‚É£ PROPRI√âTAIRES (scan complet):
- Noms en MAJUSCULES: MARTIN, DUPONT, LAMBIN, etc.
- Pr√©noms: Jean, Marie, Didier Jean Guy, etc.
- Codes MAJIC: M8BNF6, MB43HC, P7QR92 (alphanum√©riques 6 caract√®res)

3Ô∏è‚É£ PARCELLES (d√©tection fine):
- Sections: A, AB, ZY, 000ZD, etc.
- Num√©ros: 6, 0006, 123, 0123, etc.
- Contenance: 230040, 000150, 002300 (format chiffres)

4Ô∏è‚É£ ADRESSES (lecture compl√®te):
- Voies: "1 RUE D AVAT", "15 AVENUE DE LA PAIX"
- Codes postaux: 51240, 75001, 13000
- Villes: COUPEVILLE, PARIS, MARSEILLE

5Ô∏è‚É£ DROITS R√âELS:
- PP = Pleine Propri√©t√©
- US = Usufruit  
- NU = Nue-propri√©t√©

üìä EXEMPLES CONCRETS DE DONN√âES R√âELLES:

EXEMPLE 1:
{
  "department": "51",
  "commune": "179", 
  "section": "ZY",
  "numero": "0006",
  "contenance": "230040",
  "droit_reel": "US",
  "designation_parcelle": "LES ROULLIERS",
  "nom": "LAMBIN",
  "prenom": "DIDIER JEAN GUY",
  "numero_majic": "M8BNF6",
  "voie": "1 RUE D AVAT",
  "post_code": "51240",
  "city": "COUPEVILLE"
}

EXEMPLE 2:
{
  "department": "25",
  "commune": "227",
  "section": "000ZD",
  "numero": "0005",
  "contenance": "000150",
  "droit_reel": "PP",
  "designation_parcelle": "LE GRAND CHAMP",
  "nom": "MARTIN",
  "prenom": "PIERRE",
  "numero_majic": "MB43HC",
  "voie": "15 RUE DE LA PAIX",
  "post_code": "25000",
  "city": "BESANCON"
}

üîç INSTRUCTIONS DE SCAN SYST√âMATIQUE:

1. Commence par scanner TOUT LE HAUT du document pour les codes d√©partement/commune
2. Lit TOUS les noms en majuscules (ce sont les propri√©taires)
3. Trouve TOUS les codes MAJIC (6 caract√®res alphanum√©riques)
4. R√©cup√®re TOUTES les adresses compl√®tes
5. Identifie TOUTES les sections et num√©ros de parcelles
6. Collecte TOUTES les contenances (surfaces)
7. ‚≠ê PR√âFIXE (TR√àS RARE mais CRUCIAL) : Cherche activement les pr√©fixes comme "ZY", "AB", "000AC" dans les tableaux "Propri√©t√©(s) non b√¢tie(s)" - ils apparaissent AVANT les sections !

‚ö†Ô∏è R√àGLES STRICTES:
- Si tu vois une information partiellement, INCLUS-LA quand m√™me
- Ne mets JAMAIS "N/A" - utilise "" si vraiment absent
- Scan CHAQUE ZONE du document m√©thodiquement
- IGNORE aucun d√©tail, m√™me petit
- Retourne TOUS les propri√©taires trouv√©s

üì§ FORMAT DE R√âPONSE OBLIGATOIRE:
{
  "proprietes": [
    {
      "department": "XX",
      "commune": "XXX",
      "prefixe": "",
      "section": "XXXX",
      "numero": "XXXX",
      "contenance": "XXXXXX",
      "droit_reel": "XX",
      "designation_parcelle": "LIEU-DIT",
      "nom": "NOM_PROPRIETAIRE",
      "prenom": "PRENOM_PROPRIETAIRE",
      "numero_majic": "XXXXXX",
      "voie": "ADRESSE_COMPLETE",
      "post_code": "XXXXX",
      "city": "VILLE"
    }
  ]
}

üéØ OBJECTIF: Z√âRO champ vide si l'info existe dans le document !
"""
            
            # PREMI√àRE PASSE: Extraction principale ultra-d√©taill√©e
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": adapted_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4000,
                temperature=0.0
            )
            
            # Parser la r√©ponse JSON
            response_text = response.choices[0].message.content.strip()
            
            # Nettoyer la r√©ponse
            if response_text.startswith('```json'):
                response_text = response_text[7:-3]
            elif response_text.startswith('```'):
                response_text = response_text[3:-3]
            
            main_result = safe_json_parse(response_text, f"extraction principale {filename}")
            
            if main_result and "proprietes" in main_result and main_result["proprietes"]:
                properties = main_result["proprietes"]
                logger.info(f"Extraction principale: {len(properties)} propri√©t√©(s) pour {filename}")
                
                # DEUXI√àME PASSE: R√©cup√©ration des champs manquants
                enhanced_properties = self.enhance_missing_fields(properties, base64_image, filename)
                
                if enhanced_properties:
                    logger.info(f"Extraction ULTRA-OPTIMIS√âE termin√©e: {len(enhanced_properties)} propri√©t√©(s) pour {filename}")
                    return {"proprietes": enhanced_properties}
                else:
                    return main_result
            else:
                logger.warning(f"Extraction principale sans r√©sultat pour {filename}")
                # PASSE DE SECOURS: Extraction d'urgence
                return self.emergency_extraction(base64_image, filename)
                
        except Exception as e:
            logger.error(f"Erreur extraction pour {filename}: {e}")
            return None

    def enhance_missing_fields(self, properties: List[Dict], base64_image: str, filename: str) -> List[Dict]:
        """
        DEUXI√àME PASSE: Am√©lioration cibl√©e des champs manquants.
        """
        try:
            # Analyser quels champs sont manquants
            missing_fields = {}
            critical_fields = ['department', 'commune', 'nom', 'prenom', 'section', 'numero']
            
            for prop in properties:
                for field in critical_fields:
                    if not prop.get(field) or prop.get(field) == "":
                        if field not in missing_fields:
                            missing_fields[field] = 0
                        missing_fields[field] += 1
            
            if not missing_fields:
                logger.info(f"‚úÖ Tous les champs critiques pr√©sents pour {filename}")
                return properties
            
            logger.info(f"üîç R√©cup√©ration cibl√©e pour {filename}: {missing_fields}")
            
            # Prompt cibl√© pour les champs manquants les plus critiques
            if 'department' in missing_fields or 'commune' in missing_fields:
                properties = self.extract_location_info(properties, base64_image, filename)
            
            if 'nom' in missing_fields or 'prenom' in missing_fields:
                properties = self.extract_owner_info(properties, base64_image, filename)
            
            return properties
            
        except Exception as e:
            logger.error(f"Erreur enhancement pour {filename}: {e}")
            return properties

    def extract_location_info(self, properties: List[Dict], base64_image: str, filename: str) -> List[Dict]:
        """Extraction cibl√©e des informations de localisation."""
        try:
            location_prompt = """
üéØ MISSION SP√âCIALIS√âE: Trouve les codes d√©partement et commune dans ce document cadastral.

üîç RECHERCHE INTENSIVE:
- Scan le HEADER du document
- Cherche des codes √† 5 chiffres comme: 51179, 25227, 75001
- Format: 2 chiffres d√©partement + 3 chiffres commune
- Peut √™tre √©crit: 51 179, 51-179, ou 51179

Exemples typiques:
- "51179" ‚Üí d√©partement: "51", commune: "179"
- "25227" ‚Üí d√©partement: "25", commune: "227"
- "75001" ‚Üí d√©partement: "75", commune: "001"

üì§ R√âPONSE FORMAT:
{
  "location": {
    "department": "XX",
    "commune": "XXX"
  }
}

‚ö†Ô∏è CRUCIAL: Ces codes sont TOUJOURS dans l'en-t√™te du document !
"""
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": location_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=500,
                temperature=0.0
            )
            
            location_text = response.choices[0].message.content.strip()
            if "```json" in location_text:
                location_text = location_text.split("```json")[1].split("```")[0].strip()
            
            location_data = safe_json_parse(location_text, f"extraction localisation {filename}")
            
            if not location_data:
                logger.warning(f"√âchec parsing localisation pour {filename}")
                return properties
            
            if "location" in location_data:
                dept = location_data["location"].get("department")
                commune = location_data["location"].get("commune")
                
                if dept or commune:
                    for prop in properties:
                        if not prop.get("department") and dept:
                            prop["department"] = dept
                        if not prop.get("commune") and commune:
                            prop["commune"] = commune
                    
                    logger.info(f"‚úÖ Localisation r√©cup√©r√©e: dept={dept}, commune={commune}")
            
        except Exception as e:
            logger.warning(f"Erreur extraction localisation: {e}")
        
        return properties

    def extract_owner_info(self, properties: List[Dict], base64_image: str, filename: str) -> List[Dict]:
        """Extraction cibl√©e des informations propri√©taires."""
        try:
            owner_prompt = """
üéØ MISSION: Trouve TOUS les propri√©taires dans ce document cadastral.

üîç RECHERCHE SYST√âMATIQUE:
- Noms en MAJUSCULES: MARTIN, DUPONT, LAMBIN, BERNARD, etc.
- Pr√©noms: Jean, Marie, Pierre, Didier Jean Guy, etc.
- Codes MAJIC: M8BNF6, MB43HC, P7QR92 (6 caract√®res alphanum√©riques)
- Adresses compl√®tes: "1 RUE D AVAT", "15 AVENUE DE LA PAIX"

üì§ FORMAT R√âPONSE:
{
  "owners": [
    {
      "nom": "NOM_MAJUSCULE",
      "prenom": "Pr√©nom Complet",
      "numero_majic": "XXXXXX",
      "voie": "Adresse compl√®te",
      "post_code": "XXXXX",
      "city": "VILLE"
    }
  ]
}

‚ö†Ô∏è Scan TOUT le document pour les propri√©taires !
"""
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": owner_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.0
            )
            
            owner_text = response.choices[0].message.content.strip()
            if "```json" in owner_text:
                owner_text = owner_text.split("```json")[1].split("```")[0].strip()
            
            owner_data = safe_json_parse(owner_text, f"extraction propri√©taires {filename}")
            
            if not owner_data:
                logger.warning(f"√âchec parsing propri√©taires pour {filename}")
                return properties
            
            if "owners" in owner_data and owner_data["owners"]:
                owners = owner_data["owners"]
                
                # Fusionner avec les propri√©t√©s existantes
                for i, prop in enumerate(properties):
                    if i < len(owners):
                        owner = owners[i]
                        for field in ['nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city']:
                            if not prop.get(field) and owner.get(field):
                                prop[field] = owner[field]
                
                logger.info(f"‚úÖ Propri√©taires r√©cup√©r√©s: {len(owners)}")
        
        except Exception as e:
            logger.warning(f"Erreur extraction propri√©taires: {e}")
        
        return properties

    def emergency_extraction(self, base64_image: str, filename: str) -> Optional[Dict]:
        """
        EXTRACTION D'URGENCE: Derni√®re tentative avec prompt ultra-simple.
        """
        try:
            logger.info(f"üö® Extraction d'urgence pour {filename}")
            
            emergency_prompt = """
Regarde ce document cadastral fran√ßais et trouve:
1. Tous les NOMS en MAJUSCULES
2. Tous les codes √† 5 chiffres (d√©partement+commune)
3. Toutes les sections (lettres comme A, ZY)
4. Tous les num√©ros de parcelles

Retourne TOUT ce que tu vois en JSON:
{
  "proprietes": [
    {
      "department": "",
      "commune": "",
      "section": "",
      "numero": "",
      "nom": "",
      "prenom": "",
      "voie": "",
      "city": ""
    }
  ]
}
"""
            
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": emergency_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.1
            )
            
            emergency_text = response.choices[0].message.content.strip()
            if "```json" in emergency_text:
                emergency_text = emergency_text.split("```json")[1].split("```")[0].strip()
            
            result = safe_json_parse(emergency_text, f"extraction urgence {filename}")
            
            if not result:
                logger.error(f"√âchec total extraction pour {filename}")
                return None
            if "proprietes" in result and result["proprietes"]:
                logger.info(f"üÜò Extraction d'urgence r√©ussie: {len(result['proprietes'])} propri√©t√©(s)")
                return result
            
        except Exception as e:
            logger.error(f"√âchec extraction d'urgence pour {filename}: {e}")
        
        return None

    def generate_parcel_id(self, department: str, commune: str, section: str = None, plan_number: int = None) -> str:
        """
        G√©n√®re un identifiant parcellaire selon la formule sp√©cifi√©e.
        
        Args:
            department: Code d√©partement (2 chiffres)
            commune: Code commune (3 chiffres)
            section: Section (5 caract√®res, optionnel)
            plan_number: Num√©ro de plan (4 chiffres, optionnel)
            
        Returns:
            Identifiant parcellaire format√©
        """
        # Utiliser les valeurs par d√©faut si non fournies
        if section is None:
            section = self.default_section
        if plan_number is None:
            plan_number = self.default_plan_number
        
        # Formater selon les r√®gles
        dept_formatted = department.zfill(2)
        commune_formatted = commune.zfill(3)
        section_formatted = str(section).ljust(5, '0')[:5]
        plan_formatted = str(plan_number).zfill(4)
        
        parcel_id = f"{dept_formatted}{commune_formatted}{section_formatted}{plan_formatted}"
        return parcel_id

    def decompose_contenance(self, contenance: str) -> Dict[str, str]:
        """
        D√©compose une contenance de 7 chiffres en hectares, ares et centiares.
        
        Args:
            contenance: Cha√Æne de 7 chiffres (ex: "0130221")
            
        Returns:
            Dictionnaire avec les cl√©s HA, A, CA
        """
        if not contenance or contenance == "N/A" or len(contenance) != 7 or not contenance.isdigit():
            return {"HA": "N/A", "A": "N/A", "CA": "N/A"}
        
        # D√©composer selon le format : HHAACC
        ha = contenance[:2]  # 2 premiers chiffres (hectares)
        a = contenance[2:4]  # 2 suivants (ares)
        ca = contenance[4:7]  # 3 derniers (centiares)
        
        return {"HA": ha, "A": a, "CA": ca}

    def generate_unique_id(self, department: str, commune: str, section: str, numero: str, prefixe: str = "") -> str:
        """
        G√âN√âRATION D'ID ULTRA-ROBUSTE - EXACTEMENT 14 CARACT√àRES GARANTIS
        Format strict: DD(2) + CCC(3) + SSSSS(5) + NNNN(4) = 14
        
        Args:
            department: Code d√©partement
            commune: Code commune
            section: Section cadastrale
            numero: Num√©ro de parcelle
            prefixe: Pr√©fixe optionnel (ignor√© dans cette version)
            
        Returns:
            ID unique format√© sur EXACTEMENT 14 caract√®res (ex: 25227ZD0000005)
        """
        # √âTAPE 1: D√©partement - EXACTEMENT 2 caract√®res
        dept = str(department or "00").strip()
        if dept == "N/A" or not dept:
            dept = "00"
        dept = dept.zfill(2)[:2]  # Force exactement 2 caract√®res
        
        # √âTAPE 2: Commune - EXACTEMENT 3 caract√®res  
        comm = str(commune or "000").strip()
        if comm == "N/A" or not comm:
            comm = "000"
        comm = comm.zfill(3)[:3]  # Force exactement 3 caract√®res
        
        # √âTAPE 3: Section - EXACTEMENT 5 caract√®res (POINT CRITIQUE)
        if section and str(section).strip() and section != "N/A":
            sect = str(section).strip().upper()
            # FOR√áAGE STRICT: Toujours exactement 5 caract√®res
            if len(sect) < 5:
                sect = sect.rjust(5, '0')  # Compl√©ter √† GAUCHE avec des z√©ros
            elif len(sect) > 5:
                sect = sect[:5]  # Tronquer √† exactement 5
        else:
            sect = "0000A"  # Section par d√©faut (5 caract√®res garantis)
        
        # Validation section
        if len(sect) != 5:
            logger.error(f"üö® Section probl√®me: '{section}' ‚Üí '{sect}' (longueur: {len(sect)})")
            sect = (sect + "00000")[:5]  # Force correction d'urgence
        
        # √âTAPE 4: Num√©ro - EXACTEMENT 4 caract√®res
        if numero and str(numero).strip() and numero != "N/A":
            num = str(numero).strip()
            # Traitement selon le type
            if num.isdigit():
                num = num.zfill(4)[-4:]  # Derniers 4 chiffres si trop long
            else:
                # Pour les num√©ros alphanum√©riques, compl√©ter ou tronquer
                if len(num) < 4:
                    num = num.ljust(4, '0')
                elif len(num) > 4:
                    num = num[:4]
        else:
            num = "0001"  # Num√©ro par d√©faut (4 caract√®res garantis)
        
        # Validation num√©ro
        if len(num) != 4:
            logger.error(f"üö® Num√©ro probl√®me: '{numero}' ‚Üí '{num}' (longueur: {len(num)})")
            num = (num + "0000")[:4]  # Force correction d'urgence
        
        # ASSEMBLAGE FINAL
        unique_id = f"{dept}{comm}{sect}{num}"
        
        # VALIDATION ULTRA-STRICTE FINALE
        if len(unique_id) != 14:
            logger.error(f"üö® ID LONGUEUR CRITIQUE: '{unique_id}' = {len(unique_id)} caract√®res")
            logger.error(f"üîç ANALYSE: dept='{dept}'({len(dept)}) comm='{comm}'({len(comm)}) sect='{sect}'({len(sect)}) num='{num}'({len(num)})")
            
            # CORRECTION FORC√âE ABSOLUE
            if len(unique_id) < 14:
                unique_id = unique_id.ljust(14, '0')
                logger.warning(f"üîß ID COMPL√âT√â: '{unique_id}'")
            elif len(unique_id) > 14:
                unique_id = unique_id[:14]
                logger.warning(f"üîß ID TRONQU√â: '{unique_id}'")
        
        # ASSERTION FINALE - Garantie absolue 14 caract√®res
        if len(unique_id) != 14:
            raise ValueError(f"ERREUR FATALE: ID '{unique_id}' = {len(unique_id)} caract√®res (devrait √™tre 14)")
        
        logger.debug(f"‚úÖ ID ROBUSTE: '{unique_id}' (longueur: {len(unique_id)})")
        return unique_id

    def extract_tables_with_pdfplumber(self, pdf_path: Path) -> Dict:
        """
        EXTRACTION HYBRIDE √âTAPE 1: Extraction des tableaux structur√©s avec pdfplumber.
        R√©plique exactement l'approche du code Make/Python Anywhere.
        """
        logger.info(f"üìã Extraction tableaux pdfplumber pour {pdf_path.name}")
        
        try:
            prop_batie = []
            non_prop_batie = []
            contenance_totale = {}
            property_batie_in_new_page = False
            
            with pdfplumber.open(pdf_path) as pdf:
                # Parcourir toutes les pages
                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        if not table or not table[0]:
                            continue
                            
                        # D√©tecter les tableaux de propri√©t√©s b√¢ties
                        if table[0][0] == 'Propri√©t√©(s) b√¢tie(s)':
                            logger.info(f"üìä Trouv√© tableau propri√©t√©s b√¢ties")
                            property_batie_in_new_page = True
                            prop_batie = self.extract_property_batie(table)
                        
                        # D√©tecter les tableaux de propri√©t√©s non b√¢ties
                        elif table[0][0] == 'Propri√©t√©(s) non b√¢tie(s)':
                            logger.info(f"üìä Trouv√© tableau propri√©t√©s non b√¢ties")
                            prop_non_batie_dict = self.extract_property_non_batie(table)
                            non_prop_batie.extend(prop_non_batie_dict)
                        
                        # NOUVEAU: D√©tecter le tableau "Contenance totale"
                        elif any(row and 'Contenance totale' in str(row[0]) for row in table if row):
                            logger.info(f"üéØ Trouv√© tableau Contenance totale")
                            contenance_totale = self.extract_contenance_totale(table)
                            # Appliquer les valeurs HA, A, CA aux propri√©t√©s d√©j√† extraites
                            if contenance_totale:
                                self.apply_contenance_totale_to_properties(non_prop_batie, contenance_totale)
                
                # Fallback: chercher dans la premi√®re page si pas trouv√© ailleurs
                if not property_batie_in_new_page and pdf.pages:
                    first_page_tables = pdf.pages[0].extract_tables()
                    if first_page_tables:
                        for idx, row in enumerate(first_page_tables[0]):
                            if row and row[0] == 'Propri√©t√©(s) b√¢tie(s)':
                                property_batie_table = first_page_tables[0][idx:]
                                prop_batie = self.extract_property_batie(property_batie_table)
            
            logger.info(f"‚úÖ pdfplumber: {len(prop_batie)} b√¢ties, {len(non_prop_batie)} non b√¢ties")
            return {
                "prop_batie": prop_batie,
                "non_batie": non_prop_batie
            }
            
        except Exception as e:
            logger.error(f"Erreur pdfplumber pour {pdf_path.name}: {e}")
            return {"prop_batie": [], "non_batie": []}

    def extract_property_batie(self, table: List[List]) -> List[Dict]:
        """Extraction des propri√©t√©s b√¢ties (r√©plique du code Make)."""
        if len(table) < 3:
            return []
            
        property_rows = table[2:]
        headers = property_rows[0]
        clean_headers = [header.replace('\n', ' ').strip() if header is not None else '' for header in headers]
        property_dicts = []

        if len(property_rows) > 1 and property_rows[1] and "Total" not in str(property_rows[1][0]):
            for row in property_rows[1:]:
                if row and "Total" in str(row[0]):
                    break
                
                property_dict = {
                    clean_headers[i]: row[i] if i < len(row) else None 
                    for i in range(len(clean_headers))
                }
                property_dicts.append(property_dict)
        
        return property_dicts

    def extract_property_non_batie(self, table: List[List]) -> List[Dict]:
        """Extraction des propri√©t√©s non b√¢ties (r√©plique du code Make)."""
        if len(table) < 3:
            return []
            
        property_rows = table[2:]
        headers = property_rows[0]
        clean_headers = [header.replace('\n', ' ').strip() if header is not None else '' for header in headers]
        property_dicts = []

        # Chercher les en-t√™tes HA, A, CA suppl√©mentaires
        ha_pos = None
        a_pos = None
        ca_pos = None
        
        # V√©rifier s'il y a une ligne avec HA, A, CA apr√®s les en-t√™tes principaux
        for i, row in enumerate(property_rows):
            if row:
                for j, cell in enumerate(row):
                    if cell == 'HA':
                        ha_pos = j
                    elif cell == 'A':
                        a_pos = j
                    elif cell == 'CA':
                        ca_pos = j
                
                # Si on a trouv√© HA, A, CA dans cette ligne
                if ha_pos is not None and a_pos is not None and ca_pos is not None:
                    logger.info(f"üéØ En-t√™tes HA/A/CA trouv√©s dans tableau non b√¢ties aux positions {ha_pos}, {a_pos}, {ca_pos}")
                    break

        if len(property_rows) > 2 and property_rows[2] and "totale" not in str(property_rows[2][0]).lower():
            for row in property_rows[2:]:
                if row and "totale" in str(row[0]).lower():
                    break
                
                property_dict = {
                    clean_headers[i]: row[i] if i < len(row) else None 
                    for i in range(len(clean_headers))
                }
                
                # üîß CORRECTION SECTION : Debug et recherche am√©lior√©e
                section_value = None
                
                # Debug : afficher toute la ligne pour diagnostic
                logger.debug(f"DEBUG Ligne compl√®te: {row}")
                
                # M√©thode 1 : Par en-t√™te (existante)
                for i, header in enumerate(clean_headers):
                    if header and ('Sec' in header or 'Section' in header) and i < len(row):
                        if row[i]:
                            section_value = str(row[i]).strip()
                            logger.debug(f"Section par en-t√™te '{header}': '{section_value}'")
                            break
                
                # M√©thode 2 : Si section vide ou trop courte, chercher dans toutes les cellules
                import re
                if not section_value or len(section_value) < 2:
                    for cell in row:
                        if cell:
                            cell_str = str(cell).strip()
                            # Rechercher pattern : 1-2 lettres majuscules (ZK, AA, C, etc.)
                            if re.match(r'^[A-Z]{1,2}$', cell_str):
                                section_value = cell_str
                                logger.debug(f"Section trouv√©e par regex: '{section_value}'")
                                break
                
                if section_value:
                    property_dict['Sec'] = section_value
                    logger.info(f"Section finale extraite: '{section_value}'")
                else:
                    logger.warning(f"Section non trouv√©e dans la ligne")
                
                # Ajouter les valeurs HA, A, CA si elles existent
                if ha_pos is not None and ha_pos < len(row) and row[ha_pos]:
                    property_dict['HA'] = str(row[ha_pos]).strip()
                if a_pos is not None and a_pos < len(row) and row[a_pos]:
                    property_dict['A'] = str(row[a_pos]).strip()
                if ca_pos is not None and ca_pos < len(row) and row[ca_pos]:
                    property_dict['CA'] = str(row[ca_pos]).strip()
                
                property_dicts.append(property_dict)
        
        return property_dicts

    def extract_contenance_totale(self, table: List[List]) -> Dict:
        """Extraction du tableau 'Contenance totale' avec colonnes HA, A, CA"""
        try:
            # Chercher les en-t√™tes HA, A, CA ET la ligne de donn√©es correspondante
            contenance_data = {}
            
            for i, row in enumerate(table):
                if not row:
                    continue
                    
                # Chercher une ligne avec exactement HA, A, CA cons√©cutifs
                ha_pos = None
                a_pos = None  
                ca_pos = None
                
                for j, cell in enumerate(row):
                    if cell == 'HA':
                        ha_pos = j
                    elif cell == 'A' and j > 0 and row[j-1] == 'HA':  # A juste apr√®s HA
                        a_pos = j
                    elif cell == 'CA' and j > 1 and row[j-1] == 'A' and row[j-2] == 'HA':  # CA apr√®s A apr√®s HA
                        ca_pos = j
                
                # Si on a trouv√© les trois colonnes cons√©cutives
                if ha_pos is not None and a_pos is not None and ca_pos is not None:
                    logger.info(f"üéØ En-t√™tes HA/A/CA trouv√©s aux positions {ha_pos}, {a_pos}, {ca_pos}")
                    
                    # Chercher les donn√©es dans la m√™me ligne ou les lignes suivantes
                    for data_row_idx in range(i, min(i + 3, len(table))):  # Chercher dans les 3 lignes suivantes
                        data_row = table[data_row_idx]
                        if data_row and len(data_row) > ca_pos:
                            # V√©rifier si on a des valeurs num√©riques aux bonnes positions
                            ha_val = str(data_row[ha_pos]).strip() if ha_pos < len(data_row) and data_row[ha_pos] else ''
                            a_val = str(data_row[a_pos]).strip() if a_pos < len(data_row) and data_row[a_pos] else ''
                            ca_val = str(data_row[ca_pos]).strip() if ca_pos < len(data_row) and data_row[ca_pos] else ''
                            
                            # Si au moins une valeur est num√©rique, on prend cette ligne
                            if ha_val.isdigit() or a_val.isdigit() or ca_val.isdigit():
                                contenance_data = {
                                    'HA': ha_val,
                                    'A': a_val, 
                                    'CA': ca_val
                                }
                                logger.info(f"üéØ Contenance totale extraite: {contenance_data}")
                                return contenance_data
                    break  # On a trouv√© les en-t√™tes, pas besoin de continuer
            
            if not contenance_data:
                logger.warning("üéØ Tableau contenance totale d√©tect√© mais donn√©es non extraites")
            
            return contenance_data
            
        except Exception as e:
            logger.warning(f"Erreur extraction contenance totale: {e}")
            return {}

    def apply_contenance_totale_to_properties(self, properties: List[Dict], contenance_totale: Dict):
        """Applique les valeurs HA, A, CA du tableau contenance totale aux propri√©t√©s"""
        if not contenance_totale or not properties:
            return
            
        logger.info(f"üîÑ Application contenance totale √† {len(properties)} propri√©t√©(s)")
        
        for prop in properties:
            # Ajouter les valeurs HA, A, CA si elles n'existent pas d√©j√†
            if 'HA' not in prop and 'HA' in contenance_totale:
                prop['HA'] = contenance_totale['HA']
            if 'A' not in prop and 'A' in contenance_totale:
                prop['A'] = contenance_totale['A']
            if 'CA' not in prop and 'CA' in contenance_totale:
                prop['CA'] = contenance_totale['CA']

    def extract_owners_with_vision_simple(self, pdf_path: Path) -> List[Dict]:
        """
        EXTRACTION HYBRIDE √âTAPE 2: Extraction des propri√©taires avec OpenAI Vision.
        Utilise le prompt simplifi√© du style Make.
        """
        logger.info(f"üë§ Extraction propri√©taires OpenAI pour {pdf_path.name}")
        
        # Convertir PDF en images
        images = self.pdf_to_images(pdf_path)
        if not images:
            return []
        
        all_owners = []
        
        for page_num, image_data in enumerate(images, 1):
            try:
                # Encoder l'image
                base64_image = base64.b64encode(image_data).decode('utf-8')
                
                # PROMPT SIMPLIFI√â (style Make)
                simple_prompt = """
In the following image, you will find information of owners such as nom, prenom, adresse, droit reel, numero proprietaire, department and commune. If there are any leading zero's before commune or deparment, keep it as it is. Format the address as street address, city and post code. If city or postcode is not available, just leave it blank. There can be one or multiple owners. I want to extract all of them and return them in json format.

Output example:
{
  "owners": [
    {
      "nom": "MARTIN",
      "prenom": "MARIE MADELEINE", 
      "street_address": "2 RUE DE PARIS",
      "city": "KINGERSHEIM",
      "post_code": "68260",
      "numero_proprietaire": "MBRWL8",
      "department": "21",
      "commune": "026",
      "droit_reel": "Propri√©taire/Indivision"
    }
  ]
}
"""
                
                # Appel OpenAI (param√®tres identiques √† Make)
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": simple_prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_image}",
                                        "detail": "high"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=2048,
                    temperature=1.0,  # Exactement comme Make
                    response_format={"type": "json_object"}
                )
                
                # Parser la r√©ponse
                response_text = response.choices[0].message.content.strip()
                result = safe_json_parse(response_text, f"vision simple page {page_num}")
                if result and "owners" in result and result["owners"]:
                    all_owners.extend(result["owners"])
                    logger.info(f"Page {page_num}: {len(result['owners'])} propri√©taire(s)")
                else:
                    logger.warning(f"Pas de propri√©taires trouv√©s page {page_num}")
                    
            except Exception as e:
                logger.error(f"Erreur extraction page {page_num}: {e}")
                continue
        
        logger.info(f"üë§ Total propri√©taires extraits: {len(all_owners)}")
        return all_owners

    def merge_structured_and_vision_data(self, structured_data: Dict, owners_data: List[Dict], filename: str) -> List[Dict]:
        """
        EXTRACTION HYBRIDE √âTAPE 3: Fusion intelligente des donn√©es tableaux + propri√©taires.
        """
        logger.info(f"üîó Fusion hybride pour {filename}")
        
        merged_properties = []
        
        # R√©cup√©rer les donn√©es structur√©es
        prop_batie = structured_data.get("prop_batie", [])
        non_prop_batie = structured_data.get("non_batie", [])
        
        # Combiner toutes les propri√©t√©s structur√©es
        all_structured = prop_batie + non_prop_batie
        
        if not all_structured and not owners_data:
            logger.warning(f"Aucune donn√©e extraite pour {filename}")
            return []
        
        # Strat√©gie de fusion
        if all_structured and owners_data:
            # CAS OPTIMAL: Les deux types de donn√©es
            logger.info(f"üéØ Fusion: {len(all_structured)} propri√©t√©s + {len(owners_data)} propri√©taires")
            
            for i, structured_prop in enumerate(all_structured):
                merged_prop = self.convert_structured_to_standard_format(structured_prop)
                
                # Associer avec un propri√©taire si disponible
                if i < len(owners_data):
                    owner = owners_data[i]
                    merged_prop.update({
                        'nom': owner.get('nom', ''),
                        'prenom': owner.get('prenom', ''),
                        'numero_majic': owner.get('numero_proprietaire', ''),
                        'voie': owner.get('street_address', ''),
                        'post_code': owner.get('post_code', ''),
                        'city': owner.get('city', ''),
                        'droit_reel': owner.get('droit_reel', ''),
                        'department': owner.get('department', ''),
                        'commune': owner.get('commune', '')
                    })
                
                merged_properties.append(merged_prop)
            
            # Ajouter les propri√©taires restants s'il y en a plus
            for j in range(len(all_structured), len(owners_data)):
                owner = owners_data[j]
                merged_prop = {
                    'department': owner.get('department', ''),
                    'commune': owner.get('commune', ''),
                    'prefixe': '',
                    'section': '',
                    'numero': '',
                    'contenance': '',
                    'droit_reel': owner.get('droit_reel', ''),
                    'designation_parcelle': '',
                    'nom': owner.get('nom', ''),
                    'prenom': owner.get('prenom', ''),
                    'numero_majic': owner.get('numero_proprietaire', ''),
                    'voie': owner.get('street_address', ''),
                    'post_code': owner.get('post_code', ''),
                    'city': owner.get('city', '')
                }
                merged_properties.append(merged_prop)
                
        elif all_structured:
            # Seulement des donn√©es structur√©es
            logger.info(f"üìä Seulement donn√©es structur√©es: {len(all_structured)}")
            for structured_prop in all_structured:
                merged_prop = self.convert_structured_to_standard_format(structured_prop)
                merged_properties.append(merged_prop)
                
        elif owners_data:
            # Seulement des propri√©taires
            logger.info(f"üë§ Seulement propri√©taires: {len(owners_data)}")
            for owner in owners_data:
                merged_prop = {
                    'department': owner.get('department', ''),
                    'commune': owner.get('commune', ''),
                    'prefixe': '',
                    'section': '',
                    'numero': '',
                    'contenance': '',
                    'droit_reel': owner.get('droit_reel', ''),
                    'designation_parcelle': '',
                    'nom': owner.get('nom', ''),
                    'prenom': owner.get('prenom', ''),
                    'numero_majic': owner.get('numero_proprietaire', ''),
                    'voie': owner.get('street_address', ''),
                    'post_code': owner.get('post_code', ''),
                    'city': owner.get('city', '')
                }
                merged_properties.append(merged_prop)
        
        logger.info(f"üéâ Fusion hybride termin√©e: {len(merged_properties)} propri√©t√©s")
        return merged_properties

    def convert_structured_to_standard_format(self, structured_prop: Dict) -> Dict:
        """Convertit les donn√©es pdfplumber au format standard."""
        return {
            'department': '',
            'commune': '',
            'prefixe': structured_prop.get('Pr√©fixe', ''),
            'section': structured_prop.get('Sec', structured_prop.get('Section', '')),
            'numero': structured_prop.get('N¬∞ Plan', structured_prop.get('Num√©ro', '')),
            'contenance': structured_prop.get('Contenance', ''),
            'droit_reel': '',
            'designation_parcelle': structured_prop.get('Adresse', structured_prop.get('D√©signation', '')),
            'nom': '',
            'prenom': '',
            'numero_majic': '',
            'voie': '',
            'post_code': '',
            'city': ''
        }

    def process_single_pdf_hybrid(self, pdf_path: Path) -> List[Dict]:
        """
        TRAITEMENT HYBRIDE PRINCIPAL : pdfplumber + OpenAI Vision
        R√©plique l'approche Make pour des r√©sultats optimaux.
        """
        logger.info(f"üöÄ TRAITEMENT HYBRIDE de {pdf_path.name}")
        
        # √âTAPE 1: Extraction tableaux avec pdfplumber
        structured_data = self.extract_tables_with_pdfplumber(pdf_path)
        
        # √âTAPE 2: Extraction propri√©taires avec OpenAI Vision
        owners_data = self.extract_owners_with_vision_simple(pdf_path)
        
        # √âTAPE 3: Fusion intelligente
        merged_properties = self.merge_structured_and_vision_data(structured_data, owners_data, pdf_path.name)
        
        # √âTAPE 4: Finalisation avec IDs uniques et propagation
        final_properties = []
        for prop in merged_properties:
            # G√©n√©rer l'ID unique
            unique_id = self.generate_unique_id(
                department=prop.get('department', '00'),
                commune=prop.get('commune', '000'),
                section=prop.get('section', 'A'),
                numero=prop.get('numero', '0000'),
                prefixe=prop.get('prefixe', '')
            )
            
            # Ajouter l'ID unique et le fichier source
            prop['id'] = unique_id
            prop['fichier_source'] = pdf_path.name
            
            final_properties.append(prop)
        
        # S√©paration automatique des pr√©fixes coll√©s
        if final_properties:
            final_properties = self.separate_stuck_prefixes(final_properties)
        
        # Propagation des valeurs
        if final_properties:
            final_properties = self.propagate_values_downward(final_properties, ['designation_parcelle', 'prefixe'])
        
        logger.info(f"‚úÖ HYBRIDE termin√©: {len(final_properties)} propri√©t√©(s) pour {pdf_path.name}")
        return final_properties

    def process_single_pdf(self, pdf_path: Path) -> List[Dict]:
        """
        Traite un PDF MULTI-PAGES avec fusion intelligente des donn√©es.
        
        Args:
            pdf_path: Chemin vers le fichier PDF
            
        Returns:
            Liste des propri√©taires avec leurs informations fusionn√©es
        """
        logger.info(f"üîÑ Traitement MULTI-PAGES de {pdf_path.name}")
        
        # Convertir en images
        images = self.pdf_to_images(pdf_path)
        if not images:
            logger.error(f"‚ùå √âchec de la conversion en images pour {pdf_path.name}")
            return []
        
        # PHASE 1: Extraction de TOUTES les pages
        all_page_data = []
        for page_num, image_data in enumerate(images, 1):
            logger.info(f"üìÑ Extraction page {page_num}/{len(images)} pour {pdf_path.name}")
            extracted_data = self.extract_info_with_gpt4o(image_data, f"{pdf_path.name} (page {page_num})")
            if extracted_data and 'proprietes' in extracted_data:
                all_page_data.extend(extracted_data['proprietes'])
                logger.info(f"‚úÖ Page {page_num}: {len(extracted_data['proprietes'])} √©l√©ment(s) extraits")
            else:
                logger.warning(f"‚ö†Ô∏è Page {page_num}: aucune donn√©e extraite")
        
        if not all_page_data:
            logger.error(f"‚ùå Aucune donn√©e extraite de {pdf_path.name}")
            return []
        
        logger.info(f"üìä Total √©l√©ments bruts extraits: {len(all_page_data)}")
        
        # PHASE 2: FUSION INTELLIGENTE multi-pages
        merged_properties = self.smart_merge_multi_page_data(all_page_data, pdf_path.name)
        
        # PHASE 3: Finalisation avec IDs uniques
        final_properties = []
        for prop in merged_properties:
            # G√©n√©rer l'ID unique
            unique_id = self.generate_unique_id(
                department=prop.get('department', '00'),
                commune=prop.get('commune', '000'),
                section=prop.get('section', 'A'),
                numero=prop.get('numero', '0000'),
                prefixe=prop.get('prefixe', '')
            )
            
            # Ajouter l'ID unique et le fichier source
            prop['id'] = unique_id
            prop['fichier_source'] = pdf_path.name
            
            final_properties.append(prop)
        
        logger.info(f"üéâ {pdf_path.name} FUSIONN√â avec succ√®s - {len(final_properties)} propri√©t√©(s) compl√®te(s)")
        return final_properties

    def detect_pdf_format(self, image_data: bytes) -> Dict:
        """
        D√âTECTE automatiquement le type/format du PDF cadastral.
        Chaque d√©partement/commune a son propre format !
        """
        detection_prompt = """
        Tu es un expert en documents cadastraux fran√ßais. Analyse cette image et d√©termine :

        1. TYPE DE DOCUMENT :
           - Extrait cadastral (avec propri√©taires)
           - Matrice cadastrale  
           - √âtat de section
           - Plan cadastral
           - Autre

        2. FORMAT/√âPOQUE :
           - Moderne (post-2010) - tableaux structur√©s, codes MAJIC
           - Interm√©diaire (2000-2010) - semi-structur√©
           - Ancien (pr√©-2000) - format libre

        3. MISE EN PAGE :
           - Une seule page avec tout
           - Multi-pages (info dispers√©e)
           - Tableau structur√©
           - Texte libre

        4. INFORMATIONS VISIBLES :
           - D√©partement/commune en en-t√™te
           - Codes MAJIC visibles (6 chars alphanum√©riques)
           - Parcelles avec sections/num√©ros
           - Propri√©taires avec noms/pr√©noms
           - Adresses compl√®tes

        R√©ponds en JSON :
        {
            "document_type": "extrait|matrice|section|plan|autre",
            "format_era": "moderne|intermediaire|ancien", 
            "layout": "single_page|multi_page|tableau|texte_libre",
            "visible_info": {
                "location_header": true/false,
                "majic_codes": true/false,
                "parcels_listed": true/false,
                "owners_listed": true/false,
                "addresses_present": true/false
            },
            "extraction_strategy": "complete|location_focus|parcel_focus|owner_focus|mixed"
        }
        """

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": detection_prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64.b64encode(image_data).decode()}"}}
                        ]
                    }
                ],
                max_tokens=500,
                temperature=0.1
            )
            
            content = response.choices[0].message.content
            if not content or content.strip() == "":
                logger.warning("R√©ponse API vide pour d√©tection format")
                raise ValueError("R√©ponse vide")
            
            # Nettoyer le contenu et extraire le JSON
            content = content.strip()
            
            # Chercher un objet JSON dans la r√©ponse
            start_idx = content.find('{')
            end_idx = content.rfind('}')
            
            if start_idx == -1 or end_idx == -1:
                logger.warning(f"Pas de JSON trouv√© dans la r√©ponse: {content[:100]}...")
                raise ValueError("Pas de JSON dans la r√©ponse")
            
            json_content = content[start_idx:end_idx+1]
            detection_result = json.loads(json_content)
            
            # V√©rifier que tous les champs requis sont pr√©sents
            required_fields = ["document_type", "format_era", "layout", "extraction_strategy"]
            for field in required_fields:
                if field not in detection_result:
                    logger.warning(f"Champ manquant dans d√©tection: {field}")
                    raise ValueError(f"Champ manquant: {field}")
            
            logger.info(f"Format d√©tect√©: {detection_result.get('document_type')} - {detection_result.get('format_era')} - Strat√©gie: {detection_result.get('extraction_strategy')}")
            return detection_result
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"√âchec d√©tection format: {e}")
            # Format par d√©faut pour extraction maximale
            return {
                "document_type": "extrait",
                "format_era": "moderne", 
                "layout": "multi_page",
                "visible_info": {
                    "location_header": True,
                    "majic_codes": True,
                    "parcels_listed": True,
                    "owners_listed": True,
                    "addresses_present": True
                },
                "extraction_strategy": "complete"
            }
        except Exception as e:
            logger.error(f"Erreur d√©tection format: {e}")
            return {
                "document_type": "extrait",
                "format_era": "moderne", 
                "layout": "multi_page",
                "visible_info": {
                    "location_header": True,
                    "majic_codes": True,
                    "parcels_listed": True,
                    "owners_listed": True,
                    "addresses_present": True
                },
                "extraction_strategy": "complete"
            }

    def adapt_extraction_prompt(self, format_info: Dict) -> str:
        """
        ADAPTE le prompt d'extraction selon le format d√©tect√©.
        Chaque type de PDF n√©cessite une approche diff√©rente !
        """
        base_examples = """
üéØ EXEMPLES ULTRA-PR√âCIS avec TOUTES les colonnes importantes:

EXEMPLE D√âPARTEMENT 51:
{
  "department": "51",
  "commune": "179", 
  "prefixe": "",
  "section": "000ZE",
  "numero": "0025",
  "contenance": "001045",     ‚¨ÖÔ∏è CONTENANCE = SURFACE en m¬≤ (OBLIGATOIRE!)
  "droit_reel": "US",
  "designation_parcelle": "LES ROULLIERS",
  "nom": "LAMBIN",
  "prenom": "DIDIER JEAN GUY",
  "numero_majic": "M8BNF6",
  "voie": "1 RUE D AVAT",
  "post_code": "51240",
  "city": "COUPEVILLE"
}

EXEMPLE D√âPARTEMENT 25:
{
  "department": "25",
  "commune": "227",
  "prefixe": "",
  "section": "000ZD",
  "numero": "0005",
  "contenance": "000150",     ‚¨ÖÔ∏è SURFACE = 150m¬≤ (CHERCHE √ßa partout!)
  "droit_reel": "PP",
  "designation_parcelle": "LE GRAND CHAMP",
  "nom": "MARTIN",
  "prenom": "PIERRE",
  "numero_majic": "MB43HC",
  "voie": "15 RUE DE LA PAIX", 
  "post_code": "25000",
  "city": "BESANCON"
}
"""

        # Adaptations selon le format d√©tect√©
        if format_info.get('document_type') == 'matrice':
            specific_instructions = """
üéØ DOCUMENT TYPE: MATRICE CADASTRALE
- Focus sur les TABLES avec colonnes structur√©es
- Propri√©taires souvent dans colonne de droite
- Parcelles dans colonnes de gauche
- ‚≠ê CONTENANCE dans colonne "Surface" ou "Cont." ou chiffres + m¬≤
- Scan ligne par ligne m√©thodiquement
"""
        elif format_info.get('layout') == 'tableau':
            specific_instructions = """
üéØ FORMAT: TABLEAU STRUCTUR√â
- Identifie les EN-T√äTES de colonnes
- Cherche colonnes: "Surface", "Contenance", "m¬≤", "ares", "ca"
- Lit chaque LIGNE du tableau
- Associe chaque valeur √† sa colonne
- ‚≠ê CONTENANCE = valeurs num√©riques dans colonnes surface
- Attention aux cellules fusionn√©es
"""
        elif format_info.get('format_era') == 'ancien':
            specific_instructions = """
üéØ FORMAT: ANCIEN/LIBRE
- Scan TOUT le texte zone par zone
- Cherche les motifs : "NOM Pr√©nom", codes, adresses
- Les infos peuvent √™tre DISPERS√âES
- Utilise le contexte pour regrouper
"""
        else:
            specific_instructions = """
üéØ FORMAT: MODERNE/STANDARD
- Scan syst√©matique de HAUT en BAS
- En-t√™tes pour d√©partement/commune
- Propri√©taires en MAJUSCULES
- Codes MAJIC (6 caract√®res)
- ‚≠ê CONTENANCE = recherche "m¬≤", "ares", "ca" + chiffres associ√©s
- Sections format "000XX" ou "XX" + num√©ros parcelles
"""

        # Instructions sp√©cifiques selon ce qui est visible
        if not format_info.get('visible_info', {}).get('majic_codes'):
            majic_note = "‚ö†Ô∏è CODES MAJIC probablement ABSENTS - concentre-toi sur noms/adresses"
        else:
            majic_note = "üéØ CODES MAJIC PR√âSENTS - scan tous les codes 6 caract√®res"

        if format_info.get('extraction_strategy') == 'location_focus':
            strategy_note = "üéØ PRIORIT√â: D√©partement/Commune/Localisation"
        elif format_info.get('extraction_strategy') == 'owner_focus':
            strategy_note = "üéØ PRIORIT√â: Propri√©taires/Noms/Adresses/MAJIC"
        elif format_info.get('extraction_strategy') == 'parcel_focus':
            strategy_note = "üéØ PRIORIT√â: Parcelles/Sections/Num√©ros/Contenances"
        else:
            strategy_note = "üéØ EXTRACTION COMPL√àTE de TOUTES les informations"

        # Prompt adaptatif final
        adapted_prompt = f"""
Tu es un EXPERT en extraction de donn√©es cadastrales fran√ßaises. Ce document a √©t√© analys√© automatiquement.

{specific_instructions}

{strategy_note}
{majic_note}

{base_examples}

üîç INSTRUCTIONS DE SCAN ADAPTATIF ULTRA-PR√âCIS:
1. Applique la strat√©gie d√©tect√©e ci-dessus
2. Scan M√âTHODIQUEMENT selon le type de mise en page  
3. ‚≠ê CONTENANCE = SURFACE : Cherche PARTOUT les chiffres suivis de "m¬≤", "ares", "ca" (centiares)
4. ‚≠ê SECTION + NUM√âRO : Formats "000ZE", "ZE", "003", toujours ensemble
5. ‚≠ê NOMS/PR√âNOMS : MAJUSCULES = nom, minuscules = pr√©nom  
6. ‚≠ê CODES MAJIC : Exactement 6 caract√®res alphanum√©riques
7. ‚≠ê ADRESSES : Num√©ro + nom de rue + code postal + ville
8. Collecte TOUTES les informations visibles
9. Regroupe intelligemment les donn√©es dispers√©es
10. Ne laisse RIEN passer, m√™me les valeurs partielles
11. ‚≠ê PR√âFIXE (TR√àS RARE mais CRUCIAL) : Le pr√©fixe n'appara√Æt que dans quelques PDFs sp√©ciaux, mais quand il existe, il est IMP√âRATIF de l'extraire !
    - Position : dans les tableaux "Propri√©t√©(s) non b√¢tie(s)" AVANT la d√©signation du lieu-dit
    - Format typique : "ZY 8", "AB 12", "000AC 5" ‚Üí pr√©fixe="ZY"/"AB"/"000AC", section="8"/"12"/"5"  
    - Colonne souvent nomm√©e "Pr√©fixe", "Pfxe" ou pr√©c√®de directement la section
    - Si AUCUN pr√©fixe visible = "", ne JAMAIS inventer
    - Si pr√©fixe trouv√© = l'extraire avec PR√âCISION ABSOLUE

‚ö†Ô∏è R√àGLES STRICTES:
- ADAPTE ta lecture au format d√©tect√©
- Si info partiellement visible, INCLUS-LA
- JAMAIS de "N/A" - utilise "" si vraiment absent
- IGNORE aucun d√©tail m√™me petit
- Retourne TOUS les propri√©taires trouv√©s

üì§ FORMAT R√âPONSE:
{{
  "proprietes": [
    {{
      "department": "",
      "commune": "",
      "prefixe": "ZY",
      "section": "",
      "numero": "",
      "contenance": "",
      "droit_reel": "",
      "designation_parcelle": "",
      "nom": "",
      "prenom": "",
      "numero_majic": "",
      "voie": "",
      "post_code": "",
      "city": ""
    }}
  ]
}}
"""
        return adapted_prompt

    def propagate_values_downward(self, properties: List[Dict], fields: List[str]) -> List[Dict]:
        """
        Propage les valeurs des champs sp√©cifi√©s vers le bas si elles sont vides.
        M√©morise la derni√®re valeur vue pour chaque champ et la propage.
        """
        last_seen_values = {field: None for field in fields}
        
        updated_properties = []
        for prop in properties:
            updated_prop = prop.copy()
            for field in fields:
                if updated_prop.get(field) is None or updated_prop.get(field) == "":
                    if last_seen_values[field] is not None:
                        updated_prop[field] = last_seen_values[field]
                else:
                    last_seen_values[field] = updated_prop[field]
            updated_properties.append(updated_prop)
            
        return updated_properties

    def separate_stuck_prefixes(self, properties: List[Dict]) -> List[Dict]:
        """
        S√âPARATION AUTOMATIQUE des pr√©fixes coll√©s aux sections.
        D√©tecte et s√©pare les patterns comme "302A" ‚Üí pr√©fixe="302", section="A"
        """
        import re
        
        updated_properties = []
        separated_count = 0
        
        for prop in properties:
            updated_prop = prop.copy()
            section = str(updated_prop.get('section', '')).strip()
            current_prefixe = str(updated_prop.get('prefixe', '')).strip()
            
            # Ne traiter que si la section n'est pas vide et le pr√©fixe est vide
            if section and not current_prefixe:
                # Pattern pour d√©tecter pr√©fixe num√©rique coll√© √† section alphab√©tique
                # Exemples: "302A", "302 A", "302AB", "001ZD", "123AC", etc.
                pattern = r'^(\d+)\s*([A-Z]+)$'  # \s* permet les espaces optionnels
                match = re.match(pattern, section)
                
                if match:
                    detected_prefixe = match.group(1)  # La partie num√©rique (302)
                    detected_section = match.group(2)  # La partie alphab√©tique (A)
                    
                    # Mettre √† jour les champs
                    updated_prop['prefixe'] = detected_prefixe
                    updated_prop['section'] = detected_section
                    
                    separated_count += 1
                    logger.info(f"üîç Pr√©fixe s√©par√©: '{section}' ‚Üí pr√©fixe='{detected_prefixe}' section='{detected_section}'")
            
            updated_properties.append(updated_prop)
        
        if separated_count > 0:
            logger.info(f"‚úÇÔ∏è S√©paration automatique: {separated_count} pr√©fixe(s) d√©tect√©(s) et s√©par√©(s)")
        
        return updated_properties

    def smart_merge_multi_page_data(self, all_page_data: List[Dict], filename: str) -> List[Dict]:
        """
        FUSION INTELLIGENTE des donn√©es multi-pages.
        Combine les informations dispers√©es sur plusieurs pages en propri√©t√©s compl√®tes.
        """
        logger.info(f"üß† Fusion intelligente de {len(all_page_data)} √©l√©ments pour {filename}")
        
        # S√©parer les donn√©es par type d'information
        location_data = {}  # d√©partement/commune communs
        owners_data = []    # propri√©taires avec infos compl√®tes
        parcels_data = []   # parcelles avec sections/num√©ros
        mixed_data = []     # donn√©es mixtes
        
        # PHASE 1: Classification des donn√©es extraites
        for item in all_page_data:
            has_owner = bool(item.get('nom') or item.get('prenom'))
            has_parcel = bool(item.get('section') or item.get('numero'))
            has_location = bool(item.get('department') or item.get('commune'))
            
            if has_location and not location_data:
                # Stocker les infos de localisation (d√©partement/commune)
                location_data = {
                    'department': item.get('department', ''),
                    'commune': item.get('commune', ''),
                    'post_code': item.get('post_code', ''),
                    'city': item.get('city', '')
                }
                logger.info(f"üìç Localisation trouv√©e: {location_data}")
            
            if has_owner and has_parcel:
                # Donn√©es compl√®tes
                mixed_data.append(item)
            elif has_owner:
                # Donn√©es propri√©taire uniquement
                owners_data.append(item)
            elif has_parcel:
                # Donn√©es parcelle uniquement  
                parcels_data.append(item)
        
        logger.info(f"üìã Classification: {len(mixed_data)} compl√®tes, {len(owners_data)} propri√©taires, {len(parcels_data)} parcelles")
        
        # PHASE 2: Strat√©gie de fusion selon les donn√©es disponibles
        merged_properties = []
        
        if mixed_data:
            # Cas optimal: donn√©es d√©j√† compl√®tes
            for item in mixed_data:
                # Enrichir avec les infos de localisation si manquantes
                if not item.get('department') and location_data.get('department'):
                    item['department'] = location_data['department']
                if not item.get('commune') and location_data.get('commune'):
                    item['commune'] = location_data['commune']
                if not item.get('post_code') and location_data.get('post_code'):
                    item['post_code'] = location_data['post_code']
                if not item.get('city') and location_data.get('city'):
                    item['city'] = location_data['city']
                
                merged_properties.append(item)
                
        elif owners_data and parcels_data:
            # Fusion propri√©taires + parcelles
            logger.info(f"üîó Fusion {len(owners_data)} propri√©taires avec {len(parcels_data)} parcelles")
            
            # Strat√©gie: croiser chaque propri√©taire avec chaque parcelle
            for owner in owners_data:
                for parcel in parcels_data:
                    merged_prop = self.merge_owner_parcel(owner, parcel, location_data)
                    merged_properties.append(merged_prop)
                    
        elif owners_data:
            # Seulement des propri√©taires - enrichir avec localisation
            for owner in owners_data:
                merged_prop = owner.copy()
                # Ajouter les infos de localisation
                for key, value in location_data.items():
                    if not merged_prop.get(key) and value:
                        merged_prop[key] = value
                merged_properties.append(merged_prop)
                
        elif parcels_data:
            # Seulement des parcelles - enrichir avec localisation  
            for parcel in parcels_data:
                merged_prop = parcel.copy()
                # Ajouter les infos de localisation
                for key, value in location_data.items():
                    if not merged_prop.get(key) and value:
                        merged_prop[key] = value
                merged_properties.append(merged_prop)
        
        else:
            # Cas de secours: utiliser toutes les donn√©es brutes
            merged_properties = all_page_data
        
        # PHASE 3: Nettoyage et d√©duplication
        cleaned_properties = self.clean_and_deduplicate(merged_properties, filename)
        
        # S√©paration automatique des pr√©fixes coll√©s
        cleaned_properties = self.separate_stuck_prefixes(cleaned_properties)
        
        # Propager les valeurs de designation_parcelle et prefixe vers le bas
        cleaned_properties = self.propagate_values_downward(cleaned_properties, ['designation_parcelle', 'prefixe'])
        
        logger.info(f"‚ú® Fusion termin√©e: {len(cleaned_properties)} propri√©t√©s finales")
        return cleaned_properties

    def merge_owner_parcel(self, owner: Dict, parcel: Dict, location: Dict) -> Dict:
        """
        Fusionne intelligemment les donn√©es propri√©taire + parcelle + localisation.
        """
        merged = {}
        
        # Priorit√© aux donn√©es propri√©taire pour les champs propri√©taire
        owner_fields = ['nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city']
        for field in owner_fields:
            merged[field] = owner.get(field, '')
        
        # Priorit√© aux donn√©es parcelle pour les champs parcelle
        parcel_fields = ['section', 'numero', 'contenance', 'droit_reel', 'designation_parcelle', 'prefixe']
        for field in parcel_fields:
            merged[field] = parcel.get(field, '')
        
        # Utiliser les donn√©es de localisation pour les champs manquants
        location_fields = ['department', 'commune', 'post_code', 'city']
        for field in location_fields:
            if not merged.get(field) and location.get(field):
                merged[field] = location[field]
        
        # Si encore des champs manquants, essayer de les r√©cup√©rer de l'autre source
        for field in ['department', 'commune']:
            if not merged.get(field):
                merged[field] = owner.get(field, parcel.get(field, ''))
        
        return merged

    def clean_and_deduplicate(self, properties: List[Dict], filename: str) -> List[Dict]:
        """
        Nettoie et d√©duplique les propri√©t√©s fusionn√©es.
        """
        if not properties:
            return []
        
        cleaned = []
        seen_combinations = set()
        
        for prop in properties:
            # Cr√©er une cl√© unique bas√©e sur les champs critiques
            key_fields = [
                prop.get('nom', ''),
                prop.get('prenom', ''),
                prop.get('section', ''),
                prop.get('numero', ''),
                prop.get('numero_majic', '')
            ]
            unique_key = '|'.join(str(f).strip() for f in key_fields)
            
            # Ignorer les entr√©es compl√®tement vides
            if not any(key_fields) or unique_key == '||||':
                continue
            
            # D√©duplication
            if unique_key not in seen_combinations:
                seen_combinations.add(unique_key)
                
                # Assurer que tous les champs requis existent
                required_fields = [
                    'department', 'commune', 'prefixe', 'section', 'numero', 
                    'contenance', 'droit_reel', 'designation_parcelle', 
                    'nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city'
                ]
                
                for field in required_fields:
                    if field not in prop:
                        prop[field] = ''
                
                cleaned.append(prop)
        
        logger.info(f"üßπ Nettoyage: {len(properties)} ‚Üí {len(cleaned)} propri√©t√©s apr√®s d√©duplication")
        return cleaned

    def export_to_csv(self, all_properties: List[Dict], output_filename: str = "output.csv") -> None:
        """
        Exporte toutes les donn√©es vers un fichier CSV avec s√©parateur point-virgule.
        
        Args:
            all_properties: Liste de toutes les propri√©t√©s
            output_filename: Nom du fichier de sortie
        """
        if not all_properties:
            logger.warning("Aucune donn√©e √† exporter")
            return
        
        # Cr√©er le DataFrame
        df = pd.DataFrame(all_properties)
        
        # Colonnes selon les sp√©cifications du client (avec contenance d√©taill√©e)
        columns_order = [
            'department', 'commune', 'prefixe', 'section', 'numero', 
            'contenance_ha', 'contenance_a', 'contenance_ca',
            'droit_reel', 'designation_parcelle', 'nom', 'prenom', 'numero_majic', 
            'voie', 'post_code', 'city', 'id', 'fichier_source'
        ]
        
        # Renommer les colonnes pour plus de clart√©
        column_mapping = {
            'department': 'D√©partement',
            'commune': 'Commune', 
            'prefixe': 'Pr√©fixe',
            'section': 'Section',
            'numero': 'Num√©ro',
            'contenance_ha': 'Contenance HA',
            'contenance_a': 'Contenance A', 
            'contenance_ca': 'Contenance CA',
            'droit_reel': 'Droit r√©el',
            'designation_parcelle': 'Designation Parcelle',
            'nom': 'Nom Propri',
            'prenom': 'Pr√©nom Propri',
            'numero_majic': 'N¬∞MAJIC',
            'voie': 'Voie',
            'post_code': 'CP',
            'city': 'Ville',
            'id': 'id',
            'fichier_source': 'Fichier source'
        }
        
        # R√©organiser et renommer
        df = df.reindex(columns=columns_order, fill_value='')
        df = df.rename(columns=column_mapping)
        
        # Export CSV avec s√©parateur point-virgule (meilleur pour Excel fran√ßais)
        output_path = self.output_dir / output_filename
        df.to_csv(output_path, index=False, encoding='utf-8-sig', sep=';')
        
        logger.info(f"üìä Donn√©es CSV export√©es vers {output_path} (s√©parateur: ;)")
        logger.info(f"üìà Total: {len(all_properties)} propri√©t√©(s) dans {len(df['Fichier source'].unique())} fichier(s)")
        
        return output_path

    def export_to_excel(self, all_properties: List[Dict], output_filename: str = "output.xlsx") -> None:
        """
        Exporte toutes les donn√©es vers un fichier Excel (.xlsx).
        
        Args:
            all_properties: Liste de toutes les propri√©t√©s
            output_filename: Nom du fichier de sortie
        """
        if not all_properties:
            logger.warning("Aucune donn√©e √† exporter en Excel")
            return
        
        # Cr√©er le DataFrame
        df = pd.DataFrame(all_properties)
        
        # Colonnes selon les sp√©cifications du client (avec contenance d√©taill√©e)
        columns_order = [
            'department', 'commune', 'prefixe', 'section', 'numero', 
            'contenance_ha', 'contenance_a', 'contenance_ca',
            'droit_reel', 'designation_parcelle', 'nom', 'prenom', 'numero_majic', 
            'voie', 'post_code', 'city', 'id', 'fichier_source'
        ]
        
        # Renommer les colonnes pour plus de clart√©
        column_mapping = {
            'department': 'D√©partement',
            'commune': 'Commune', 
            'prefixe': 'Pr√©fixe',
            'section': 'Section',
            'numero': 'Num√©ro',
            'contenance_ha': 'Contenance HA',
            'contenance_a': 'Contenance A', 
            'contenance_ca': 'Contenance CA',
            'droit_reel': 'Droit r√©el',
            'designation_parcelle': 'Designation Parcelle',
            'nom': 'Nom Propri',
            'prenom': 'Pr√©nom Propri',
            'numero_majic': 'N¬∞MAJIC',
            'voie': 'Voie',
            'post_code': 'CP',
            'city': 'Ville',
            'id': 'id',
            'fichier_source': 'Fichier source'
        }
        
        # R√©organiser et renommer
        df = df.reindex(columns=columns_order, fill_value='')
        df = df.rename(columns=column_mapping)
        
        # Export Excel
        output_path = self.output_dir / output_filename
        df.to_excel(output_path, index=False, engine='openpyxl')
        
        logger.info(f"üìä Donn√©es Excel export√©es vers {output_path}")
        logger.info(f"üìà Total: {len(all_properties)} propri√©t√©(s) dans {len(df['Fichier source'].unique())} fichier(s)")
        
        return output_path

    def run(self) -> None:
        """
        TRAITEMENT PAR LOTS OPTIMIS√â pour extraction maximale.
        """
        logger.info("üöÄ D√©marrage de l'extraction BATCH OPTIMIS√âE")
        
        # Lister les fichiers PDF
        pdf_files = self.list_pdf_files()
        
        if not pdf_files:
            logger.warning("‚ùå Aucun fichier PDF trouv√© dans le dossier input/")
            return
        
        logger.info(f"üìÑ {len(pdf_files)} PDF(s) d√©tect√©(s) pour traitement par lots")
        
        # PHASE 1: PR√â-ANALYSE du lot pour strat√©gie globale
        batch_strategy = self.analyze_pdf_batch(pdf_files)
        logger.info(f"üß† Strat√©gie globale: {batch_strategy.get('approach', 'standard')}")
        
        # PHASE 2: TRAITEMENT OPTIMIS√â par lots
        all_properties = self.process_pdf_batch_optimized(pdf_files, batch_strategy)
        
        # PHASE 3: POST-TRAITEMENT pour combler les trous
        if all_properties:
            enhanced_properties = self.post_process_batch_results(all_properties, pdf_files)
            
            # PHASE 4: EXPORT avec statistiques d√©taill√©es
            self.export_to_csv_with_stats(enhanced_properties)
            
            # Statistiques finales
            total_props = len(enhanced_properties)
            total_pdfs = len(pdf_files)
            avg_per_pdf = total_props / total_pdfs if total_pdfs > 0 else 0
            
            logger.info(f"‚úÖ EXTRACTION BATCH TERMIN√âE!")
            logger.info(f"üìä {total_props} propri√©t√©s extraites de {total_pdfs} PDFs")
            logger.info(f"üìà Moyenne: {avg_per_pdf:.1f} propri√©t√©s/PDF")
        else:
            logger.warning("‚ùå Aucune donn√©e extraite du lot")

    def analyze_pdf_batch(self, pdf_files: List[Path]) -> Dict:
        """
        PR√â-ANALYSE du lot de PDFs pour d√©terminer la strat√©gie optimale.
        """
        logger.info(f"üîç Pr√©-analyse de {len(pdf_files)} PDFs...")
        
        batch_info = {
            'total_files': len(pdf_files),
            'formats_detected': {},
            'total_pages': 0,
            'approach': 'standard',
            'common_location': {},
            'estimated_properties': 0
        }
        
        # Analyser un √©chantillon pour d√©tecter les patterns
        sample_size = min(3, len(pdf_files))  # Analyser max 3 PDFs pour la strat√©gie
        
        for i, pdf_file in enumerate(pdf_files[:sample_size]):
            logger.info(f"üîç Analyse √©chantillon {i+1}/{sample_size}: {pdf_file.name}")
            
            # Convertir premi√®re page pour analyse rapide
            images = self.pdf_to_images(pdf_file)
            if images:
                batch_info['total_pages'] += len(images)
                
                # D√©tecter le format de ce PDF
                format_info = self.detect_pdf_format(images[0])
                format_key = f"{format_info.get('document_type')}_{format_info.get('format_era')}"
                
                if format_key in batch_info['formats_detected']:
                    batch_info['formats_detected'][format_key] += 1
                else:
                    batch_info['formats_detected'][format_key] = 1
        
        # D√©terminer la strat√©gie globale bas√©e sur l'analyse
        if len(batch_info['formats_detected']) == 1:
            # Format homog√®ne - strat√©gie sp√©cialis√©e
            batch_info['approach'] = 'homogeneous_optimized'
        elif len(pdf_files) > 10:
            # Beaucoup de PDFs - strat√©gie volume
            batch_info['approach'] = 'high_volume_batch'
        else:
            # Format mixte - strat√©gie adaptative
            batch_info['approach'] = 'mixed_adaptive'
        
        logger.info(f"üìä Formats d√©tect√©s: {batch_info['formats_detected']}")
        logger.info(f"üéØ Strat√©gie choisie: {batch_info['approach']}")
        
        return batch_info

    def process_pdf_batch_optimized(self, pdf_files: List[Path], batch_strategy: Dict) -> List[Dict]:
        """
        TRAITEMENT PAR LOTS OPTIMIS√â selon la strat√©gie d√©termin√©e.
        """
        all_properties = []
        approach = batch_strategy.get('approach', 'standard')
        
        logger.info(f"‚öôÔ∏è Traitement par lots - Approche: {approach}")
        
        if approach == 'homogeneous_optimized':
            # Format homog√®ne - traitement optimis√©
            all_properties = self.process_homogeneous_batch(pdf_files)
        elif approach == 'high_volume_batch':
            # Volume √©lev√© - traitement parall√®le simul√©
            all_properties = self.process_high_volume_batch(pdf_files)
        else:
            # Approche adaptative mixte (par d√©faut)
            all_properties = self.process_mixed_adaptive_batch(pdf_files)
        
        logger.info(f"üìä {len(all_properties)} propri√©t√©s extraites au total")
        return all_properties

    def process_homogeneous_batch(self, pdf_files: List[Path]) -> List[Dict]:
        """
        Traitement optimis√© pour un lot de PDFs homog√®nes.
        """
        logger.info("üîÑ Traitement homog√®ne optimis√© STYLE MAKE")
        all_properties = []
        
        # Traiter avec approche Make exacte
        for i, pdf_file in enumerate(pdf_files, 1):
            logger.info(f"üìÑ Traitement Make {i}/{len(pdf_files)}: {pdf_file.name}")
            
            properties = self.process_like_make(pdf_file)
            all_properties.extend(properties)
            
            # Log interm√©diaire pour suivi
            if i % 5 == 0:
                logger.info(f"üìä Progr√®s: {len(all_properties)} propri√©t√©s extraites jusqu'ici")
        
        return all_properties

    def process_high_volume_batch(self, pdf_files: List[Path]) -> List[Dict]:
        """
        Traitement optimis√© pour gros volume avec style Make.
        """
        logger.info("üöÄ Traitement haut volume STYLE MAKE")
        all_properties = []
        
        for i, pdf_file in enumerate(pdf_files, 1):
            logger.info(f"üìÑ Volume Make {i}/{len(pdf_files)}: {pdf_file.name}")
            
            properties = self.process_like_make(pdf_file)
            all_properties.extend(properties)
            
            # Logs de progression
            if i % 10 == 0:
                logger.info(f"üìä Progression: {i}/{len(pdf_files)} fichiers trait√©s")
        
        return all_properties

    def process_mixed_adaptive_batch(self, pdf_files: List[Path]) -> List[Dict]:
        """
        Traitement adaptatif mixte avec style Make.
        """
        logger.info("üéØ Traitement adaptatif mixte STYLE MAKE")
        all_properties = []
        
        for i, pdf_file in enumerate(pdf_files, 1):
            logger.info(f"üìÑ Adaptatif Make {i}/{len(pdf_files)}: {pdf_file.name}")
            
            properties = self.process_like_make(pdf_file)
            all_properties.extend(properties)
            
            # Suivi adaptatif
            if len(properties) == 0:
                logger.warning(f"‚ö†Ô∏è Aucune propri√©t√© extraite pour {pdf_file.name}")
        
        return all_properties

    def post_process_batch_results(self, all_properties: List[Dict], pdf_files: List[Path]) -> List[Dict]:
        """
        POST-TRAITEMENT S√âCURIS√â - AUCUN cross-r√©f√©rencement pour garantir la fiabilit√©.
        Mode extraction PURE uniquement.
        """
        logger.info(f"üîß Post-traitement S√âCURIS√â de {len(all_properties)} propri√©t√©s")
        logger.info("üõ°Ô∏è Mode FIABILIT√â MAXIMALE - Aucun cross-r√©f√©rencement")
        
        if not all_properties:
            return all_properties
        
        # Analyser les champs manquants √† l'√©chelle du lot (pour statistiques seulement)
        missing_stats = self.analyze_missing_fields_batch(all_properties)
        logger.info(f"üìä Champs incomplets d√©tect√©s: {len(missing_stats)} types")
        
        # ‚ùå CROSS-R√âF√âRENCEMENT D√âSACTIV√â pour √©viter les erreurs
        # Les colonnes vides restent vides = FIABILIT√â GARANTIE
        logger.info("üö´ Cross-r√©f√©rencement D√âSACTIV√â - Extraction pure uniquement")
        
        # D√©duplication finale √† l'√©chelle du lot (s√©curis√©e)
        final_properties = self.deduplicate_batch_results(all_properties)
        
        # Statistiques de qualit√©
        removed_duplicates = len(all_properties) - len(final_properties)
        if removed_duplicates > 0:
            logger.info(f"üßπ {removed_duplicates} doublons supprim√©s")
        
        logger.info(f"‚úÖ Post-traitement termin√©: {len(final_properties)} propri√©t√©s FIABLES")
        logger.info("üéØ Toutes les donn√©es sont extraites directement des PDFs (AUCUNE invention)")
        
        return final_properties

    def analyze_missing_fields_batch(self, properties: List[Dict]) -> Dict:
        """
        Analyse les champs manquants √† l'√©chelle du lot complet.
        """
        missing_stats = {}
        total_props = len(properties)
        
        required_fields = [
            'department', 'commune', 'section', 'numero', 'contenance',
            'nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city'
        ]
        
        for field in required_fields:
            empty_count = sum(1 for prop in properties if not prop.get(field))
            if empty_count > 0:
                missing_stats[field] = {
                    'empty_count': empty_count,
                    'completion_rate': ((total_props - empty_count) / total_props) * 100
                }
        
        for field, stats in missing_stats.items():
            logger.info(f"üìä {field}: {stats['completion_rate']:.1f}% compl√©t√© ({stats['empty_count']} manquants)")
        
        return missing_stats

    def deduplicate_batch_results(self, properties: List[Dict]) -> List[Dict]:
        """
        D√©duplication finale √† l'√©chelle du lot complet.
        """
        seen_keys = set()
        deduplicated = []
        
        for prop in properties:
            # Cl√© unique plus robuste
            key_parts = [
                prop.get('nom', ''),
                prop.get('prenom', ''),
                prop.get('section', ''),
                prop.get('numero', ''),
                prop.get('numero_majic', ''),
                prop.get('fichier_source', '')  # Inclure le fichier source pour √©viter les conflits
            ]
            unique_key = '|'.join(str(p).strip().upper() for p in key_parts)
            
            if unique_key not in seen_keys and unique_key != '|||||':
                seen_keys.add(unique_key)
                deduplicated.append(prop)
        
        removed = len(properties) - len(deduplicated)
        if removed > 0:
            logger.info(f"üßπ {removed} doublons supprim√©s lors de la d√©duplication finale")
        
        return deduplicated

    def export_to_csv_with_stats(self, all_properties: List[Dict]) -> None:
        """
        Export CSV et Excel avec statistiques d√©taill√©es.
        """
        if not all_properties:
            logger.warning("Aucune donn√©e √† exporter")
            return
        
        # Export CSV (avec point-virgule) ET Excel
        csv_path = self.export_to_csv(all_properties)
        excel_path = self.export_to_excel(all_properties, "output.xlsx")
        
        # G√©n√©rer des statistiques de qualit√©
        self.generate_quality_report(all_properties)
        
        logger.info(f"‚úÖ EXPORTS TERMIN√âS:")
        logger.info(f"üìÑ CSV: {csv_path}")
        logger.info(f"üìä Excel: {excel_path}")

    def generate_quality_report(self, properties: List[Dict]) -> None:
        """
        G√©n√®re un rapport de qualit√© d√©taill√© pour le lot trait√©.
        MODE S√âCURIS√â - Extraction pure uniquement.
        """
        total_props = len(properties)
        
        # Calculer les taux de compl√©tion par champ
        required_fields = [
            'department', 'commune', 'section', 'numero', 'contenance',
            'nom', 'prenom', 'numero_majic', 'voie', 'post_code', 'city'
        ]
        
        completion_stats = {}
        for field in required_fields:
            filled_count = sum(1 for prop in properties if prop.get(field))
            completion_rate = (filled_count / total_props) * 100 if total_props > 0 else 0
            completion_stats[field] = completion_rate
        
        # Log du rapport de qualit√©
        logger.info("üìä RAPPORT DE QUALIT√â - MODE S√âCURIS√â")
        logger.info("=" * 50)
        logger.info(f"üõ°Ô∏è EXTRACTION PURE - Fiabilit√© 100% garantie")
        logger.info(f"Total propri√©t√©s extraites: {total_props}")
        logger.info("\nTaux de compl√©tion par champ:")
        
        for field, rate in completion_stats.items():
            status = "üü¢" if rate >= 90 else "üü°" if rate >= 70 else "üî¥"
            logger.info(f"  {status} {field:<20}: {rate:5.1f}%")
        
        # Moyenne globale
        avg_completion = sum(completion_stats.values()) / len(completion_stats)
        overall_status = "üü¢" if avg_completion >= 90 else "üü°" if avg_completion >= 70 else "üî¥"
        logger.info(f"\n{overall_status} TAUX GLOBAL DE COMPL√âTION: {avg_completion:.1f}%")
        
        # Message de s√©curit√©
        logger.info("\nüéØ GARANTIES DE FIABILIT√â:")
        logger.info("  ‚úÖ Toutes les donn√©es proviennent directement des PDFs")
        logger.info("  ‚úÖ Aucune invention ou interpolation de donn√©es")
        logger.info("  ‚úÖ Colonnes vides = vraiment absentes du PDF original")
        logger.info("  ‚úÖ Aucun risque de m√©lange entre propri√©taires/adresses")

    def process_like_make(self, pdf_path: Path) -> List[Dict]:
        """
        R√âPLIQUE EXACTE DU WORKFLOW MAKE
        
        Suit exactement la m√™me logique que l'automatisation Make :
        1. pdfplumber pour les tableaux (comme Python Anywhere)
        2. OpenAI Vision simple pour les propri√©taires (prompt Make)
        3. Traitement individuel (comme BasicFeeder) 
        4. G√©n√©ration ID avec OpenAI (comme Make)
        5. Fusion 1:1 simple
        """
        logger.info(f"üéØ TRAITEMENT STYLE MAKE pour {pdf_path.name}")
        
        try:
            # √âTAPE 1: Extraction tableaux (comme Python Anywhere)
            structured_data = self.extract_tables_with_pdfplumber(pdf_path)
            logger.info(f"üìã Tableaux extraits: {len(structured_data.get('prop_batie', []))} b√¢tis, {len(structured_data.get('non_batie', []))} non-b√¢tis")
            
            # √âTAPE 2: Extraction propri√©taires (prompt Make exact)
            owners = self.extract_owners_make_style(pdf_path)
            logger.info(f"Proprietaires extraits: {len(owners)}")
            
            if not owners and not structured_data.get('prop_batie') and not structured_data.get('non_batie'):
                logger.warning(f"Aucune donn√©e extraite pour {pdf_path.name}")
                return []
            
            # √âTAPE 3: Traitement individuel (comme BasicFeeder Make)
            final_results = []
            
            # Traiter les propri√©t√©s non b√¢ties (comme route 1 Make)
            non_batie_props = structured_data.get('non_batie', [])
            if non_batie_props and owners:
                logger.info("üèûÔ∏è Traitement propri√©t√©s non b√¢ties style Make")
                for owner in owners:
                    for prop in non_batie_props:
                        if prop.get('Adresse'):  # Filtre comme Make
                            # G√©n√©ration ID avec OpenAI (comme Make)
                            unique_id = self.generate_id_with_openai_like_make(owner, prop)
                            
                            # Fusion 1:1 simple (comme Make)
                            combined = self.merge_like_make(owner, prop, unique_id, 'non_batie', pdf_path.name)
                            final_results.append(combined)
            
            # Traiter les propri√©t√©s b√¢ties (comme route 2 Make)
            prop_batie = structured_data.get('prop_batie', [])
            if prop_batie and owners:
                logger.info("üè† Traitement propri√©t√©s b√¢ties style Make")
                for owner in owners:
                    for prop in prop_batie:
                        if prop.get('Adresse'):  # Filtre comme Make
                            # G√©n√©ration ID avec OpenAI (comme Make)
                            unique_id = self.generate_id_with_openai_like_make(owner, prop)
                            
                            # Fusion 1:1 simple (comme Make)
                            combined = self.merge_like_make(owner, prop, unique_id, 'batie', pdf_path.name)
                            final_results.append(combined)
            
            # Si pas de structured data, juste les propri√©taires
            if not non_batie_props and not prop_batie and owners:
                logger.info("üë§ Seulement propri√©taires (pas de tableaux)")
                for owner in owners:
                    combined = self.merge_like_make(owner, {}, "", 'owners_only', pdf_path.name)
                    final_results.append(combined)
            
            # √âTAPE 4: S√©paration automatique des pr√©fixes coll√©s
            final_results = self.separate_stuck_prefixes(final_results)
            
            # √âTAPE 5: Propagation des valeurs manquantes (prefixe, contenance d√©taill√©e)
            final_results = self.propagate_values_downward(final_results, ['prefixe', 'contenance_ha', 'contenance_a', 'contenance_ca'])
            
            logger.info(f"Traitement Make termine: {len(final_results)} proprietes finales")
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement Make {pdf_path.name}: {e}")
            return []

    def extract_owners_make_style(self, pdf_path: Path) -> List[Dict]:
        """
        Extraction des propri√©taires EXACTEMENT comme Make.
        Utilise le prompt exact et les param√®tres exacts de Make.
        """
        logger.info(f"Extraction propri√©taires style Make pour {pdf_path.name}")
        
        # Convertir PDF en images
        images = self.pdf_to_images(pdf_path)
        if not images:
            return []
        
        all_owners = []
        
        for page_num, image_data in enumerate(images, 1):
            try:
                # Encoder l'image
                base64_image = base64.b64encode(image_data).decode('utf-8')
                
                # PROMPT EXACT DE MAKE (copi√© √† l'identique avec am√©lioration adresses)
                make_prompt = """In the following image, you will find information of owners such as nom, prenom, adresse, droit reel, numero proprietaire, department and commune. If there are any leading zero's before commune or deparment, keep it as it is. 

For addresses: Extract street address, city and post code separately. If some parts are missing, try to extract whatever is available. If completely no address is found, leave all address fields blank.

There can be one or multiple owners. I want to extract all of them and return them in json format.
output example:

{"owners": [
    {
        "nom": "MARTIN",
        "prenom": "MARIE MADELEINE",
        "street_address": "2 RUE DE PARIS",
       "city": "KINGERSHEIM",
        "post_code": "68260",
        "numero_proprietaire": "MBRWL8",
"department": 21,
"commune": 026,
"droit reel": "Propri√©taire/Indivision"
    },
    {
        "nom": "LALLEMAND",
        "prenom": "ERIC",
         "street_address": "2 RUE DE PARIS",
       "city": "KINGERSHEIM",
        "post_code": "68260",
        "numero_proprietaire": "MBXNZ8",
"department": 21,
"commune": 026,
"droit reel": "Propri√©taire/Indivision"
    }
]
}"""
                
                # Appel OpenAI avec PARAM√àTRES EXACTS DE MAKE
                response = self.client.chat.completions.create(
                    model="gpt-4o",  # M√™me mod√®le que Make
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": make_prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_image}",
                                        "detail": "high"  # M√™me que Make
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=2048,        # M√™me que Make
                    temperature=1,          # M√™me que Make (pas 1.0)
                    n=1,        # Corrig√©: n au lieu de n_completions
                    response_format={"type": "json_object"}  # M√™me que Make
                )
                
                # Parser la r√©ponse EXACTEMENT comme Make
                response_text = response.choices[0].message.content.strip()
                result = safe_json_parse(response_text, f"make style page {page_num}")
                if result and "owners" in result and result["owners"]:
                    page_owners = result["owners"]
                    all_owners.extend(page_owners)
                    logger.info(f"Page {page_num}: {len(page_owners)} proprietaire(s) extraits")
                else:
                    logger.warning(f"Pas de propri√©taires extraits page {page_num}")
                    
            except Exception as e:
                logger.error(f"Erreur extraction proprietaires page {page_num}: {e}")
                continue
        
        logger.info(f"Total proprietaires Make style: {len(all_owners)}")
        return all_owners

    def generate_id_with_openai_like_make(self, owner: Dict, prop: Dict) -> str:
        """
        G√âN√âRATION D'ID CORRIG√âE - Utilise notre m√©thode locale ultra-robuste
        au lieu du prompt OpenAI d√©faillant qui g√©n√©rait des IDs √† 13 caract√®res.
        
        GARANTIT exactement 14 caract√®res √† chaque fois.
        """
        # Extraire les donn√©es comme Make
        department = owner.get('department', '')
        commune = owner.get('commune', '')
        section = prop.get('Sec', '')
        plan_number = prop.get('N¬∞ Plan', '')
        
        # ‚úÖ UTILISATION DIRECTE de notre m√©thode locale CORRIG√âE
        # Plus fiable, plus rapide, et √©conomise les tokens OpenAI
        generated_id = self.generate_unique_id(
            str(department), str(commune), 
            str(section), str(plan_number)
        )
        
        logger.debug(f"ID g√©n√©r√© localement (14 car. garantis): {generated_id}")
        return generated_id

    def merge_like_make(self, owner: Dict, prop: Dict, unique_id: str, prop_type: str, pdf_path_name: str) -> Dict:
        """
        Fusion EXACTEMENT comme Make (mapping direct des champs).
        R√©plique la logique Google Sheets de Make.
        """
        
        # S√âPARATION AUTOMATIQUE DES PR√âFIXES COLL√âS
        import re
        raw_section = str(prop.get('Sec', ''))
        raw_prefixe = str(prop.get('Pr√©fixe', prop.get('Pfxe', '')))
        
        # Si pas de pr√©fixe et section contient pattern num√©rique+alphab√©tique
        if not raw_prefixe and raw_section:
            pattern = r'^(\d+)\s*([A-Z]+)$'  # \s* permet les espaces optionnels
            match = re.match(pattern, raw_section)
            if match:
                final_prefixe = match.group(1)  # 302
                final_section = match.group(2)  # A
                logger.info(f"üîç PR√âFIXE S√âPAR√â: '{raw_section}' ‚Üí pr√©fixe='{final_prefixe}' section='{final_section}'")
            else:
                final_prefixe = raw_prefixe
                final_section = raw_section
        else:
            final_prefixe = raw_prefixe
            final_section = raw_section
        
        # Mapping exact comme dans Make Google Sheets
        merged = {
            # Colonnes A-E (informations parcelle)
            'department': str(owner.get('department', '')),  # Colonne A
            'commune': str(owner.get('commune', '')),        # Colonne B  
            'prefixe': final_prefixe,                        # Colonne C (CORRIG√â - s√©paration auto)
            'section': final_section,                        # Colonne D (CORRIG√â - s√©paration auto)
            'numero': str(prop.get('N¬∞ Plan', '')),         # Colonne E
            
            # Colonnes F-H (gestion/demande - vides dans Make)
            'demandeur': '',    # Colonne F
            'date': '',         # Colonne G  
            'envoye': '',       # Colonne H
            
            # Colonne I (designation + contenance d√©taill√©e)
            'designation_parcelle': str(prop.get('Adresse', '')),  # Colonne I
            'contenance_ha': str(prop.get('HA', prop.get('Contenance', ''))),           # Hectares (fallback sur Contenance)
            'contenance_a': str(prop.get('A', '')),             # Ares  
            'contenance_ca': str(prop.get('CA', '')),           # Centiares
            
            # Colonnes J-O (propri√©taire)
            'nom': str(owner.get('nom', '')),                    # Colonne J
            'prenom': str(owner.get('prenom', '')),             # Colonne K
            'numero_majic': str(owner.get('numero_proprietaire', '')),  # Colonne L
            'voie': str(owner.get('street_address', '')),       # Colonne M
            'post_code': str(owner.get('post_code', '')),       # Colonne N
            'city': str(owner.get('city', '')),                 # Colonne O
            
            # Colonnes P-R (statuts - vides dans Make)
            'identifie': '',    # Colonne P
            'rdp': '',          # Colonne Q
            'sig': '',          # Colonne R
            
            # Colonnes S-T (ID et droit)
            'id': unique_id,                                    # Colonne S
            'droit_reel': str(owner.get('droit reel', '')),    # Colonne T
            
            # M√©tadonn√©es internes
            'fichier_source': pdf_path_name,
            'type_propriete': prop_type
        }
        
        return merged


def main():
    """Fonction principale."""
    # Charger les variables d'environnement
    load_dotenv()
    
    # Cr√©er et lancer l'extracteur
    extractor = PDFPropertyExtractor()
    extractor.run()


if __name__ == "__main__":
    main() 